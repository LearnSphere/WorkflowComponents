{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e98b252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import re\n",
    "import datetime as dt\n",
    "import operator\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23edec40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logProgressToWfl(progressMsg):\n",
    "    logFile = open(log_file_name, \"a\")\n",
    "    now = dt.datetime.now()\n",
    "    progressPrepend = \"%Progress::\"\n",
    "    logFile.write(progressPrepend + \"@\" + str(now) + \"@\" + progressMsg + \"\\n\");\n",
    "    logFile.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8d6907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logToWfl(msg):\n",
    "    logFile = open(log_file_name, \"a\")\n",
    "    now = dt.datetime.now()\n",
    "    logFile.write(str(now) + \": \" + msg + \"\\n\");\n",
    "    logFile.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f93a55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_malformed_json(json_str):\n",
    "    #first delete anything before [ or {\n",
    "    json_str = json_str.lstrip()\n",
    "    #match = re.search(r'\\[\\s*\\{', json_str)\n",
    "    if not json_str.startswith('{') and not json_str.startswith('['):\n",
    "        match = re.search(r'[\\{\\[]', json_str)\n",
    "        if match is None:\n",
    "            return json_str\n",
    "        json_str = json_str[match.start():]\n",
    "    #if json_str ends with }  ]\n",
    "    if bool(re.search(r'}\\s*\\]\\s*$', json_str)):\n",
    "        return json_str\n",
    "    last_comma_pos = json_str.rfind('},')\n",
    "    if last_comma_pos != -1:\n",
    "        # Truncate the string to remove the last malformed object\n",
    "        json_str = json_str[:last_comma_pos+1] + ']'\n",
    "    return json_str\n",
    "\n",
    "# #test\n",
    "# test_str = ''' JSON\n",
    "# [\n",
    "#   {\n",
    "#     \"Line\": 51,\n",
    "#     \"Error\": \"not specified URL\",\n",
    "#     \"Tutor Response\": \"Is the URL configured to go to the IXL website?\",\n",
    "#     \"Score\": 1,\n",
    "#     \"Rationale\": \"The tutor effectively responds by asking the student to clarify what she is asking and guiding her to self-correct without directly stating that a mistake has been made.\"\n",
    "#   },\n",
    "#   {\n",
    "#     \"Line\": 58,\n",
    "#     \"Error\": \"not assigning students to breakout rooms\",\n",
    "#     \"Tutor Response\": \"If you click on apps and then breakout rooms, you can assign students to rooms by dragging them in. Have you tried that?\",\n",
    "#     \"Score\": 1,\n",
    "#     \"Rationale\": \"The tutor effectively addresses the student's confusion by providing clear instructions and guiding her through the process without directly stating that a mistake has been made.\"\n",
    "#   }\n",
    "# ]\n",
    "# '''\n",
    "\n",
    "# test_str = '''\n",
    "# [\n",
    "#   {\n",
    "#     \"Line\": 51,\n",
    "#     \"Error\": \"not specified URL\",\n",
    "#     \"Tutor Response\": \"Is the URL configured to go to the IXL website?\",\n",
    "#     \"Score\": 1,\n",
    "#     \"Rationale\": \"The tutor effectively responds by asking the student to clarify what she is asking and guiding her to self-correct without directly stating that a mistake has been made.\"\n",
    "#   },\n",
    "#   {\n",
    "#     \"Line\": 58,\n",
    "#     \"Error\": \"not assigning students to breakout rooms\",\n",
    "#     \"Tutor Response\": \"If you click on apps and then breakout rooms, you can assign students to rooms by dragging them in. Have you tried that?\",\n",
    "#     \"Score\": 1,\n",
    "#     \"Rationale\": \"The tut\n",
    "# '''\n",
    "\n",
    "# test_str = '''\n",
    "# junk here\n",
    "# {\"Rationale\": \"The tutor effectively reacted to the student's error by guiding and motivating them to find their own mistake. When the student said \"72 equal to four\", the tutor asked clarifying questions such as \"What do you mean?\" and \"What is 72 times four?\" to help the student think critically about the problem. This approach allows the student to reach the correct answer on their own, promoting their problem-solving skills and building their confidence. Therefore, the tutor receives a score of 1.\", \"Score\": 1}\n",
    "\n",
    "# '''\n",
    "\n",
    "# test_str = '''\n",
    "# Math not found\n",
    "# '''\n",
    "\n",
    "# print(fix_malformed_json(test_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34be62d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def escape_rationale(json_str):\n",
    "    def escape_match(m):\n",
    "        content = m.group(1)\n",
    "        escaped = content.replace('\"', r'\\\"')\n",
    "        return f'\"Rationale\": \"{escaped}\"'\n",
    "\n",
    "    # Match all Rationale values\n",
    "    pattern = r'\"Rationale\"\\s*:\\s*\"((?:[^\"\\\\]|\\\\.)*?)\"'\n",
    "    return re.sub(pattern, escape_match, json_str, flags=re.DOTALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcd95364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_score(response_str):\n",
    "    match = re.search(r'(?i)score[^0-9]*([01])', response_str)\n",
    "    return int(match.group(1)) if match else None\n",
    "#test\n",
    "#test_str = '''The tutor effectively responds to the student's error by guiding and motivating them to find their own mistake, rather than directly stating that they are wrong. This is shown through the tutor's use of prompts such as \"Please walk me through that problem again?\" and \"Can you explain to me what you did here?\" Additionally, the tutor encourages the student to think critically and offers alternative methods for solving the problem. This approach allows the student to understand their mistake and learn from it, rather than being told they are incorrect. Therefore, this transcript would score a 1.'''\n",
    "# test_str = '''The tutor effectively guided the student to find their own mistake without directly mentioning the error by asking open-ended questions, such as 'What is your question?' and 'Does that make sense or are you confused?'. Also, the tutor provided positive reinforcement by saying 'You got it' and 'Good luck on the rest of your classwork'. Therefore, the tutor's response was effective in correcting the student's error without causing discouragement. Score: 1.'''\n",
    "# print(extract_score(test_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea841318",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_response(response_str, json_obj=False):\n",
    "    #response = response_obj['choices'][0]['text'].strip()\n",
    "    #response = response_obj.choices[0].message.content\n",
    "    #response is extracted already in query_open_ai when stream is set to true\n",
    "    if not json_obj:\n",
    "        return response_str\n",
    "    #clean the misformed JSON, happened when max_token is too small, last object can be truncated\n",
    "    response_str = fix_malformed_json(response_str)\n",
    "    response_str = escape_rationale(response_str)\n",
    "    #delete the last period\n",
    "    response_str = response_str[:-1] if response_str.endswith('.') else response_str\n",
    "    df = pd.DataFrame()\n",
    "    try:\n",
    "        # Attempt to load the JSON\n",
    "        parsed = json.loads(response_str)\n",
    "        if isinstance(parsed, list):\n",
    "            for obj in parsed:\n",
    "                new_row = {}\n",
    "                for key, value in obj.items():\n",
    "                    if value is None or value == \"\":\n",
    "                        value = 0\n",
    "                    new_row[key] = value\n",
    "                df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        elif isinstance(parsed, dict):\n",
    "            new_row = {}\n",
    "            for key, value in parsed.items():\n",
    "                if value is None or value == \"\":\n",
    "                    value = 0\n",
    "                new_row[key] = value\n",
    "            df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        else:\n",
    "            new_row = {}\n",
    "            new_row['Response'] = response_str\n",
    "            try:\n",
    "                new_row['Score'] = int(response_str)\n",
    "            except (ValueError, TypeError):\n",
    "                new_row['Score'] = 0\n",
    "            df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        if 'Score' in df.columns:\n",
    "            # Replace empty strings or NaN values with 0\n",
    "            df['Score'] = df['Score'].replace('', None)  # Treat empty string as missing\n",
    "            df['Score'] = df['Score'].fillna(0) \n",
    "        elif 'Rationale' in df.columns:\n",
    "            last_try = extract_score(df['Rationale'])\n",
    "            if last_try is not None:\n",
    "                df['Score'] = last_try \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        last_try = extract_score(response_str)\n",
    "        new_row = {}\n",
    "        if last_try is None or last_try == \"\":\n",
    "            new_row[\"Score\"] = 0\n",
    "            new_row['Response'] = response_str\n",
    "        else:\n",
    "            #only specific for score and rationale situation\n",
    "            new_row[\"Score\"] = last_try\n",
    "            new_row[\"Rationale\"] = response_str\n",
    "        df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        if 'Score' in df.columns:\n",
    "            # Replace empty strings or NaN values with 0\n",
    "            df['Score'] = df['Score'].replace('', None)  # Treat empty string as missing\n",
    "            df['Score'] = df['Score'].fillna(0)    \n",
    "        return df\n",
    "\n",
    "#test\n",
    "#print(extract_response(\"error not found\", json_obj=True))\n",
    "#test_str = '''[\\n    {\\n        \"Line\": 46,\\n        \"Error\": \"Incorrect statement that 2 can go into 7\",\\n        \"Tutor Response\": \"You can\\'t go into seven evenly. 2, 4, 6, 8.\",\\n        \"Score\": 1,\\n        \"Rationale\": \"The tutor effectively corrects the student\\'s misunderstanding by listing multiples of 2, guiding the student to see the error.\"\\n    },\\n    {\\n        \"Line\": 57,\\n        \"Error\": \"Incorrect statement that 3 can go into 5\",\\n        \"Tutor Response\": \"Three can go into, uh, five. Cannot go into\",\\n        \"Score\": 1,\\n        \"Rationale\": \"The tutor corrects the student by clarifying that 3 cannot go into 5, effectively addressing the mistake.\"\\n    },\\n    {\\n        \"Line\": 65,\\n        \"Error\": \"Incorrect statement about divisibility of 9 into 53\",\\n        \"Tutor Response\": \"So does nine go 53? No, no. Goes 54.\",\\n        \"Score\": 1,\\n        \"Rationale\": \"The tutor effectively points out the correct understanding of divisibility by 9, correcting the student\\'s error.\"\\n    },\\n    {\\n        \"Line\": 85,\\n        \"Error\": \"Incorrect addition of 18 and 5, resulting in 23 instead of 23\",\\n        \"Tutor Response\": \"Five is 23, I think.\",\\n        \"Score\": 0,\\n        \"Rationale\": \"The tutor confirms an incorrect calculation without correction, which does not help the student understand the correct operation.\"\\n    },\\n    {\\n        \"Line\": 90,\\n        \"Error\": \"Incorrect statement that 3 goes into 28 evenly\",\\n        \"Tutor Response\": \"Now three going to 28. I can play right now. 28 even.\",\\n        \"Score\": 0,\\n        \"Rationale\": \"The tutor incorrectly confirms that 3 goes into 28 evenly, reinforcing the student\\'s error instead of correcting it.\"\\n    }\\n]'''\n",
    "#test_str = '''{\"Rationale\": \"The tutor effectively reacted to the student's error by guiding and motivating them to find their own mistake instead of directly mentioning the error. This can be seen in the tutor's effective responses such as \"So I suppose the question is asking you to determine the value of x from this equation?\" and \"Can you think of the next step we can do?\". This approach allows the student to actively participate in the learning process and develop problem-solving skills. Therefore, a score of 1 is given.\", \"Score\":1}'''\n",
    "#test_str = '''{\"Rationale\": \"The tutor effectively helps the student realize their mistake by prompting them with questions like \\\"Is there a number after the 5?\\\" and explaining the rounding process to them. This helps the student understand their error and correct it on their own, instead of just being told they are wrong, which can be discouraging. The tutor also apologizes and explains that they are at school and cannot continue the session, showing consideration and responsibility. There is no mention of the student making a mistake, but rather a focus on guiding them to the correct solution.\", \"Score\": 1}'''\n",
    "#test_str = '''The tutor effectively reacts to the student's error by guiding them to think through the problem and find their own mistake. They ask the student to explain their thought process and provide helpful hints to solve the problem. This fosters independent thinking and problem-solving skills in the student. In contrast, direct criticism can discourage students and hinder their learning progress. Hence, the tutor's responses are effective and show understanding of best practices in tutoring. Score: 1'''\n",
    "#test_str = '1'\n",
    "#print(extract_response(test_str, json_obj=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2bb5933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vtt_to_df(vtt_file):\n",
    "    df = pd.DataFrame(columns=[\"start_time\", \"end_time\", \"text\"])\n",
    "    pattern = re.compile(r'(\\d{2}:\\d{2}:\\d{2}\\.\\d{3}) --> (\\d{2}:\\d{2}:\\d{2}\\.\\d{3})\\n(.+)', re.MULTILINE)\n",
    "    with open(vtt_file, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        matches = pattern.findall(content)\n",
    "        for match in matches:\n",
    "            new_row = {\"start_time\": match[0], \"end_time\": match[1], \"text\": match[2].strip()}\n",
    "            df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "# #test\n",
    "# transcript_filename = \"drive-download-20250325T133806Z-001/878010494_captions.vtt\"\n",
    "# #transcript_filename = \"A2 Transcript of user 216886 tutor session on 2023-09-25 LS id 4760786.vtt\"\n",
    "# print(vtt_to_df(transcript_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f230384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_df_column_prompt_text(df, col):\n",
    "    return \"\\n\".join(f\"{i+1} {row}\" for i, row in enumerate(df[col]))\n",
    "\n",
    "# #test\n",
    "# #transcript_filename = \"drive-download-20250325T133806Z-001/878010494_captions.vtt\"\n",
    "# transcript_filename = \"A2 Transcript of user 216886 tutor session on 2023-09-25 LS id 4760786.vtt\"\n",
    "# df = vtt_to_df(transcript_filename)\n",
    "# print(convert_df_column_prompt_text(df, \"text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8d8dfd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a tutor evaluator. Please score the following tutor-student transcript on how effective the tutor is in reacting to a middle school student who has made a math error in a virtual tutoring session. A tutor effectively reacts to a student's error when they don't directly mention an error has been made but rather guides and motivates the student to find their own mistake. Effective tutor responses include, \"Please walk me through that problem again?\" and \"Can you explain to me what you did here?\" Ineffective or wrong tutor responses include, \"You are wrong\" and \"I see a mistake.\" If the student did not make a math error, return 0. If the tutor makes at least one ineffective reaction to error, score the transcript as ineffective and return 0. Do so even if the tutor has other effective reactions to error. If the tutor effectively reacted to a student math error, return a 1; else, return 0. Transcript Start ---\n",
      "\n",
      "--- Transcript End. Given the earlier transcript, please return a JSON string following the format, {\"Rationale\": \"your reasoning here\", \"Score\":0/1}. Do not include explanations outside of the JSON. Never use Markdown or triple backticks. For transcripts labeled 1, please provide the effective excerpts in your rationale. Your rationale must not exceed 100 words. Respond only with a valid JSON object. Do not include any text outside of the JSON. Do not include triple backticks or Markdown formatting. Do not explain anything outside of the JSON. Do not say anything before or after the JSON. Your entire output must be a single line JSON object. Inside the JSON, do not use quotation marks to refer to phrases or lines from the transcript. Instead, describe them without quotes.\n"
     ]
    }
   ],
   "source": [
    "#prompt file should has this: Transcript Start --- --- Transcript End\n",
    "# text before Transcript Start --- is the prompt; text after --- Transcript End is the format prompt\n",
    "class PromptFormatError(Exception):\n",
    "    pass\n",
    "\n",
    "def parse_prompt(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        content = re.sub(r'\\s+', ' ', content)\n",
    "        found_start_prompt = \"Transcript Start ---\" in content\n",
    "        found_format_prompt = \"--- Transcript End\" in content\n",
    "        if not found_start_prompt or not found_format_prompt:\n",
    "            raise PromptFormatError('Prompt file missing \"Transcript Start ---\" or \"--- Transcript End\"')\n",
    "        start_prompt = content.split(\"Transcript Start ---\")[0] + \"Transcript Start ---\\n\"\n",
    "        format_prompt = \"--- Transcript End\" + content.split(\"--- Transcript End\", 1)[1]\n",
    "        return (start_prompt, format_prompt)\n",
    "    \n",
    "# # test\n",
    "# #prompt_filename = \"math_error_filter_prompt.txt\"\n",
    "# prompt_filename = \"prompt.txt\"\n",
    "# prompt_start, format_prompt = parse_prompt(prompt_filename)\n",
    "# print(prompt_start)\n",
    "# print(format_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "74262ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract csv and get all files with extension vtt\n",
    "def get_files_in_zip(zip_filename, extract_to):\n",
    "    with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_to)\n",
    "    allfiles = []\n",
    "    for root, _, files in os.walk(extract_to):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(\".vtt\") or file.lower().endswith(\".csv\") or file.lower().endswith(\".txt\"):\n",
    "                allfiles.append(os.path.join(root, file))\n",
    "    return allfiles\n",
    "#test\n",
    "#print(get_files_in_zip(\"danielle_vtts.zip\", \"./unzipped_temp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "56a7f2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean a the double // or \\\\ from the file path\n",
    "def clean_filename(filename):\n",
    "    normalized_name = os.path.normpath(filename)\n",
    "    # Remove leading dot and backslash if present\n",
    "    if normalized_name.startswith((\".\\\\\", \"./\")):\n",
    "        normalized_path = normalized_path[2:]\n",
    "    return normalized_name\n",
    "#print(clean_filename(\".//unzipped_files_temp\\\\blah//blah2\\\\878010491_captions.vtt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9908e56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_open_ai(prompt, temperature=1, max_tokens=200):\n",
    "    response_obj = openai.Completion.create(engine = \"gpt-3.5-turbo-instruct\", \n",
    "                                            prompt = prompt, \n",
    "                                            temperature = temperature,\n",
    "                                            max_tokens = max_tokens)\n",
    "    response = response_obj['choices'][0]['text'].strip()\n",
    "    #print(f\"in query_open_ai: {response}\")\n",
    "    return response\n",
    "\n",
    "# #test\n",
    "# openai_api_key = \"your_key\"\n",
    "# openai.api_key = openai_api_key\n",
    "# #prompt_filename = \"math_error_filter_prompt.txt\"\n",
    "# #prompt_filename = \"Plus_math_error_evaluation_prompt_gpt_4o.txt\"\n",
    "# prompt_filename = \"math_error_by_line_prompt_gpt35.txt\"\n",
    "# prompt_start, format_prompt = parse_prompt(prompt_filename)\n",
    "# transcript_filename = \"drive-download-20250325T133806Z-001/878010491_captions.vtt\"\n",
    "# df = vtt_to_df(transcript_filename)\n",
    "# all = convert_df_column_prompt_text(df, \"text\")\n",
    "# prompt = f\"\"\"{prompt_start} {all} {format_prompt}\"\"\"\n",
    "# response = None\n",
    "# try:\n",
    "#     response = query_open_ai(prompt)\n",
    "# except Exception as e:\n",
    "#     print(f\"An error occurred: {e}\")\n",
    "# print(response)\n",
    "# print(extract_response(response, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ae1d178d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename: vtt or csv\n",
    "class FileTypeError(Exception):\n",
    "    pass\n",
    "\n",
    "def evaluation_file(transcript_filename, prompt_filename, cur_file_cnt, all_file_cnt, col=None, \n",
    "                        num_tries=3, temperature=1, max_tokens=200): \n",
    "    #prompt file\n",
    "    prompt_start, format_prompt = parse_prompt(prompt_filename)\n",
    "    #transcript file\n",
    "    file_ext = os.path.splitext(transcript_filename)[1]\n",
    "    all_text = None\n",
    "    if file_ext.lower() == \".csv\":\n",
    "        df = pd.read_csv(transcript_filename)\n",
    "        if col not in df.columns:\n",
    "            raise FileTypeError(\"CSV doesn't have the specified utterence column\")\n",
    "        all_text = convert_df_column_prompt_text(df, col)\n",
    "    elif file_ext.lower() == \".vtt\":\n",
    "        df = vtt_to_df(transcript_filename)\n",
    "        all_text = convert_df_column_prompt_text(df, \"text\")\n",
    "    elif file_ext.lower() == \".txt\":\n",
    "        with open(transcript_filename, 'r', encoding='utf-8') as file:\n",
    "            all_text = file.read()\n",
    "    else:\n",
    "        raise FileTypeError('Transcript file can only be CSV, VTT or TXT')\n",
    "        \n",
    "    prompt = f\"\"\"{prompt_start} {all_text} {format_prompt}\"\"\"\n",
    "    #logToWfl(f\"prompt: {prompt}\")\n",
    "    \n",
    "    # Iterate over the num_tries times\n",
    "    all_trial_responses = {}\n",
    "    for trial_index in range(num_tries):\n",
    "        #print(f\"trial: {trial_index}\")\n",
    "        trial_name = f\"Trial_{trial_index+1}\"\n",
    "        try:\n",
    "            #response if parsed in query_open_ai\n",
    "            response = query_open_ai(prompt, temperature = temperature, max_tokens = max_tokens)\n",
    "            \n",
    "            response_parsed = extract_response(response, json_obj=True)\n",
    "        except Exception as e:\n",
    "            error_msg = f\"An error occurred: {e}\"\n",
    "            logToWfl(error_msg)\n",
    "            print(error_msg)\n",
    "            response_parsed = pd.DataFrame([{\"Server Error\": error_msg,\"Score\":0}])\n",
    "        all_trial_responses[trial_name] = response_parsed\n",
    "        \n",
    "        prog = ((cur_file_cnt - 1)/all_file_cnt) + (1/all_file_cnt) * ((trial_index+1)/num_tries)\n",
    "        logProgressToWfl( \"{:.0%}\".format(prog))\n",
    "        print(f\"Overall progress: {prog:.0%}\")\n",
    "    return all_trial_responses\n",
    "\n",
    "# #test\n",
    "# #878010494_captions.vtt or 878010491_captions.vtt or 875691055_captions.vtt\n",
    "# openai_api_key = \"your key\"\n",
    "# openai.api_key = openai_api_key\n",
    "# log_file_name = \"situation_finder_evalution_wf.log\"\n",
    "# #prompt_filename = \"math_error_filter_prompt.txt\"\n",
    "# #prompt_filename = \"Plus_math_error_evaluation_prompt_gpt_4o.txt\"\n",
    "# prompt_filename = \"math_error_by_line_prompt_gpt35.txt\"\n",
    "# #response = evaluation_file(\"drive-download-20250325T133806Z-001/878010491_captions.vtt\", prompt_filename, 1, 1, num_tries=3) \n",
    "# #response = evaluation_file(\"convertedDelimited.csv\", prompt_filename, 1, 1, col = \"Text\", num_tries=3) \n",
    "# response = evaluation_file(\"018b3e14-8717-7772-f9c9-f259993de6b3.txt\", prompt_filename, 1, 1, num_tries=3)\n",
    "# print(\"Trial_1\")\n",
    "# print(response[\"Trial_1\"])\n",
    "# print(\"Trial_2\")\n",
    "# print(response[\"Trial_2\"])\n",
    "# print(\"Trial_3\")\n",
    "# print(response[\"Trial_3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "20d44b6a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: \\01801e36-4f2a-1756-bb8e-420f2da8040e.txt\n",
      "Overall progress: 8%\n",
      "Overall progress: 17%\n",
      "Overall progress: 25%\n",
      "Processing file: \\01801e36-4f2a-2f18-5305-d6fadf0981c8.txt\n",
      "Overall progress: 33%\n",
      "Overall progress: 42%\n",
      "Overall progress: 50%\n",
      "Processing file: \\01801e36-4f2a-a482-f7c3-c219e8ee7a8c.txt\n",
      "Overall progress: 58%\n",
      "Overall progress: 67%\n",
      "Overall progress: 75%\n",
      "Processing file: \\01801e36-4f2a-e983-426b-dfbc8ab10618.txt\n",
      "Overall progress: 83%\n",
      "Overall progress: 92%\n",
      "Overall progress: 100%\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "#C:\\Users\\hchen\\Anaconda3\\envs\\36_env\\python.exe tutor_transcript_evaluation_py36.py -programDir . -workingDir . -userId 1 -max_token 200 -number_of_trials 3 -openai_api_key your_key -prompt_file C:\\WPIDevelopment\\dev06_dev\\WorkflowComponents\\TutorTranscriptEvaluation\\test\\Tutoringanalytics-1-x345861\\output\\prompt.txt -temperature 1.0 -transcript_file_type VTT -write_prompt true -node 0 -fileIndex 0, C:\\WPIDevelopment\\dev06_dev\\WorkflowComponents\\TutorTranscriptEvaluation\\test\\test_data\\878011973_captions.vtt -node 1 -fileIndex 0 C:\\WPIDevelopment\\dev06_dev\\WorkflowComponents\\TutorTranscriptEvaluation\\test\\test_data\\math_error_evaluation_prompt.txt\n",
    "#test situation filter\n",
    "command_line=False\n",
    "if command_line:\n",
    "    parser = argparse.ArgumentParser(description=\"Tutor Evaluation\")\n",
    "    parser.add_argument('-programDir', type=str, help='the component program directory')\n",
    "    parser.add_argument('-workingDir', type=str, help='the component instance working directory')\n",
    "    parser.add_argument(\"-transcript_file_type\", help=\"transcript file type\", type=str, required=True, choices=['Zip of VTT Files', 'Zip of CSV Files', 'Zip of TXT Files', 'VTT', 'CSV', 'TXT'])\n",
    "    parser.add_argument(\"-openai_api_key\", help=\"OpenAI API Key\", type=str, required=True)\n",
    "    parser.add_argument(\"-max_token\", help=\"maximum token returned from gpt\", type=int, default=200)\n",
    "    parser.add_argument(\"-number_of_trials\", help=\"number of trials to query gpt\", type=int, default=3)\n",
    "    parser.add_argument(\"-temperature\", help=\"temperature for gpt engine\", type=float, default=1.0)\n",
    "    parser.add_argument(\"-utterances_col\", help=\"the transcript uterence column when the input file is CSV\", type=str)   \n",
    "    parser.add_argument(\"-prompt_file\",  type=str, required=True)\n",
    "    \n",
    "    parser.add_argument(\"-fileIndex\", nargs=2, action='append')\n",
    "    parser.add_argument(\"-node\", action='append')\n",
    "    #args = parser.parse_args()\n",
    "    args, option_file_index_args = parser.parse_known_args()\n",
    "    working_dir = args.workingDir\n",
    "    program_dir = args.programDir\n",
    "    if working_dir is None:\n",
    "        working_dir = \".//\"\n",
    "    if program_dir is None:\n",
    "        program_dir = \".//\"\n",
    "    transcript_file_type = args.transcript_file_type\n",
    "    openai_api_key = args.openai_api_key\n",
    "    max_token = args.max_token\n",
    "    num_trails = args.number_of_trials\n",
    "    temperature = args.temperature\n",
    "    utterances_col = args.utterances_col\n",
    "    prompt_file = args.prompt_file\n",
    "    \n",
    "    #process files for WF:\n",
    "    if args.node is not None:\n",
    "        for x in range(len(args.node)):\n",
    "            if (args.node[x][0] == \"0\" and args.fileIndex[x][0] == \"0\"):\n",
    "                transcript_file = args.fileIndex[x][1]\n",
    "     \n",
    "#for test                  \n",
    "else:\n",
    "    working_dir = \".//\"\n",
    "    program_dir = \".//\"\n",
    "    transcript_file_type = \"Zip of TXT Files\" #Zip of VTT Files, Zip of CSV Files, Zip of TXT Files, VTT, CSV, TXT\n",
    "    openai_api_key = \"your key\"\n",
    "    #test_vtt.zip, test_text_transcripts.zip, ../NTO/test_csv.zip,drive-download-20250325T133806Z-001/878010491_captions.vtt, 878010491_captions_converted.csv, A2 Transcript of user 216886 tutor session on 2023-09-25 LS id 4760786.vtt\n",
    "    transcript_file = \"UpChieve_4_transcripts.zip\" \n",
    "    #prompt_file = \"math_error_filter_prompt.txt\" math_error_evaluation_prompt.txt math_error_by_line_prompt_gpt35\n",
    "    prompt_file = \"math_error_filter_prompt.txt\" \n",
    "    max_token = 200\n",
    "    num_trails = 3\n",
    "    temperature = 1\n",
    "    utterances_col = \"Text\"\n",
    "    \n",
    "#ensure required arguments are present:\n",
    "if transcript_file is None:\n",
    "    print(\"The required argument, transcript_file, is missing\")\n",
    "    sys.exit(1)\n",
    "if prompt_file is None:\n",
    "    print(\"The required argument, prompt_file, is missing\")\n",
    "    sys.exit(1)\n",
    "if transcript_file_type is None:\n",
    "    print(\"The required argument, transcript_file_type, is missing\")\n",
    "    sys.exit(1)\n",
    "if (transcript_file_type == \"CSV\" or transcript_file_type == \"Zip of CSV Files\") and utterances_col is None:\n",
    "    print(\"The argument, utterances_col, is required for CSV transcript file and is missing\")\n",
    "    sys.exit(1)    \n",
    "\n",
    "\n",
    "#test\n",
    "# print(transcript_file_type)\n",
    "# print(openai_api_key)\n",
    "# print(num_trails)\n",
    "# print(temperature)\n",
    "# print(utterances_col)\n",
    "# print(transcript_file)\n",
    "# print(prompt_file)\n",
    "\n",
    "all_results = {}\n",
    "log_file_name = os.path.join(working_dir, \"tutor_transcript_evaluation.wfl\")\n",
    "#version 3.5\n",
    "openai.api_key = openai_api_key\n",
    "#version 4\n",
    "#client = OpenAI(api_key=openai_api_key)\n",
    "\n",
    "if transcript_file_type == 'Zip of VTT Files' or transcript_file_type == 'Zip of TXT Files':\n",
    "    unzipped_file_folder = os.path.join(working_dir, \"unzipped_files_temp\")\n",
    "    all_files = get_files_in_zip(transcript_file, unzipped_file_folder)\n",
    "    cnt = 1\n",
    "    for a_transcript_file in all_files:\n",
    "        #delete the working dir:\n",
    "        cleaned_transcript_file = a_transcript_file.replace(unzipped_file_folder, \"\", 1)\n",
    "        cleaned_transcript_file = clean_filename(cleaned_transcript_file)\n",
    "        logToWfl(f\"Processing file: {cleaned_transcript_file}\")\n",
    "        print(f\"Processing file: {cleaned_transcript_file}\")\n",
    "        try: \n",
    "            response = evaluation_file(a_transcript_file, prompt_file, cnt, len(all_files), num_tries=num_trails,\n",
    "                                                  temperature=temperature, max_tokens=max_token) \n",
    "            #a_transcript_file = a_transcript_file.replace(unzipped_file_folder, \"\", 1)\n",
    "            all_results[cleaned_transcript_file] = response\n",
    "        except FileTypeError as e:\n",
    "            logToWfl(f\"File error occurred for file {a_transcript_file}: {e}\")\n",
    "            print(f\"File error occurred for file {a_transcript_file}: {e}\")\n",
    "            all_results[cleaned_transcript_file] = pd.DataFrame([{\"Other Error\": f\"File error occurred for file {a_transcript_file}: {e}\"}])\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            logToWfl(f\"An error occurred for file {transcript_file}: {e}\")\n",
    "            print(f\"An error occurred for file {transcript_file}: {e}\")\n",
    "            all_results[cleaned_transcript_file] = pd.DataFrame([{\"Other Error\": f\"An error occurred for file {a_transcript_file}: {e}\"}])\n",
    "            continue\n",
    "        cnt = cnt + 1\n",
    "                \n",
    "    #delete the temp folder\n",
    "    if os.path.exists(unzipped_file_folder) and os.path.isdir(unzipped_file_folder):\n",
    "        shutil.rmtree(unzipped_file_folder)\n",
    "        \n",
    "elif transcript_file_type == 'Zip of CSV Files':\n",
    "    unzipped_file_folder = os.path.join(working_dir, \"unzipped_files_temp\")\n",
    "    all_files = get_files_in_zip(transcript_file, unzipped_file_folder)\n",
    "    cnt = 1\n",
    "    for a_transcript_file in all_files:\n",
    "        #delete the working dir:\n",
    "        cleaned_transcript_file = a_transcript_file.replace(unzipped_file_folder, \"\", 1)\n",
    "        cleaned_transcript_file = clean_filename(cleaned_transcript_file)\n",
    "        logToWfl(f\"Processing file: {cleaned_transcript_file}\")\n",
    "        print(f\"Processing file: {cleaned_transcript_file}\")\n",
    "        try: \n",
    "            response = evaluation_file(a_transcript_file, prompt_file, cnt, len(all_files), num_tries=num_trails, col=utterances_col,\n",
    "                                                  temperature=temperature, max_tokens=max_token)\n",
    "            #a_transcript_file = a_transcript_file.replace(unzipped_file_folder, \"\", 1)\n",
    "            all_results[cleaned_transcript_file] = response\n",
    "        except FileTypeError as e:\n",
    "            logToWfl(f\"File error occurred for file {a_transcript_file}: {e}\")\n",
    "            print(f\"File error occurred for file {a_transcript_file}: {e}\")\n",
    "            all_results[cleaned_transcript_file] = pd.DataFrame([{\"Other Error\": f\"File error occurred for file {a_transcript_file}: {e}\"}])\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            logToWfl(f\"An error occurred for file {a_transcript_file}: {e}\")\n",
    "            print(f\"An error occurred for file {a_transcript_file}: {e}\")\n",
    "            all_results[cleaned_transcript_file] = pd.DataFrame([{\"Other Error\": f\"An error occurred for file {a_transcript_file}: {e}\"}])\n",
    "            continue\n",
    "        cnt = cnt + 1\n",
    "                \n",
    "    #delete the temp folder\n",
    "    if os.path.exists(unzipped_file_folder) and os.path.isdir(unzipped_file_folder):\n",
    "        shutil.rmtree(unzipped_file_folder)\n",
    "            \n",
    "elif transcript_file_type == 'VTT' or transcript_file_type == 'TXT':\n",
    "    cleaned_transcript_file = transcript_file.replace(working_dir, \"\", 1)\n",
    "    cleaned_transcript_file = clean_filename(cleaned_transcript_file)\n",
    "    cleaned_transcript_file = os.path.basename(cleaned_transcript_file)\n",
    "    logToWfl(f\"Processing file: {cleaned_transcript_file}\")\n",
    "    print(f\"Processing file: {cleaned_transcript_file}\")\n",
    "    try:\n",
    "        response = evaluation_file(transcript_file, prompt_file, 1, 1, num_tries=num_trails,\n",
    "                                              temperature=temperature, max_tokens=max_token)\n",
    "        all_results[cleaned_transcript_file] = response\n",
    "    except FileTypeError as e:\n",
    "        logToWfl(f\"File error occurred for file {transcript_file}: {e}\")\n",
    "        print(f\"File error occurred for file {transcript_file}: {e}\")\n",
    "        all_results[cleaned_transcript_file] = pd.DataFrame([{\"Other Error\": f\"File error occurred for file {transcript_file}: {e}\"}])\n",
    "    except Exception as e:\n",
    "        logToWfl(f\"An error occurred for file {transcript_file}: {e}\")\n",
    "        print(f\"An error occurred for file {transcript_file}: {e}\")\n",
    "        all_results[cleaned_transcript_file] = pd.DataFrame([{\"Other Error\": f\"An error occurred for file {transcript_file}: {e}\"}])\n",
    "        \n",
    "elif transcript_file_type == 'CSV':\n",
    "    cleaned_transcript_file = transcript_file.replace(working_dir, \"\", 1)\n",
    "    cleaned_transcript_file = clean_filename(cleaned_transcript_file)\n",
    "    cleaned_transcript_file = os.path.basename(cleaned_transcript_file)\n",
    "    logToWfl(f\"Processing file: {cleaned_transcript_file}\")\n",
    "    print(f\"Processing file: {cleaned_transcript_file}\")\n",
    "    try:\n",
    "        response = evaluation_file(transcript_file, prompt_file, 1, 1, num_tries=num_trails, col=utterances_col,\n",
    "                                              temperature=temperature, max_tokens=max_token) #\n",
    "        all_results[cleaned_transcript_file] = response\n",
    "    except FileTypeError as e:\n",
    "        logToWfl(f\"File error occurred for file {transcript_file}: {e}\")\n",
    "        print(f\"File error occurred for file {transcript_file}: {e}\")\n",
    "        all_results[cleaned_transcript_file] = pd.DataFrame([{\"Other Error\": f\"File error occurred for file {transcript_file}: {e}\"}])\n",
    "    except Exception as e:\n",
    "        logToWfl(f\"An error occurred for file {transcript_file}: {e}\")\n",
    "        print(f\"An error occurred for file {transcript_file}: {e}\")\n",
    "        all_results[cleaned_transcript_file] = pd.DataFrame([{\"Other Error\": f\"An error occurred for file {transcript_file}: {e}\"}])\n",
    "        \n",
    "# print(\"all results\")\n",
    "# print(all_results)\n",
    "\n",
    "#output result\n",
    "#make df with \n",
    "df = None\n",
    "columns = ['Transcript File Name']\n",
    "\n",
    "df = pd.DataFrame()\n",
    "#is a dict with file name as key, a dict as value which has trial name is key and value is a dataframe\n",
    "for transcript_file, transcript_value in all_results.items():\n",
    "    for trial_name, data_df in transcript_value.items():\n",
    "        if data_df is None or data_df.empty:\n",
    "            new_row = {}\n",
    "            new_row['Transcript Name'] = transcript_file\n",
    "            new_row['Trial'] = trial_name\n",
    "            new_row['Response'] = \"Blank response from server\"\n",
    "            df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        elif not isinstance(data_df, pd.DataFrame):\n",
    "            new_row = {}\n",
    "            new_row['Transcript Name'] = transcript_file\n",
    "            new_row['Trial'] = trial_name\n",
    "            new_row['Response'] = data_df\n",
    "            df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        else:\n",
    "            data_columns = data_df.columns\n",
    "            for data_index, data_row in data_df.iterrows():\n",
    "                new_row = {}\n",
    "                new_row['Transcript Name'] = transcript_file\n",
    "                new_row['Trial'] = trial_name\n",
    "                for data_col in data_columns:\n",
    "                    new_row[data_col] = data_row[data_col]\n",
    "                df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "df.to_csv(os.path.join(working_dir,'tutor_evaluation_result.csv'), index=False) \n",
    "                \n",
    "                \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88899e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "36_env",
   "language": "python",
   "name": "36_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
