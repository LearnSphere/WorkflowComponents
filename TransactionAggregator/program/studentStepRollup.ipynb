{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import datetime as dt\n",
    "import argparse\n",
    "import re\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check/convert time format\n",
    "def checkDatetimeFormat(colName):\n",
    "    try:\n",
    "        return pd.to_datetime(df[colName])\n",
    "    except ValueError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check/convert numeric format\n",
    "def checkIntegerFormat(df, colName):\n",
    "    try:\n",
    "        return df[colName].astype(int)\n",
    "    except ValueError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check/convert numeric format\n",
    "def checkNumericData(colName):\n",
    "    try:\n",
    "        pd.to_numeric(df[colName])\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make dataframe with unique combination of columns passes\n",
    "def uniqueColumnsDF(cols, includeNullValue=True):\n",
    "    uniqueDF = None\n",
    "    if includeNullValue:\n",
    "         uniqueDF = df[cols].drop_duplicates().reset_index()\n",
    "    else:\n",
    "        uniqueDF = df[cols].drop_duplicates().dropna().reset_index()\n",
    "    #drop the old index column and get new index\n",
    "    uniqueDF.drop('index', axis=1, inplace=True)\n",
    "    uniqueDF.reset_index(inplace=True)\n",
    "    return uniqueDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjust for time zone: timeDF has a time column followed a time zone column\n",
    "#change everything to UTC\n",
    "def to_UTC(row):\n",
    "    if not row[1]:\n",
    "        return row[0]\n",
    "    else:\n",
    "        return row[0].tz_localize(row[1]).tz_convert('UTC')\n",
    "    \n",
    "#change UTC back to local\n",
    "def to_local(row):\n",
    "    if not row[1]:\n",
    "        return row[0]\n",
    "    else:\n",
    "        return row[0].tz_localize('UTC').tz_convert(row[1])\n",
    "    \n",
    "\n",
    "def standardizeTimeZone(timeDF):\n",
    "    try:\n",
    "        return timeDF.apply(to_UTC, axis=1)\n",
    "    except:\n",
    "        return None\n",
    "    \n",
    "def localizeTimeZone(timeDF):\n",
    "    try:\n",
    "        return timeDF.apply(to_local, axis=1)\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logToWfl(msg):\n",
    "    logFile = open(\"transactionAggregatorLog.wfl\", \"a\")\n",
    "    now = dt.datetime.now()\n",
    "    logFile.write(str(now) + \": \" + msg + \"\\n\");\n",
    "    logFile.close();\n",
    "    \n",
    "def logProgressToWfl(progressMsg):\n",
    "    logFile = open(\"transactionAggregatorLog.wfl\", \"a\")\n",
    "    now = dt.datetime.now()\n",
    "    progressPrepend = \"%Progress::\"\n",
    "    logFile.write(progressPrepend + \"@\" + str(now) + \"@\" + progressMsg + \"\\n\");\n",
    "    logFile.close();\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [-programDir PROGRAMDIR]\n",
      "                             [-workingDir WORKINGDIR] [-node NODE]\n",
      "                             [-fileIndex FILEINDEX FILEINDEX]\n",
      "                             [-aggregatedTo {Student-step rollup,Transaction}]\n",
      "                             [-kcModelsToAggregate KCMODELSTOAGGREGATE]\n",
      "                             [-userId USERID]\n",
      "ipykernel_launcher.py: error: argument -fileIndex: expected 2 arguments\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3304: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "#test command from WF component:\n",
    "#C:/Python35/Python studentStepRollup.py -programDir . -workingDir . -userId 1 -aggregatedTo \"Student-step rollup\" -kcModelsToAggregate_nodeIndex 0 -kcModelsToAggregate_fileIndex 0 -kcModelsToAggregate \"KC (Original)\" -kcModelsToAggregate_nodeIndex 0 -kcModelsToAggregate_fileIndex 0 -kcModelsToAggregate \"KC (Area)\" -kcModelsToAggregate_nodeIndex 0 -kcModelsToAggregate_fileIndex 0 -kcModelsToAggregate \"KC (Textbook)\" -node 0 -fileIndex 0 \"C:\\WPIDevelopment\\dev06_dev\\WorkflowComponents\\TransactionAggregator\\test\\test_data\\ds76_tx_All_Data_74_2018_0912_070949.txt\"\n",
    "#C:/Python35/Python studentStepRollup.py -programDir . -workingDir . -userId 1 -aggregatedTo \"Transaction\" -kcModelsToAggregate_nodeIndex 0 -kcModelsToAggregate_fileIndex 0 -kcModelsToAggregate \"KC (Original)\" -kcModelsToAggregate_nodeIndex 0 -kcModelsToAggregate_fileIndex 0 -kcModelsToAggregate \"KC (Area)\" -kcModelsToAggregate_nodeIndex 0 -kcModelsToAggregate_fileIndex 0 -kcModelsToAggregate \"KC (Textbook)\" -node 0 -fileIndex 0 \"C:\\WPIDevelopment\\dev06_dev\\WorkflowComponents\\TransactionAggregator\\test\\test_data\\ds76_tx_All_Data_74_2018_0912_070949.txt\"\n",
    "#C:/Python35/Python studentStepRollup.py -programDir . -workingDir . -userId 1 -aggregatedTo \"Transaction\" -node 0 -fileIndex 0 \"C:\\WPIDevelopment\\dev06_dev\\WorkflowComponents\\TransactionAggregator\\test\\test_data\\ds76_tx_All_Data_74_2018_0912_070949.txt\"\n",
    "\n",
    "#command line\n",
    "parser = argparse.ArgumentParser(description='Process datashop file.')\n",
    "parser.add_argument('-programDir', type=str, help='the component program directory')\n",
    "parser.add_argument('-workingDir', type=str, help='the component instance working directory')\n",
    "parser.add_argument(\"-node\", nargs=1, action='append')\n",
    "parser.add_argument(\"-fileIndex\", nargs=2, action='append')\n",
    "parser.add_argument('-aggregatedTo', choices=[\"Student-step rollup\", \"Transaction\"], help='the file type to aggregate to(default=\"Student-step rollup\")', default=\"Student-step rollup\")\n",
    "parser.add_argument('-kcModelsToAggregate', nargs=1, action='append', type=str, help='the KC models that you would like to aggregate; e.g., \"Item\"')\n",
    "parser.add_argument('-userId', type=str,  help='placeholder for WF', default='')\n",
    "args, option_file_index_args = parser.parse_known_args()\n",
    "\n",
    "#fresh new log file\n",
    "logFile = open(\"transactionAggregatorLog.wfl\", \"w\")\n",
    "logFile.close();\n",
    "\n",
    "file_encoding = 'utf8'        # set file_encoding to the file encoding (utf8, latin1, etc.)\n",
    "input_fd = open(args.fileIndex[0][1], encoding=file_encoding, errors = 'backslashreplace')\n",
    "df = pd.read_csv(input_fd, na_values=['null', 'na', 'n/a', 'nan'], sep=\"\\t\")\n",
    "originalAllColNames = df.columns\n",
    "\n",
    "#flag for converting to student step\n",
    "convertToStudentStep = True\n",
    "if args.aggregatedTo == \"Transaction\":\n",
    "    convertToStudentStep = False\n",
    "\n",
    "kcModelsToInclude = []\n",
    "if args.kcModelsToAggregate is not None:\n",
    "    flattened = [val for sublist in args.kcModelsToAggregate for val in sublist]\n",
    "    for x in flattened:\n",
    "        patternC = re.compile('\\\\s*KC\\\\s*\\\\(( .* )\\\\s*\\\\)', re.VERBOSE)\n",
    "        kcModelsToInclude.append(patternC.sub(r'\\1', x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: FutureWarning: read_table is deprecated, use read_csv instead.\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "#test with jupyter notebook\n",
    "if True:\n",
    "    file_encoding = 'utf8'        # set file_encoding to the file encoding (utf8, latin1, etc.)\n",
    "    #input_fd = open('test_data_transaction.txt', encoding=file_encoding, errors = 'backslashreplace')\n",
    "    #input_fd = open('ds445_tx_All_Data_1469_2016_0403_085024.txt', encoding=file_encoding, errors = 'backslashreplace')\n",
    "    #input_fd = open('ds76_tx_All_Data_74_2018_0912_070949_ori.txt', encoding=file_encoding, errors = 'backslashreplace')\n",
    "    input_fd = open('new_aggr_sp_no_data_in_event_type_results/ds2846_tx_test_converted_with_event_type_no_data.txt', encoding=file_encoding, errors = 'backslashreplace')\n",
    "    \n",
    "    df = pd.read_table(input_fd, na_values=['null', 'na', 'n/a', 'nan'], sep=\"\\t\")\n",
    "    originalAllColNames = df.columns\n",
    "    #print(df.dtypes)\n",
    "    \n",
    "    #fresh new log file\n",
    "    logFile = open(\"transactionAggregatorLog.wfl\", \"w\")\n",
    "    logFile.close();\n",
    "\n",
    "    #flag for converting to student step\n",
    "    convertToStudentStep = True\n",
    "    kcModelsToInclude = []\n",
    "    #kcModelsToInclude = ['KC (Area)', 'KC (Original)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "if convertToStudentStep:\n",
    "    #drop ignored columns\n",
    "    keepColumns = ['Transaction Id', \n",
    "                   'Anon Student Id',\n",
    "                    'Time',\n",
    "                    'Time Zone',\n",
    "                    'Problem Name',\n",
    "                    'Problem View',\n",
    "                    'Problem Start Time',\n",
    "                    'Step Name',\n",
    "                    'Outcome',\n",
    "                    'Selection',\n",
    "                    'Action',\n",
    "                    'Input',\n",
    "                    'Condition Name',\n",
    "                    'Condition Type',\n",
    "                    'School',\n",
    "                    'Class',\n",
    "                  'Event Type']\n",
    "    keepPartialMatchedColumns = ['Level (', 'KC (', 'KC Category (', 'Condition ']\n",
    "    dropColumns = []\n",
    "    for x in df.columns.values.tolist():\n",
    "        if (x not in keepColumns):\n",
    "            findPartial = False\n",
    "            for y in keepPartialMatchedColumns:\n",
    "                if x.find(y) == 0:\n",
    "                    findPartial = True\n",
    "                    break;\n",
    "            if not findPartial:\n",
    "                dropColumns.append(x)\n",
    "\n",
    "    df.drop(dropColumns, axis=1, inplace=True)\n",
    "    ##write dropped column names to log file\n",
    "    logToWfl('Student step rollup process has dropped these columns: %s\\n' % (', '.join(dropColumns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete un-interested KC columns for convertToStudentStep\n",
    "if convertToStudentStep:\n",
    "    if len(kcModelsToInclude) > 0:\n",
    "        allColNames = df.columns\n",
    "        for colName in allColNames:\n",
    "            if colName.find('KC (') == 0:\n",
    "                KCName = colName[len('KC (') : colName.find(')')]\n",
    "                if KCName not in kcModelsToInclude:\n",
    "                    df.drop(colName, axis=1, inplace=True)\n",
    "            elif colName.find('KC Category (') == 0:\n",
    "                KCName = colName[len('KC Category (') : colName.find(')')]\n",
    "                if KCName not in kcModelsToInclude:\n",
    "                    df.drop(colName, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check null values in required columns: Anon Student Id, time, problem name\n",
    "#if any null values, send out error message\n",
    "requiredColumns = ['Anon Student Id', 'Time', 'Problem Name']\n",
    "colNullSum = df.isnull().sum()\n",
    "#print(colNullSum)\n",
    "errorMsg = \"\"\n",
    "for reqCol in requiredColumns:\n",
    "    if colNullSum[reqCol] != 0:\n",
    "        if errorMsg == \"\":\n",
    "            errorMsg = \"Found null values in required columns \"\n",
    "        errorMsg += reqCol + \"; \"\n",
    "if errorMsg != \"\":\n",
    "    ##write error to log file\n",
    "    logToWfl(errorMsg + ', Process aborted.\\n')\n",
    "    sys.exit(errorMsg)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check and convert time format for Time \n",
    "newCol = checkDatetimeFormat('Time')\n",
    "if newCol is None:\n",
    "    errorMsg = \"Time column has invalid formatted time\"\n",
    "    ##write error to log file\n",
    "    logToWfl(errorMsg + ', Process aborted.\\n')\n",
    "    sys.exit(errorMsg)\n",
    "else:\n",
    "    df['Time'] = newCol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if at least one of problem view and problem start time columns exist\n",
    "#and check and convert problem start time format\n",
    "allColNames = df.columns\n",
    "pvExist = 'Problem View' in allColNames\n",
    "pstExist = 'Problem Start Time' in allColNames\n",
    "if not pvExist and not pstExist:\n",
    "    errorMsg = \"Data missing both Problem Start Time and Problem View columns. At least one of them are required.\"\n",
    "    ##write error to log file\n",
    "    logToWfl(errorMsg + ', Process aborted.\\n')\n",
    "    sys.exit(errorMsg)\n",
    "if pstExist:\n",
    "    newCol = checkDatetimeFormat('Problem Start Time')\n",
    "    if newCol is None:\n",
    "        errorMsg = \"Problem Start Time column has invalid formatted time\"\n",
    "        ##write error to log file\n",
    "        logToWfl(errorMsg + ', Process aborted.\\n')\n",
    "        sys.exit(errorMsg) \n",
    "    else:\n",
    "        df['Problem Start Time'] = newCol\n",
    "\n",
    "#if problem view column exists, check to make sure it can be converted to number if not null\n",
    "if pvExist and not checkNumericData('Problem View'):\n",
    "    errorMsg = \"Problem View column is not all integer\"\n",
    "    ##write error to log file\n",
    "    logToWfl(errorMsg + ', Process aborted.\\n')\n",
    "    sys.exit(errorMsg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjust for time zone for Time and problem start time column\n",
    "allColNames = df.columns\n",
    "timezoneExist = 'Time Zone' in allColNames\n",
    "timeZoneAdjusted = False\n",
    "if timezoneExist:\n",
    "    if len(df['Time Zone'].dropna().unique()) > 1:\n",
    "        timeZoneAdjusted = True\n",
    "        #get new time for Time and assign original time to another column\n",
    "        df = df.reindex(columns = df.columns.tolist() + ['Original Time'])\n",
    "        df['Original Time'] = df['Time']\n",
    "        newCol = standardizeTimeZone(df.loc[:,['Time', 'Time Zone']])\n",
    "\n",
    "        if (newCol is None):\n",
    "            errorMsg = \"Conversion of Time column to UTC time zone failed\"\n",
    "            ##write error to log file\n",
    "            logToWfl(errorMsg + ', Process aborted.\\n')\n",
    "            sys.exit(errorMsg) \n",
    "        else:\n",
    "            df['Time'] = newCol\n",
    "\n",
    "        #get new time for Start problem time and assign original problem start time to another column\n",
    "        if pstExist:\n",
    "            df = df.reindex(columns = df.columns.tolist() + ['Original Problem Start Time'])\n",
    "            df['Original Problem Start Time'] = df['Problem Start Time']\n",
    "            newCol = standardizeTimeZone(df.loc[:,['Problem Start Time', 'Time Zone']])\n",
    "            if newCol is None:\n",
    "                errorMsg = \"Conversion of Problem Start Time column to UTC time zone failed\"\n",
    "                ##write error to log file\n",
    "                logToWfl(errorMsg + ', Process aborted.\\n')\n",
    "                sys.exit(errorMsg) \n",
    "            else:\n",
    "                df['Problem Start Time'] = newCol\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#order df by student, time, problem_name PV and/or PST\n",
    "sortColm = ['Anon Student Id', 'Time', 'Problem Name']\n",
    "if pvExist:\n",
    "    sortColm.append('Problem View')\n",
    "if pstExist:\n",
    "    sortColm.append('Problem Start Time')\n",
    "df = df.sort_values(by=sortColm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#turn levels into problem hierarchy column and drop all level columns\n",
    "def row_func_levels(row, cntRound, levelName, levelColName):\n",
    "    partOne = levelName\n",
    "    partTwo = row[levelColName]\n",
    "    if pd.isnull(partTwo):\n",
    "        partTwo = ''\n",
    "    if cntRound == 0:\n",
    "        return '%s %s' % (partOne, partTwo)\n",
    "    else:\n",
    "        return '%s, %s %s' % (row['Problem Hierarchy'],partOne,partTwo)\n",
    "\n",
    "allColNames = df.columns\n",
    "levelNames = []\n",
    "levelColNames = []\n",
    "for colName in allColNames:\n",
    "    if 'Level (' in colName:\n",
    "        levelNames.append(colName[len('Level (') : colName.find(')')])\n",
    "        levelColNames.append(colName)\n",
    "#add new column: Problem Hierarchy\n",
    "df = df.reindex(columns = df.columns.tolist() + ['Problem Hierarchy'])\n",
    "if levelNames:\n",
    "    for i in range(len(levelNames)):\n",
    "        df['Problem Hierarchy'] = df.apply(row_func_levels, args=(i, levelNames[i], levelColNames[i], ), axis=1)\n",
    "if convertToStudentStep:\n",
    "    #drop level columns \n",
    "    df.drop(levelColNames, axis=1, inplace=True)\n",
    "    logToWfl('Combine these Levels columns into Problem Hierarchy column: %s\\n' % (', '.join(levelColNames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn Condition Name, Condition Type columns to Condition column\n",
    "def row_func_condition(row):\n",
    "    type = row['Condition Type']\n",
    "    name = row['Condition Name']\n",
    "    if pd.isnull(name):\n",
    "        return ''\n",
    "    elif pd.isnull(type):\n",
    "        return name\n",
    "    else:\n",
    "        return '%s, %s'%(type, name)\n",
    "\n",
    "if convertToStudentStep:\n",
    "    allColNames = df.columns\n",
    "    condNameExist = 'Condition Name' in allColNames\n",
    "    condTypeExist = 'Condition Type' in allColNames   \n",
    "    if condNameExist and condTypeExist:\n",
    "        df = df.reindex(columns = df.columns.tolist() + ['Condition'])\n",
    "        df['Condition'] = df.apply(row_func_condition, axis=1)\n",
    "        #drop condition columns  \n",
    "        df.drop(['Condition Name', 'Condition Type'], axis=1, inplace=True)\n",
    "        logToWfl('Combine Condition Name and Condition Type columns into Condition column\\n')\n",
    "    elif (condNameExist and not condTypeExist) or (not condNameExist and condTypeExist):\n",
    "        errorMsg = \"Condition Name and Condition Type must be both present\"\n",
    "        ##write error to log file\n",
    "        logToWfl(errorMsg + ', Process aborted.\\n')\n",
    "        sys.exit(errorMsg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make dataframe for unique student+school+class\n",
    "#replace the student, school class columns with student_id\n",
    "studentUniqueColumns = ['Anon Student Id']\n",
    "if 'School' in allColNames:\n",
    "    studentUniqueColumns.append('School')\n",
    "if 'Class' in allColNames:\n",
    "    studentUniqueColumns.append('Class')\n",
    "studentUniqueDF = uniqueColumnsDF(studentUniqueColumns)\n",
    "df = pd.merge(df, studentUniqueDF,  how='left', on=studentUniqueColumns)\n",
    "if convertToStudentStep:\n",
    "    #drop original columns\n",
    "    df.drop(studentUniqueColumns, axis=1, inplace=True)\n",
    "df.rename(columns={'index': 'student_id'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make uniqueProblem(hierarchy+problem),\n",
    "#replace problem name with problem_id\n",
    "problemUniqueColumns = ['Problem Hierarchy', 'Problem Name']\n",
    "problemUniqueDF = uniqueColumnsDF(problemUniqueColumns)\n",
    "df = pd.merge(df, problemUniqueDF,  how='left', on=problemUniqueColumns)\n",
    "if convertToStudentStep:\n",
    "    #drop original columns\n",
    "    df.drop(problemUniqueColumns, axis=1, inplace=True)\n",
    "df.rename(columns={'index': 'problem_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make uniqueStep(problemId+step)\n",
    "#replace step name with step_id\n",
    "stepUniqueColumns = ['problem_id', 'Step Name']\n",
    "stepUniqueDF = uniqueColumnsDF(stepUniqueColumns, includeNullValue=False)\n",
    "df = pd.merge(df, stepUniqueDF,  how='left', on=stepUniqueColumns)\n",
    "if convertToStudentStep:\n",
    "    #drop original columns\n",
    "    df.drop(['Step Name'], axis=1, inplace=True)\n",
    "df.rename(columns={'index': 'step_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine KC (model), KC Category (model) to KC (model)\n",
    "def row_func_kc(row, KCColName, KCCategoryColName):\n",
    "    kcName = row[KCColName]\n",
    "    kcCategory = row[KCCategoryColName]\n",
    "    if pd.isnull(kcName):\n",
    "        return ''\n",
    "    elif pd.isnull(kcCategory):\n",
    "        return kcName\n",
    "    else:\n",
    "        #return '%s, %s'%(kcCategory, kcName)\n",
    "        #not going to combine KC category. this is behavior in DS too\n",
    "        return kcName\n",
    "\n",
    "allColNames = df.columns\n",
    "KCNames = []\n",
    "KCColNames = []\n",
    "uniqueSkillColumns = ['model', 'skill']\n",
    "uniqueSkillDF = pd.DataFrame(columns=uniqueSkillColumns)\n",
    "for colName in allColNames:\n",
    "    if 'KC (' in colName:\n",
    "        KCColName = colName\n",
    "        KCCategoryColName = colName.replace('KC (', 'KC Category (')\n",
    "        KCName = KCColName[len('KC (') : colName.find(')')]\n",
    "        KCNames.append(KCName)\n",
    "        KCColNames.append(KCColName)\n",
    "        #if this is for transaction output, \n",
    "        if not convertToStudentStep:\n",
    "            #save the original KC (model columns)\n",
    "            df[\"Original \" + KCColName] = df[KCColName]\n",
    "        if KCCategoryColName in allColNames:\n",
    "            if len(df[KCCategoryColName].value_counts()) > 0:\n",
    "                df[KCColName] = df.apply(row_func_kc, args=(KCColName, KCCategoryColName, ), axis=1)\n",
    "            if convertToStudentStep:\n",
    "                #drop category column \n",
    "                df.drop([KCCategoryColName], axis=1, inplace=True)\n",
    "        #make uniqueSkillDF\n",
    "        tempUniqueDF = uniqueColumnsDF([KCColName])\n",
    "        tempUniqueDF.drop(['index'], axis=1, inplace=True)\n",
    "        tempUniqueDF.rename(columns={KCColName: 'skill'}, inplace=True)\n",
    "        tempUniqueDF['model'] = KCName\n",
    "        #add sort=True for new pandas version\n",
    "        uniqueSkillDF = pd.concat([uniqueSkillDF, tempUniqueDF], sort=True)\n",
    "        \n",
    "uniqueSkillDF=uniqueSkillDF.drop_duplicates().reset_index()\n",
    "uniqueSkillDF.drop('index', axis=1, inplace=True)\n",
    "uniqueSkillDF.reset_index(inplace=True)\n",
    "\n",
    "#replace skill name with id\n",
    "df_pct = 0.1\n",
    "for i in range(len(KCNames)):\n",
    "    tempDF = uniqueSkillDF.loc[(uniqueSkillDF['model'] == KCNames[i])]\n",
    "    if not convertToStudentStep:\n",
    "        if i/len(KCNames) > df_pct:\n",
    "            logProgressToWfl(\"{:.0%}\".format(df_pct))\n",
    "            df_pct = df_pct + 0.1\n",
    "    if df[KCColNames[i]].dtypes != 'object':\n",
    "        df[KCColNames[i]] = df[KCColNames[i]].astype(str)\n",
    "    df = pd.merge(df, tempDF,  how='left', left_on=KCColNames[i], right_on='skill')\n",
    "    #drop old KC (model) column\n",
    "    df.drop([KCColNames[i], 'model', 'skill'], axis=1, inplace=True)\n",
    "    #rename index to KC (model)\n",
    "    df.rename(columns={'index': KCColNames[i]}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make uniqueSkillStep(skillId+StepId), later use for kc(model) columns\n",
    "uniqueSkillStepColumns = ['skill_id', 'step_id']\n",
    "uniqueSkillStepDF = pd.DataFrame(columns=uniqueSkillStepColumns)\n",
    "for i in range(len(KCNames)):\n",
    "    tempSkillStepDF = uniqueColumnsDF([KCColNames[i], 'step_id'])\n",
    "    tempSkillStepDF.drop(['index'], axis=1, inplace=True)\n",
    "    tempSkillStepDF.rename(columns={KCColNames[i]: 'skill_id'}, inplace=True)\n",
    "    uniqueSkillStepDF = pd.concat([uniqueSkillStepDF, tempSkillStepDF])\n",
    "\n",
    "uniqueSkillStepDF=uniqueSkillStepDF.drop_duplicates().reset_index()\n",
    "uniqueSkillStepDF.drop('index', axis=1, inplace=True)\n",
    "uniqueSkillStepDF.reset_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete all columns for KC to save space. the mapping info of model to skill are stored in uniqueSkillStepDF\n",
    "df.drop(KCColNames, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert outcome column values: \n",
    "#contains hint or help = hint; equals to “ok” or “correct” = correct; equals to “error” or “bug” or “incorrect” = “incorrect”; else =”unknown”\n",
    "#Outcome is not required\n",
    "if 'Outcome' in df.columns:\n",
    "    df['Outcome'] = df[\"Outcome\"].apply(lambda x: None if pd.isnull(x) else ('correct' if x.lower() in ['correct', 'ok'] else x))\n",
    "    df['Outcome'] = df[\"Outcome\"].apply(lambda x: None if pd.isnull(x) else ('incorrect' if x.lower() in ['incorrect', 'bug', 'error'] else x))\n",
    "    df['Outcome'] = df[\"Outcome\"].apply(lambda x: None if pd.isnull(x) else ('hint' if any(substring in x.lower() for substring in ['hint','help'])  else x))\n",
    "    df['Outcome'] = df[\"Outcome\"].apply(lambda x: None if pd.isnull(x) else ('unknown' if x.lower() not in ['hint','correct', 'incorrect']  else x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add new columns: prev_txn_time\n",
    "if not pvExist:\n",
    "    df = df.reindex(columns = df.columns.tolist() + [ 'Problem View', 'prev_txn_time'])\n",
    "if not pstExist:\n",
    "    df = df.reindex(columns = df.columns.tolist() + ['Problem Start Time', 'prev_txn_time'])\n",
    "if pvExist and pstExist:\n",
    "    df = df.reindex(columns = df.columns.tolist() + ['prev_txn_time'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set prev_txn_time, should come before problem event\n",
    "def row_func_prev_txn_time(row):\n",
    "    prev_student = row['Student shifted']\n",
    "    student = row['student_id']\n",
    "    prev_time = row['Time shifted']\n",
    "    if pd.isnull(prev_student) or pd.isnull(student) or pd.isnull(prev_time):\n",
    "        return None\n",
    "    elif prev_student != student:\n",
    "        return None\n",
    "    else:\n",
    "        return prev_time\n",
    "    \n",
    "df['Time shifted'] = df['Time'].shift(1)\n",
    "df['Student shifted'] = df['student_id'].shift(1)\n",
    "df['prev_txn_time'] = df.apply(row_func_prev_txn_time, axis=1)\n",
    "df.drop(['Time shifted'], axis=1, inplace=True)\n",
    "df.drop(['Student shifted'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handle problem_event\n",
    "uniqueProblemEventColumns = ['student_id', 'problem_id', 'Problem View', 'Problem Start Time']\n",
    "uniqueProblemEventDF = uniqueColumnsDF(uniqueProblemEventColumns)\n",
    "#don't need assigned index yet\n",
    "uniqueProblemEventDF.drop(['index'], axis=1, inplace=True)\n",
    "#make sure of order by student, problem, pv and pst\n",
    "uniqueProblemEventDF = uniqueProblemEventDF.sort_values(by=uniqueProblemEventColumns)\n",
    "#handle PV and PST empty values\n",
    "#go through each row of uniqueProblemEventDF\n",
    "for index, row in uniqueProblemEventDF.iterrows():\n",
    "    student = row['student_id']\n",
    "    problem = row['problem_id']\n",
    "    pv = row['Problem View']\n",
    "    pst = row['Problem Start Time']\n",
    "    if pd.isnull(pst) and not pd.isnull(pv):\n",
    "        #is there another record with same student, problem, pv and has pst, use it\n",
    "        otherRows = uniqueProblemEventDF.loc[(uniqueProblemEventDF['student_id'] == student) \n",
    "                                             & (uniqueProblemEventDF['problem_id'] == problem) \n",
    "                                             & (uniqueProblemEventDF['Problem View'] == pv) \n",
    "                                             & (uniqueProblemEventDF['Problem Start Time'].notnull())]\n",
    "        if not otherRows.empty:\n",
    "            pst = min(otherRows['Problem Start Time'])\n",
    "        else:\n",
    "            #find the min of prev_txn_time and Time from df with this student, problem and pv for pst\n",
    "            tempDF = df.loc[(df['student_id'] == student)\n",
    "                                              & (df['problem_id'] == problem)\n",
    "                                              & (df['Problem View'] == pv)]\n",
    "            min_prev_txn_time = min(tempDF['prev_txn_time'])\n",
    "            min_time = min(tempDF['Time'])\n",
    "            if pd.isnull(min_prev_txn_time):\n",
    "                pst = min_time\n",
    "            else:\n",
    "                pst = min([min_prev_txn_time, min_time])\n",
    "        #update uniqueProblemEventDF and df with new pst\n",
    "        uniqueProblemEventDF_row_ind = uniqueProblemEventDF[(uniqueProblemEventDF['student_id'] == student)\n",
    "                                              & (uniqueProblemEventDF['problem_id'] == problem)\n",
    "                                              & (uniqueProblemEventDF['Problem View'] == pv)\n",
    "                                              & (uniqueProblemEventDF['Problem Start Time'].isnull())].index.tolist()\n",
    "        uniqueProblemEventDF.loc[uniqueProblemEventDF_row_ind, 'Problem Start Time'] = pst\n",
    "        df_row_ind = df[(df['student_id'] == student) \n",
    "                                              & (df['problem_id'] == problem) \n",
    "                                              & (df['Problem View'] == pv)\n",
    "                                           & (df['Problem Start Time'].isnull())].index.tolist()\n",
    "        df.loc[df_row_ind, 'Problem Start Time'] = pst\n",
    "    elif pd.isnull(pst) and pd.isnull(pv):\n",
    "        #set pv =1 \n",
    "        #find the min of prev_txn_time and Time from df with this student, problem for pst\n",
    "        tempDF = df.loc[(df['student_id'] == student)\n",
    "                                              & (df['problem_id'] == problem)]\n",
    "        min_prev_txn_time = min(tempDF['prev_txn_time'])\n",
    "        min_time = min(tempDF['Time'])\n",
    "        if pd.isnull(min_prev_txn_time):\n",
    "            pst = min_time\n",
    "        else:\n",
    "            pst = min([min_prev_txn_time, min_time])\n",
    "        pv = 1\n",
    "        #update uniqueProblemEventDF and df with new pst\n",
    "        uniqueProblemEventDF_row_ind = uniqueProblemEventDF[(uniqueProblemEventDF['student_id'] == student)\n",
    "                                              & (uniqueProblemEventDF['problem_id'] == problem)\n",
    "                                              & (uniqueProblemEventDF['Problem View'].isnull())\n",
    "                                              & (uniqueProblemEventDF['Problem Start Time'].isnull())].index.tolist()\n",
    "        uniqueProblemEventDF.loc[uniqueProblemEventDF_row_ind, 'Problem Start Time'] = pst\n",
    "        uniqueProblemEventDF.loc[uniqueProblemEventDF_row_ind, 'Problem View'] = pv\n",
    "        df_row_ind = df[(df['student_id'] == student) \n",
    "                                              & (df['problem_id'] == problem) \n",
    "                                              & (df['Problem View'].isnull())\n",
    "                                           & (df['Problem Start Time'].isnull())].index.tolist()\n",
    "        df.loc[df_row_ind, 'Problem Start Time'] = pst\n",
    "        df.loc[df_row_ind, 'Problem View'] = pv\n",
    "    elif pd.isnull(pv) and not pd.isnull(pst):\n",
    "        #is there another record with same student, problem, pst and has pv, use it\n",
    "        otherRows = uniqueProblemEventDF.loc[(uniqueProblemEventDF['student_id'] == student) \n",
    "                                             & (uniqueProblemEventDF['problem_id'] == problem) \n",
    "                                             & (uniqueProblemEventDF['Problem Start Time'] == pst) \n",
    "                                             & (uniqueProblemEventDF['Problem View'].notnull())]\n",
    "        if not otherRows.empty:\n",
    "            pv = min(otherRows['Problem View'])\n",
    "        else:\n",
    "            #find the pv from uniqueProblemEventDF with this student, problem and pst less \n",
    "            tempDF = uniqueProblemEventDF.loc[(uniqueProblemEventDF['student_id'] == student)\n",
    "                                              & (uniqueProblemEventDF['problem_id'] == problem)\n",
    "                                              & (uniqueProblemEventDF['Problem Start Time'] < pst)]\n",
    "            if tempDF.empty:\n",
    "                pv = 1\n",
    "            else:\n",
    "                pv = max(tempDF['Problem View']) + 1\n",
    "            \n",
    "        #update uniqueProblemEventDF and df with new pst\n",
    "        uniqueProblemEventDF_row_ind = uniqueProblemEventDF[(uniqueProblemEventDF['student_id'] == student)\n",
    "                                              & (uniqueProblemEventDF['problem_id'] == problem)\n",
    "                                              & (uniqueProblemEventDF['Problem Start Time'] == pst)\n",
    "                                              & (uniqueProblemEventDF['Problem View'].isnull())].index.tolist()\n",
    "        uniqueProblemEventDF.loc[uniqueProblemEventDF_row_ind, 'Problem View'] = pv\n",
    "        df_row_ind = df[(df['student_id'] == student) \n",
    "                                              & (df['problem_id'] == problem) \n",
    "                                              & (df['Problem Start Time'] == pst)\n",
    "                                           & (df['Problem View'].isnull())].index.tolist()\n",
    "        df.loc[df_row_ind, 'Problem View'] = pv\n",
    "\n",
    "#drop duplicates introduced by problem-event handling empty pv and pst\n",
    "uniqueProblemEventDF=uniqueProblemEventDF.drop_duplicates()\n",
    "uniqueProblemEventDF.reset_index(inplace=True)\n",
    "#bit excessive, don't really need reset index, but after this, index is in numeric order\n",
    "uniqueProblemEventDF.drop(['index'], axis=1, inplace=True)\n",
    "uniqueProblemEventDF.reset_index(inplace=True)\n",
    "#merge with df\n",
    "df = pd.merge(df, uniqueProblemEventDF, how='left', on=uniqueProblemEventColumns)\n",
    "df.rename(columns={'index': 'problem_event_id'}, inplace=True)\n",
    "#at this point problem start time, problem view, problem_event_id, prev_txn_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "#populate 'attempt_at_subgoal' and 'is_last_attempt'\n",
    "\n",
    "df['Attempt At Step'] = df.groupby(['student_id','problem_event_id','step_id']).cumcount()+1\n",
    "#is_last_attempt is at step level not at problem_event level\n",
    "df.loc[df.groupby(['student_id','step_id'])[\"Time\"].idxmax(), 'Is Last Attempt']=1\n",
    "df.loc[df['Is Last Attempt'].isnull(), 'Is Last Attempt'] = 0\n",
    "#step the rows that have no step\n",
    "df.loc[df['step_id'].isnull(), 'Attempt At Step'] = np.nan\n",
    "df.loc[df['step_id'].isnull(), 'Is Last Attempt'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute duration\n",
    "#if prob_start_time is null and prev_tx_time is null, null\n",
    "#problem_start_time will never be null\n",
    "#if prob_start_time is null and prev_tx_time is not null and prev_tx_time and transaction_time is larger than 10 min, null\n",
    "#if prob_start_time is null and prev_tx_time is not null and prev_tx_time and transaction_time is less than 10 min, use diff*1000\n",
    "#if prev_tx_time is null, duration is set to null bc it's begining of a new student\n",
    "#if prob_start_time is not null and prev_tx_time is not null, use the greatest one of the two to compute with transaction time, same logic with 10 min cut off\n",
    "def row_func_duration(row):\n",
    "    prev_txn_time = row['prev_txn_time']\n",
    "    txn_time = row['Time']\n",
    "    pst = row['Problem Start Time']\n",
    "    if pd.isnull(prev_txn_time):\n",
    "        return None\n",
    "    elif pd.isnull(pst):\n",
    "        delta = (int(pd.to_datetime(txn_time).value / 1000000) - int(pd.to_datetime(prev_txn_time).value / 1000000))/1000\n",
    "        if delta < 0 or delta > 600:\n",
    "            return None\n",
    "        else:\n",
    "            return delta\n",
    "    else:\n",
    "        delta1 = (int(pd.to_datetime(txn_time).value / 1000000) - int(pd.to_datetime(prev_txn_time).value / 1000000))/1000\n",
    "        delta2 = (int(pd.to_datetime(txn_time).value / 1000000) - int(pd.to_datetime(pst).value / 1000000))/1000\n",
    "        delta = None\n",
    "        if (delta1 < 0 or delta1 > 600) and (delta2 >= 0 and delta2 <= 600):\n",
    "            delta = delta2\n",
    "        elif (delta1 >= 0 and delta1 <=600) and (delta2 < 0 or delta2 >600):\n",
    "            delta = delta1\n",
    "        elif (delta1 >= 0 and delta1 <=600) and (delta2 >= 0 and delta2 <= 600):\n",
    "            delta = delta1 if delta1 < delta2 else delta2\n",
    "        return delta\n",
    "df['duration'] = df.apply(row_func_duration, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#handle identical txn with identical timestamp\n",
    "df_identical_timestamps = df.groupby(['student_id', 'problem_id', 'Time']).size().reset_index(name='counts')\n",
    "df_identical_timestamps = df_identical_timestamps[df_identical_timestamps['counts'] >1]\n",
    "for index, row in df_identical_timestamps.iterrows():\n",
    "    student = row['student_id']\n",
    "    problem = row['problem_id']\n",
    "    Time = row['Time']\n",
    "    cnt = row['counts']\n",
    "    df_rows_ind = df[(df['student_id'] == student) & (df['problem_id'] == problem) & (df['Time'] == Time)].index.tolist()\n",
    "    df.loc[df_rows_ind, 'duration'] = sum(df.loc[df_rows_ind, 'duration'])/cnt\n",
    "    #set all txn to the same is_last_attempt if there is one equals to 1\n",
    "    df.loc[df_rows_ind, 'Is Last Attempt'] = max(df.loc[df_rows_ind, 'Is Last Attempt'])\n",
    "    #set all txn to the same attempt_at_step with the min of all rows with these conditions\n",
    "    #this will cause bug bc if there are other attempts that come after this txn time, the number is not right\n",
    "    df.loc[df_rows_ind, 'Attempt At Step'] = min(df.loc[df_rows_ind, 'Attempt At Step'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not convertToStudentStep:\n",
    "    #change duration to Duration (sec)\n",
    "    df['Duration (sec)'] = df['duration']\n",
    "    #drop unnecessary columns\n",
    "    df.drop(['student_id','problem_id','step_id','Problem Hierarchy', 'problem_event_id', 'prev_txn_time','duration'], axis=1, inplace=True)\n",
    "    if 'Original Time' in df.columns:\n",
    "        df['Time'] = df['Original Time']\n",
    "        df.drop(['Original Time'], axis=1, inplace=True)\n",
    "    if 'Original Problem Start Time' in df.columns:\n",
    "        df['Problem Start Time'] = df['Original Problem Start Time']\n",
    "        df.drop(['Original Problem Start Time'], axis=1, inplace=True)\n",
    "    #change Original KC(model) to KC(model) and put them pack to the right place\n",
    "    allColNames = df.columns\n",
    "    for KCColName in KCColNames:\n",
    "        df.rename(columns={'Original '+KCColName:KCColName}, inplace=True)\n",
    "    \n",
    "    allColNames = df.columns\n",
    "    orderedColumnNames = []\n",
    "    for colName in allColNames:\n",
    "        if colName.find('KC Category (') == -1 and colName.find('KC (') == -1:\n",
    "            if not pvExist:\n",
    "                if colName == 'Problem Start Time':\n",
    "                    orderedColumnNames.append('Problem View')\n",
    "                    orderedColumnNames.append('Problem Start Time')\n",
    "                elif colName != 'Problem View':\n",
    "                    orderedColumnNames.append(colName)\n",
    "            elif not pstExist:\n",
    "                if colName == 'Problem View':\n",
    "                    orderedColumnNames.append('Problem View')\n",
    "                    orderedColumnNames.append('Problem Start Time')\n",
    "                elif colName != 'Problem Start Time':\n",
    "                    orderedColumnNames.append(colName)\n",
    "            else:\n",
    "                orderedColumnNames.append(colName)\n",
    "        else:\n",
    "            if colName.find('KC Category (') != -1:\n",
    "                KCName = colName[len('KC Category (') : colName.find(')')]\n",
    "                KCColName = colName.replace('KC Category (', 'KC (')\n",
    "                orderedColumnNames.append(KCColName)\n",
    "                orderedColumnNames.append(colName)\n",
    "    df = df[orderedColumnNames]\n",
    "    df.to_csv('transaction.txt', sep='\\t', index=False)\n",
    "    sys.exit()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make student_step roll up table with group by student_id, problem_id, step_id, problem_view\n",
    "rollupColumns = ['student_id', 'problem_id', 'step_id', 'problem_event_id', 'Problem View']\n",
    "# functions to get counts for hint, corrects and incorrects\n",
    "def cnt_correct(rows):\n",
    "    return(rows[rows == 'correct'].count())\n",
    "def cnt_incorrect(rows):\n",
    "    return(rows[rows == 'incorrect'].count())\n",
    "def cnt_hint(rows):\n",
    "    return(rows[rows == 'hint'].count())\n",
    "\n",
    "#functions to get correct_tx_time: min(transacion_time) correct_flag=correct \n",
    "def func_correct_txn_time(rows):\n",
    "    return rows[rows['Outcome'] == 'correct']['Time'].min()\n",
    "\n",
    "if 'Outcome' not in df.columns:\n",
    "    #get step_end_time, problem_event_time, first_transaction_time, corrects, incorrects, hints, step_duration, \n",
    "    df_rollup = df.groupby(rollupColumns).agg({'Problem Start Time':'min','Time':['min','max'], 'duration':'sum' }).reset_index()\n",
    "    df_rollup.columns = [' '.join(col).strip() for col in df_rollup.columns.values]\n",
    "    df_rollup.rename(columns={'Time min':'first_transaction_time', 'Time max':'step_end_time', 'Problem Start Time min':'problem_event_time', 'duration sum':'step_duration'}, inplace=True)\n",
    "else:\n",
    "    #drop rows that has None in Outcome, \n",
    "    df.dropna(axis=0, subset=['Outcome'])\n",
    "    #get step_end_time, problem_event_time, first_transaction_time, corrects, incorrects, hints, step_duration, \n",
    "    df_rollup = df.groupby(rollupColumns).agg({'Problem Start Time':'min','Time':['min','max'], 'Outcome':[cnt_correct, cnt_incorrect, cnt_hint], 'duration':'sum' }).reset_index()\n",
    "    df_rollup.columns = [' '.join(col).strip() for col in df_rollup.columns.values]\n",
    "    df_rollup.rename(columns={'Time min':'first_transaction_time', 'Time max':'step_end_time', 'Problem Start Time min':'problem_event_time', 'Outcome cnt_correct':'corrects', 'Outcome cnt_incorrect':'incorrects', 'Outcome cnt_hint':'hints', 'duration sum':'step_duration'}, inplace=True)\n",
    "\n",
    "    #set correct_txn_time\n",
    "    df_rollup_2= df.groupby(rollupColumns).apply(func_correct_txn_time).reset_index()\n",
    "    df_rollup_2.rename(columns={0: 'correct_transaction_time'}, inplace=True)\n",
    "    df_rollup = pd.merge(df_rollup, df_rollup_2,  how='left', on=rollupColumns)\n",
    "\n",
    "\n",
    "#get the rows that has the min transaction time, and use it for first_attempt, prev_txn_time and condition\n",
    "#prev_txn_time, should come before problem event time \n",
    "min_time_rows = df.groupby(rollupColumns)[\"Time\"].idxmin()\n",
    "if 'Outcome' not in df.columns:\n",
    "    if 'Condition' not in df.columns:\n",
    "        min_time_attempt_prev_txn = df.loc[min_time_rows, ['student_id', 'problem_id', 'step_id', 'problem_event_id', 'Problem View', 'prev_txn_time']]\n",
    "    else:\n",
    "        min_time_attempt_prev_txn = df.loc[min_time_rows, ['student_id', 'problem_id', 'step_id', 'problem_event_id', 'Problem View', 'prev_txn_time', 'Condition']]\n",
    "    df_rollup = pd.merge(df_rollup, min_time_attempt_prev_txn,  how='left', on=rollupColumns)\n",
    "else:\n",
    "    if 'Condition' not in df.columns:\n",
    "        min_time_attempt_prev_txn = df.loc[min_time_rows, ['student_id', 'problem_id', 'step_id', 'problem_event_id', 'Problem View', 'Outcome', 'prev_txn_time']]\n",
    "    else:\n",
    "        min_time_attempt_prev_txn = df.loc[min_time_rows, ['student_id', 'problem_id', 'step_id', 'problem_event_id', 'Problem View', 'Outcome', 'prev_txn_time', 'Condition']]\n",
    "    df_rollup = pd.merge(df_rollup, min_time_attempt_prev_txn,  how='left', on=rollupColumns)\n",
    "    df_rollup.rename(columns={'Outcome':'first_attempt'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the rows that has the min transaction time, and use it for event type\n",
    "#prev_txn_time, should come before problem event time \n",
    "if 'Event Type' in df.columns:\n",
    "    min_time_event_type_txn = df.loc[min_time_rows, ['student_id', 'problem_id', 'step_id', 'problem_event_id', 'Problem View', 'Event Type']]\n",
    "    df_rollup = pd.merge(df_rollup, min_time_event_type_txn,  how='left', on=rollupColumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute step_start_time\n",
    "#three related times: problem_event_time, prev_txn_time, first_txn_time\n",
    "#if only one of is not null, use it for step_start_time \n",
    "#if problem_event_time is null, set step_start_time to prev_tx_time (bc it should be earlier than first txn_time)\n",
    "#if prev_tx_time is null, use min of problem_event_time and earliest_txn_time\n",
    "#if prev_tx_time is later than problem_event_time, take prev_tx_time else use problem_event_time\n",
    "#Calculate time difference between step_start_time and earliest_tx_time. If larger than 10 min, set step_start_time to null\n",
    "\n",
    "def row_func_step_start_time(row):\n",
    "    prev_txn_time = row['prev_txn_time']\n",
    "    problem_event_time = row['problem_event_time']\n",
    "    first_txn_time = row['first_transaction_time']\n",
    "    step_start_time = None\n",
    "    if pd.notnull(problem_event_time) and pd.isnull(prev_txn_time) and pd.isnull(first_txn_time):\n",
    "        step_start_time = problem_event_time\n",
    "    elif pd.isnull(problem_event_time) and pd.notnull(prev_txn_time) and pd.isnull(first_txn_time):\n",
    "        step_start_time = prev_txn_time\n",
    "    elif pd.isnull(problem_event_time) and pd.isnull(prev_txn_time) and pd.notnull(first_txn_time):\n",
    "        step_start_time = first_txn_time\n",
    "    elif pd.isnull(problem_event_time) and pd.notnull(prev_txn_time) and pd.notnull(first_txn_time):\n",
    "        step_start_time = prev_txn_time\n",
    "    elif pd.notnull(problem_event_time) and pd.isnull(prev_txn_time) and pd.notnull(first_txn_time):\n",
    "        step_start_time = min([problem_event_time, first_txn_time])\n",
    "    elif pd.notnull(problem_event_time) and pd.notnull(prev_txn_time) and pd.isnull(first_txn_time):\n",
    "        step_start_time = max([problem_event_time, prev_txn_time])\n",
    "    elif pd.notnull(problem_event_time) and pd.notnull(prev_txn_time) and pd.notnull(first_txn_time):\n",
    "        step_start_time = max([problem_event_time, prev_txn_time])\n",
    "    \n",
    "    delta = (int(pd.to_datetime(step_start_time).value / 1000000) - int(pd.to_datetime(first_txn_time).value / 1000000))/1000\n",
    "    if delta > 600:\n",
    "        step_start_time = None\n",
    "    return step_start_time\n",
    "\n",
    "df_rollup['step_start_time'] = df_rollup.apply(row_func_step_start_time, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate correct_step_duration and error_step_duration and modif step_duration\n",
    "#set step_duration to null if step_start_time is null; \n",
    "#set correct_step_duration to step_duration if first_attempt is correct; set error_step_duration to step_duration if first_attempt is not correct\n",
    "def row_func_step_duration(row):\n",
    "    if pd.isnull(row['step_start_time']):\n",
    "        return None\n",
    "    else:\n",
    "        return row['step_duration']\n",
    "def row_func_correct_step_duration(row):\n",
    "    if row['first_attempt'] == 'correct':\n",
    "        return row['step_duration']\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def row_func_error_step_duration(row):\n",
    "    if row['first_attempt'] != 'correct':\n",
    "        return row['step_duration']\n",
    "    else:\n",
    "        return None\n",
    "        \n",
    "df_rollup['step_duration'] = df_rollup.apply(row_func_step_duration, axis=1)\n",
    "if 'Outcome' in df.columns:\n",
    "    df_rollup['correct_step_duration'] = df_rollup.apply(row_func_correct_step_duration, axis=1)\n",
    "    df_rollup['error_step_duration'] = df_rollup.apply(row_func_error_step_duration, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#before compute KC and opportunitie, reorder by student, first_transaction_time, step_start_time\n",
    "df_rollup = df_rollup.sort_values(by=['student_id', 'first_transaction_time', 'step_start_time', 'problem_id', 'Problem View'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sameRow(row1, row2):\n",
    "    if row1 is not None:\n",
    "        row1 = list(row1)\n",
    "    if row2 is not None:\n",
    "        row2 = list(row2)\n",
    "    if row1 is None and row2 is None:\n",
    "        return True\n",
    "    elif row1 is None and row2 is not None:\n",
    "        return False\n",
    "    elif row1 is not None and row2 is None:\n",
    "        return False\n",
    "    elif len(row1) != len(row2):\n",
    "        return False\n",
    "    else:\n",
    "        for i in range(len(row1)):\n",
    "            if row1[i] is None and row2[i] is None:\n",
    "                continue\n",
    "            elif pd.isnull(row1[i]) and pd.isnull(row2[i]):\n",
    "                continue\n",
    "            elif pd.isna(row1[i]) and pd.isna(row2[i]):\n",
    "                continue\n",
    "            elif row1[i] != row2[i]:\n",
    "                return False\n",
    "        return True\n",
    "                \n",
    "        \n",
    "    \n",
    "#compute KC skills and opportunity for each row. Use skill-student map to keep the highest opportunity count\n",
    "#def row_func_kc_opp(step_id, student_id, modelName, skillsSubDF):\n",
    "def row_func_kc_opp(row, modelName, skillsSubDF):\n",
    "    global lastRow\n",
    "    global lastStudentId\n",
    "    global studentSkillDic\n",
    "    global df_rollup_cnt\n",
    "    global df_rollup_pct\n",
    "    step_id = row['step_id']\n",
    "    student_id = row['student_id']\n",
    "    event_type = \"\"\n",
    "    if 'Event Type' in row.keys() and not pd.isnull(row['Event Type']):\n",
    "        event_type = row['Event Type']\n",
    "        \n",
    "    incrementOpportunity = True\n",
    "    if event_type != \"\" and 'instruct' not in event_type:\n",
    "        incrementOpportunity = False\n",
    "      \n",
    "    totalCnt = len(allModels) * len(df_rollup)\n",
    "    #get all skills for this step\n",
    "    skills = skillsSubDF[skillsSubDF['step_id']==step_id][['skill', 'skill_id']]\n",
    "    skills = skills[pd.notnull(skills['skill'])]\n",
    "    #exclude skill that is empty\n",
    "    skills = skills[skills['skill']!='']\n",
    "    skills = skills.sort_values(by='skill')\n",
    "    \n",
    "    \n",
    "    skillsStr = '~~'.join(skills['skill'].astype(str))\n",
    "    #because df_rollup isorded by student, we can refresh map for a new student\n",
    "    if lastStudentId != student_id:\n",
    "        studentSkillDic = {}\n",
    "    lastStudentId = student_id\n",
    "    \n",
    "    opportunitiesStr = ''\n",
    "    if incrementOpportunity:\n",
    "        for skill in skills['skill']:\n",
    "            oppForSkill = 1\n",
    "            if not sameRow(lastRow, row) and skill in studentSkillDic:\n",
    "                oppForSkill = studentSkillDic[skill] + 1\n",
    "            studentSkillDic[skill] = oppForSkill\n",
    "            if opportunitiesStr == '':\n",
    "                opportunitiesStr = str(oppForSkill)\n",
    "            else:\n",
    "                opportunitiesStr = opportunitiesStr + \"~~\" + str(oppForSkill)\n",
    "    else:\n",
    "        for skill in skills['skill']:\n",
    "            oppForSkill = 0\n",
    "            if not sameRow(lastRow, row) and skill in studentSkillDic:\n",
    "                oppForSkill = studentSkillDic[skill]\n",
    "            studentSkillDic[skill] = oppForSkill\n",
    "            if opportunitiesStr == '':\n",
    "                opportunitiesStr = str(oppForSkill)\n",
    "            else:\n",
    "                opportunitiesStr = opportunitiesStr + \"~~\" + str(oppForSkill)\n",
    "    lastRow = copy.copy(row)\n",
    "    df_rollup_cnt = df_rollup_cnt + 1\n",
    "    if df_rollup_cnt/totalCnt > df_rollup_pct:\n",
    "        logProgressToWfl(\"{:.0%}\".format(df_rollup_pct))\n",
    "        df_rollup_pct = df_rollup_pct + 0.1\n",
    "    return pd.Series((skillsStr, opportunitiesStr, \"\"))\n",
    "\n",
    "uniqueSkillStepDF[\"skill_id\"] = pd.to_numeric(uniqueSkillStepDF[\"skill_id\"])\n",
    "joinSkillStep = pd.merge(uniqueSkillStepDF, uniqueSkillDF,  how='left', left_on='skill_id', right_on='index')\n",
    "\n",
    "allModels = uniqueSkillDF['model'].drop_duplicates().sort_values()\n",
    "allKCRelatedColumns = []\n",
    "#lastStudentId = -1\n",
    "#studentSkillDic = {}\n",
    "#opportunitiesStr = ''\n",
    "df_rollup_cnt = 0\n",
    "df_rollup_pct = 0.1\n",
    "lastRow = None\n",
    "for model in allModels:\n",
    "    lastStudentId = -1\n",
    "    studentSkillDic = {}\n",
    "    opportunitiesStr = ''\n",
    "    newKCColumn = 'KC (' + model + ')'\n",
    "    newOppColumn = 'Opportunity (' + model + ')'\n",
    "    newPredErrColumn = 'Predicted Error Rate (' + model + ')'\n",
    "    allKCRelatedColumns.append(newKCColumn)\n",
    "    allKCRelatedColumns.append(newOppColumn)\n",
    "    allKCRelatedColumns.append(newPredErrColumn)\n",
    "    df_rollup = df_rollup.reindex(columns = df_rollup.columns.tolist() + [newKCColumn, newOppColumn])\n",
    "    skillsSubDF = joinSkillStep[joinSkillStep['model']==model][['step_id', 'skill', 'skill_id']]\n",
    "    #df_rollup[[newKCColumn, newOppColumn]] = df_rollup.apply(lambda row: row_func_kc_opp(row['step_id'], row['student_id'], model, skillsSubDF), axis=1)\n",
    "    df_rollup[[newKCColumn, newOppColumn, newPredErrColumn]] = df_rollup.apply(row_func_kc_opp, args = (model, skillsSubDF,), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put student, school, class back\n",
    "df_rollup = pd.merge(df_rollup, studentUniqueDF,  how='left', left_on=\"student_id\", right_on=\"index\")\n",
    "df_rollup.drop(['index','student_id'], axis=1, inplace=True)\n",
    "#put problem hierarch and problem name back\n",
    "df_rollup = pd.merge(df_rollup, problemUniqueDF,  how='left', left_on=\"problem_id\", right_on=\"index\")\n",
    "df_rollup.drop(['index','problem_id'], axis=1, inplace=True)\n",
    "#put step back\n",
    "df_rollup = pd.merge(df_rollup, stepUniqueDF,  how='left', left_on=\"step_id\", right_on=\"index\")\n",
    "df_rollup.drop(['index','problem_id', 'step_id'], axis=1, inplace=True)\n",
    "#drop problem_event_id\n",
    "df_rollup.drop(['problem_event_id', 'problem_event_time'], axis=1, inplace=True)\n",
    "\n",
    "#reorder columns\n",
    "orderedColumnNames = None\n",
    "if 'Outcome' not in df.columns:\n",
    "    orderedColumnNames = ['Anon Student Id', 'Problem Hierarchy', 'Problem Name', 'Problem View',  'Step Name', 'step_start_time', 'first_transaction_time', 'step_end_time', 'step_duration']\n",
    "else:\n",
    "    orderedColumnNames = ['Anon Student Id', 'Problem Hierarchy', 'Problem Name', 'Problem View',  'Step Name', 'step_start_time', 'first_transaction_time', 'correct_transaction_time', 'step_end_time', 'step_duration', 'correct_step_duration', 'error_step_duration', 'first_attempt', 'incorrects', 'hints', 'corrects']\n",
    "\n",
    "if 'Condition Name' in originalAllColNames:\n",
    "    orderedColumnNames = orderedColumnNames + ['Condition']\n",
    "if 'School' in originalAllColNames:\n",
    "    orderedColumnNames = orderedColumnNames + ['School']\n",
    "if 'Class' in originalAllColNames:\n",
    "    orderedColumnNames = orderedColumnNames + ['Class']\n",
    "df_rollup = df_rollup[orderedColumnNames + allKCRelatedColumns]\n",
    "if 'Outcome' not in df.columns:\n",
    "    df_rollup.rename(columns={'step_start_time':'Step Start Time', 'first_transaction_time':'First Transaction Time', 'step_end_time':'Step End Time', 'step_duration':'Step Duration (sec)'}, inplace=True)\n",
    "else:\n",
    "    df_rollup.rename(columns={'step_start_time':'Step Start Time', 'first_transaction_time':'First Transaction Time', 'correct_transaction_time':'Correct Transaction Time', 'step_end_time':'Step End Time', 'step_duration':'Step Duration (sec)', 'correct_step_duration':'Correct Step Duration (sec)', 'error_step_duration':'Error Step Duration (sec)', 'first_attempt':'First Attempt', 'incorrects':'Incorrects', 'hints':'Hints', 'corrects':'Corrects'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#close all output files and finish!!\n",
    "df_rollup.to_csv('studentStepRollup.txt', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
