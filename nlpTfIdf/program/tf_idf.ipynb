{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "from io import StringIO\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import zipfile\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from tika import parser # pip install tika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the function to remove punctuation and extra space\n",
    "def remove_punctuation(text):\n",
    "    if(type(text)==float):\n",
    "        return text\n",
    "    #remove punctuation\n",
    "    ans_no_punct = \"\"\n",
    "    for i in text:\n",
    "        if i not in string.punctuation:\n",
    "            ans_no_punct += i\n",
    "    #remove the extra space\n",
    "    ans_no_extra_space = \"\"\n",
    "    prev_char = \"\"\n",
    "    for i in ans_no_punct:\n",
    "        if i.strip() != \"\" or ( i.strip() == \"\" and prev_char.strip() != \"\"):\n",
    "            ans_no_extra_space += i\n",
    "        prev_char = i  \n",
    "    return ans_no_extra_space.strip()\n",
    "#remove_punctuation(\"Goodwill and other intangible assets account for some 2.0 mln euro ( $ 2.6 mln ) of the purchase price , 20 pct of which payable in Aspo shares .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to generate n-grams:\n",
    "#params:\n",
    "#text-the text for which we have to generate n-grams\n",
    "#ngram-number of grams to be generated from the text(1,2,3,4 etc., default value=1)\n",
    "def generate_N_grams(text, ngram=1, exclude_stopwords=True):\n",
    "    words = []\n",
    "    if exclude_stopwords:\n",
    "        words=[word for word in text.split(\" \") if word not in STOPWORDS] \n",
    "    else:\n",
    "        words=[word for word in text.split(\" \") if word not in []] \n",
    "    temp=zip(*[words[i:] for i in range(0,ngram)])\n",
    "    all_combinations = [' '.join(ngram) for ngram in temp]\n",
    "    all_combination_count = defaultdict(int)\n",
    "    for combination in all_combinations:\n",
    "        all_combination_count[combination] += 1\n",
    "    return all_combination_count\n",
    "#generate_N_grams(remove_punctuation(\"Goodwill and other intangible Goodwill and other intangible assets account for some 2.0 mln euro ( $ 2.6 mln ) of the purchase price , 20 pct of which payable in Aspo shares .\"), 2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanStopWords(text):\n",
    "    words = []\n",
    "    words=[word for word in text.split(\" \") if word not in STOPWORDS] \n",
    "    return \" \".join(words)\n",
    "#print(cleanStopWords(\"and al and the alds\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logToWfl(msg):\n",
    "    logFile = open(\"tf_idf.wfl\", \"a\")\n",
    "    now = dt.datetime.now()\n",
    "    logFile.write(str(now) + \": \" + msg + \"\\n\");\n",
    "    logFile.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test on command line\n",
    "#\"C:/ProgramData/Anaconda3/Python\" tf_idf.py -programDir . -workingDir . -userId 1 -exclude_stopwords Yes -term \"good job\" -term_type \"Phrase\" -text_column_nodeIndex 0 -text_column_fileIndex 0 -text_column answer -text_column_nodeIndex 0 -text_column_fileIndex 0 -text_column answer_2 -text_corpus \"Columns in a file\" -node 0 -fileIndex 0 user_comment.txt       \n",
    "#\"C:/ProgramData/Anaconda3/Python\" tf_idf.py -programDir . -workingDir . -userId 1 -exclude_stopwords Yes -term \"good job\" -term_type \"Phrase\" -text_column_nodeIndex 0 -text_column_fileIndex 0 -text_column answer -text_column_nodeIndex 0 -text_column_fileIndex 0 -text_column answer_2 -text_corpus \"Files\" -node 0 -fileIndex 0 praises.zip      \n",
    "#all variables\n",
    "command_line = False\n",
    "working_dir = \"\"\n",
    "program_dir = \"\"\n",
    "file_name = \"\"\n",
    "text_corpus = \"\"\n",
    "text_column = \"\"\n",
    "term_type = \"\"\n",
    "exclude_stopwords = \"Yes\"\n",
    "term = \"\"\n",
    "   \n",
    "#command line     \n",
    "if command_line:\n",
    "    arg_parser = argparse.ArgumentParser(description='Python program for NLP TFIDF.')\n",
    "    arg_parser.add_argument('-programDir', type=str, help='the component program directory')\n",
    "    arg_parser.add_argument('-workingDir', type=str, help='the component instance working directory')\n",
    "    arg_parser.add_argument(\"-node\", nargs=1, action='append')\n",
    "    arg_parser.add_argument(\"-fileIndex\", nargs=2, action='append')\n",
    "    arg_parser.add_argument(\"-text_corpus\", type=str, choices=[\"Columns in a file\", \"Files\"], default=\"File\")\n",
    "    arg_parser.add_argument(\"-text_column\", type=str, action='append')\n",
    "    arg_parser.add_argument(\"-exclude_stopwords\", type=str, choices=[\"Yes\", \"No\"], default=\"Yes\")\n",
    "    arg_parser.add_argument(\"-term_type\", type=str, choices=[\"Word\", \"Phrase\"], default=\"Word\")\n",
    "    arg_parser.add_argument(\"-term\", type=str)\n",
    "    args, option_file_index_args = arg_parser.parse_known_args()\n",
    "    #var for both text corpus\n",
    "    working_dir = args.workingDir\n",
    "    program_dir = args.programDir\n",
    "    file_name = args.fileIndex[0][1]\n",
    "    text_corpus = args.text_corpus\n",
    "    #for Columns in a file\n",
    "    if text_corpus == \"Columns in a file\":\n",
    "        text_column = args.text_column\n",
    "    exclude_stopwords = args.exclude_stopwords\n",
    "    if exclude_stopwords == \"No\":\n",
    "        exclude_stopwords = False\n",
    "    else:\n",
    "        exclude_stopwords = True\n",
    "    term_type = args.term_type\n",
    "    term = args.term \n",
    "else: #for testing\n",
    "    working_dir = \".\"\n",
    "    program_dir = \".\"\n",
    "    #file_name = \"user_comment.txt\"\n",
    "    file_name = \"praises.zip\"\n",
    "    #text_corpus = \"Columns in a file\"\n",
    "    text_corpus = \"Files\"\n",
    "    text_column = ['answer', 'answer_2']\n",
    "    term_type = \"Phrase\"\n",
    "    exclude_stopwords = True\n",
    "    term = \"good job\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          file    tf_idf\n",
      "0             praiseA_colL.txt  0.002762\n",
      "1             praiseA_colQ.txt  0.000000\n",
      "2             praiseA_colV.txt  0.000000\n",
      "3             praiseA_colZ.txt  0.000000\n",
      "4  subfolder\\praiseA_colAC.txt  0.006689\n",
      "5  subfolder\\praiseA_colAH.txt  0.000000\n",
      "6\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#modify search word\n",
    "search_text = \"\"\n",
    "n_grams = 1\n",
    "if term_type == \"Phrase\" and exclude_stopwords:\n",
    "    search_text = cleanStopWords(term).strip()\n",
    "else:\n",
    "    search_text = term\n",
    "n_grams = len(search_text.split())\n",
    "doc_cnt_with_term = defaultdict(int)\n",
    "#process \"Columns in a file\"\n",
    "if text_corpus == \"Columns in a file\":\n",
    "    text_corpus_df = pd.read_csv(file_name,sep=\"\\t\",encoding='ISO-8859-1', quotechar='\"',skipinitialspace=True, error_bad_lines=False)\n",
    "    original_columns = text_corpus_df.columns.tolist()\n",
    "    new_columns = original_columns.copy()\n",
    "    for col_to_search in text_column:\n",
    "        new_columns.append(\"tf_idf_\" + str(col_to_search))\n",
    "    output_df = pd.DataFrame(columns = new_columns)\n",
    "    for index, row in text_corpus_df.iterrows():\n",
    "        row_as_dict = {}\n",
    "        for col_to_search in text_column:\n",
    "            # Clean text\n",
    "            text_c = remove_punctuation(row[col_to_search])\n",
    "            if type(text_c) == str:\n",
    "                # clean more\n",
    "                text_c = re.sub('[^A-Za-z0-9Â°]+', ' ', text_c)\n",
    "                text_c = text_c.replace('\\n', '').lower()\n",
    "            else:\n",
    "                text_c = str(text_c)\n",
    "            all_combination_count = generate_N_grams(text_c, n_grams, exclude_stopwords)\n",
    "            total_term_cnt = 0\n",
    "            search_term_cnt = 0\n",
    "            for key in all_combination_count:\n",
    "                total_term_cnt = total_term_cnt + all_combination_count[key]\n",
    "                if key == search_text:\n",
    "                    search_term_cnt = all_combination_count[search_text]\n",
    "            if search_term_cnt > 0:\n",
    "                doc_cnt_with_term[col_to_search] += 1\n",
    "            \n",
    "            for new_col in new_columns:\n",
    "                if new_col != \"tf_idf_\" + str(col_to_search) and new_col in original_columns:\n",
    "                    row_as_dict[new_col] = row[new_col]\n",
    "                elif new_col != \"tf_idf_\" + str(col_to_search):\n",
    "                    if total_term_cnt > 0:\n",
    "                        row_as_dict[\"tf_idf_\" + str(col_to_search)] = search_term_cnt/total_term_cnt\n",
    "                    else:\n",
    "                        row_as_dict[\"tf_idf_\" + str(col_to_search)] = None\n",
    "        output_df = output_df.append(row_as_dict, ignore_index = True)\n",
    "    total_documents = text_corpus_df.shape[0]\n",
    "    for col_to_search in text_column:\n",
    "        idf = 0\n",
    "        if doc_cnt_with_term[col_to_search] > 0:\n",
    "            idf = np.log(total_documents/doc_cnt_with_term[col_to_search])\n",
    "        output_df['tf_idf_' + str(col_to_search)] = output_df['tf_idf_' + str(col_to_search)]*idf\n",
    "    output_df.to_csv('tf_idf_result.txt', sep=\"\\t\", index=False)\n",
    "elif text_corpus == \"Files\":\n",
    "    zip_temp_dir = os.path.join(working_dir, \"zip_temp\")\n",
    "    if os.path.exists(zip_temp_dir):\n",
    "        try:\n",
    "            shutil.rmtree(zip_temp_dir)\n",
    "        except OSError as e:\n",
    "            logToWfl(\"Error: %s : %s\" % (dir_path, e.strerror))\n",
    "            sys.exit(\"Error: %s : %s\" % (dir_path, e.strerror))\n",
    "    #make new dir\n",
    "    if not os.path.exists(zip_temp_dir):\n",
    "        os.makedirs(zip_temp_dir)\n",
    "    #if zip dir already exist\n",
    "    with zipfile.ZipFile(file_name, 'r') as zip_ref:\n",
    "        zip_ref.extractall(zip_temp_dir)\n",
    "        zip_ref.close()\n",
    "    output_df = pd.DataFrame(columns = ['file', 'tf_idf'])\n",
    "    doc_cnt_with_term = 0\n",
    "    doc_cnt = 0\n",
    "    #go throught each file\n",
    "    for subdir, dirs, files in os.walk(zip_temp_dir):\n",
    "        for file in files:\n",
    "            row_as_dict = {}\n",
    "            file_full_path = os.path.join(subdir, file)\n",
    "            file_name_for_id = file_full_path[(file_full_path.find(zip_temp_dir) + len(zip_temp_dir)+1):]\n",
    "            #file name without zip_temp\n",
    "            file_ext = os.path.splitext(file_full_path)[1]\n",
    "            text = \"\"\n",
    "            acceptable_file_type = True\n",
    "            if file_ext == \".pdf\":\n",
    "                raw = parser.from_file(file_full_path)\n",
    "                text = raw['content']\n",
    "                doc_cnt+=1;\n",
    "            elif file_ext == \".doc\" or file_ext == \".docx\":\n",
    "                raw = parser.from_file(file_full_path)\n",
    "                text = raw['content']\n",
    "                doc_cnt+=1;\n",
    "            elif file_ext == \".txt\" or file_ext == \".csv\":\n",
    "                f = open(file_full_path, \"r\", encoding=\"ISO-8859-1\")\n",
    "                text = f.read()\n",
    "                f.close()\n",
    "                doc_cnt+=1;\n",
    "            else:\n",
    "                acceptable_file_type = False;\n",
    "            if acceptable_file_type == False:\n",
    "                row_as_dict[\"file\"] = file_name_for_id\n",
    "                row_as_dict[\"tf_idf\"] = None\n",
    "            else:\n",
    "                # Clean text\n",
    "                text_c = remove_punctuation(text)\n",
    "                if type(text_c) == str:\n",
    "                    # clean more\n",
    "                    text_c = re.sub('[^A-Za-z0-9Â°]+', ' ', text_c)\n",
    "                    text_c = text_c.replace('\\n', '').lower()\n",
    "                else:\n",
    "                    text_c = str(text_c)\n",
    "                all_combination_count = generate_N_grams(text_c, n_grams, exclude_stopwords)\n",
    "                #print(all_combination_count)\n",
    "                total_term_cnt = 0\n",
    "                search_term_cnt = 0\n",
    "                for key in all_combination_count:\n",
    "                    total_term_cnt = total_term_cnt + all_combination_count[key]\n",
    "                    if key == search_text:\n",
    "                        search_term_cnt = all_combination_count[search_text]\n",
    "                if search_term_cnt > 0:\n",
    "                    doc_cnt_with_term += 1\n",
    "                row_as_dict[\"file\"] = file_name_for_id\n",
    "                \n",
    "                if total_term_cnt > 0:\n",
    "                    row_as_dict[\"tf_idf\"] = search_term_cnt/total_term_cnt\n",
    "                else:\n",
    "                    row_as_dict[\"tf_idf\"] = None\n",
    "            output_df = output_df.append(row_as_dict, ignore_index = True)\n",
    "    idf = 0\n",
    "    if doc_cnt_with_term > 0:\n",
    "        idf = np.log(doc_cnt/doc_cnt_with_term)\n",
    "    output_df['tf_idf'] = output_df['tf_idf']*idf\n",
    "    output_df.to_csv('tf_idf_result.txt', sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
