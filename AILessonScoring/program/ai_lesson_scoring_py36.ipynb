{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "import openai\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import datetime as dt\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_response(response_obj):\n",
    "    response_json = json.loads(response_obj['choices'][0]['text'])\n",
    "    rationale = response_json[\"Rationale\"]\n",
    "    score = response_json[\"Score\"]\n",
    "    return (score, rationale)\n",
    "    \n",
    "def logProgressToWfl(progressMsg):\n",
    "    logFile = open(log_file_name, \"a\")\n",
    "    now = dt.datetime.now()\n",
    "    progressPrepend = \"%Progress::\"\n",
    "    logFile.write(progressPrepend + \"@\" + str(now) + \"@\" + progressMsg + \"\\n\");\n",
    "    logFile.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_inputs(inputs, prompt_start):\n",
    "    new_df = pd.DataFrame(columns = ['score','rationale'])\n",
    "    #not over RUN_UP_TO, If an upper bound is set, get response less than this number\n",
    "    if RUN_UP_TO >=  0:  \n",
    "        inputs_upto = inputs[:RUN_UP_TO]\n",
    "    else:\n",
    "        inputs_upto = inputs  # Take the whole set of responses\n",
    "    loop_cnt = 1\n",
    "    for inpt in inputs_upto:\n",
    "        progress = loop_cnt/len(inputs_upto)\n",
    "        progress = f\"{progress * 100:.{0}f}%\"\n",
    "        logProgressToWfl(progress)\n",
    "        new_row = {}\n",
    "        if pd.isnull(inpt):\n",
    "            new_row['score'] =  \"---\"\n",
    "            new_row['rationale'] = \"---\" \n",
    "        else:\n",
    "            prompt = f\"\"\"{prompt_start} {inpt} --- Response End.\n",
    "                    Given the response, please score the tutor response and give your rationale in a JSON string following the format, {{\"Rationale\": \"your reasoning here\", \"Score\":0/1}}. \"\"\"            \n",
    "            try:\n",
    "                response = openai.Completion.create(engine = MODEL, prompt = prompt, temperature = TEMPERATURE, max_tokens = MAX_TOKENS)\n",
    "                score, rationale = extract_response(response)\n",
    "                new_row['score'] = score\n",
    "                new_row['rationale'] = rationale\n",
    "            except Exception as e:\n",
    "                new_row['score'] =  \"---\"\n",
    "                new_row['rationale'] = f\"OpenAI experienced error: {e}\" \n",
    "        new_df = pd.concat([new_df, pd.DataFrame([new_row])], ignore_index=True) # Failsafe\n",
    "        loop_cnt = loop_cnt + 1\n",
    "    \n",
    "    #if data is more than RUN_UP_TO\n",
    "    if len(inputs) > len(new_df):\n",
    "        for i in range(len(inputs)-len(new_df)):\n",
    "            new_row = {'score':\"---\", 'rationale':\"---\"}\n",
    "            new_df = pd.concat([new_df, pd.DataFrame([new_row])], ignore_index=True) \n",
    "    new_df = new_df[[\"score\", \"rationale\"]]\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test command\n",
    "#test: explain with key\n",
    "#C:\\Users\\hchen\\Anaconda3\\python.exe ai_lesson_scoring_py36.py -programDir . -workingDir . -userId hcheng -lesson \"Helping Students Manage Inequity\" -openai_api_key somekey -predict_explain Explain -scoringCol_nodeIndex 0 -scoringCol_fileIndex 0 -scoringCol Input -node 0 -fileIndex 0 HSME_predict.csv\n",
    "\n",
    "command_line = False\n",
    "if command_line:\n",
    "    parser = argparse.ArgumentParser(description=\"AI Lessons Scoring\")\n",
    "    parser.add_argument('-programDir', type=str, help='the component program directory')\n",
    "    parser.add_argument('-workingDir', type=str, help='the component instance working directory')\n",
    "    parser.add_argument(\"-fileIndex\", nargs=2, action='append')\n",
    "    parser.add_argument(\"-node\", action='append')\n",
    "    parser.add_argument(\"-lesson\", help=\"4 lessons to pick\", type=str, required=True)\n",
    "    parser.add_argument(\"-predict_explain\", help=\"predict or explain\", type=str, required=True, choices=['Predict', 'Explain'])\n",
    "    parser.add_argument(\"-scoringCol\", type=str, help='column to score')\n",
    "    #parser.add_argument(\"-have_api_key\", help=\"Boolean to decide which key to use.\", type=str, choices=['Yes', 'No'], default=\"Yes\")\n",
    "    #parser.add_argument(\"-use_config\", help=\"Boolean to decide if key is from config file.\", type=str, choices=['Yes', 'No'], default=\"Yes\")\n",
    "    parser.add_argument(\"-openai_api_key\", help=\"API key for the account that we want to use azure\", type=str)\n",
    "    \n",
    "    args, option_file_index_args = parser.parse_known_args()\n",
    "    \n",
    "    working_dir = args.workingDir\n",
    "    program_dir = args.programDir\n",
    "    data_file = None\n",
    "    #config_file = None\n",
    "    \n",
    "    for x in range(len(args.node)):\n",
    "        if (args.node[x][0] == \"0\" and args.fileIndex[x][0] == \"0\"):\n",
    "            data_file = args.fileIndex[x][1]\n",
    "#         if (args.node[x][0] == \"1\" and args.fileIndex[x][0] == \"0\"):\n",
    "#             config_file = args.fileIndex[x][1]\n",
    "            \n",
    "    column_to_score = args.scoringCol\n",
    "    lesson = args.lesson\n",
    "    predict_explain = (args.predict_explain).lower()\n",
    "    \n",
    "    #have_api_key = (args.have_api_key).lower()\n",
    "    #use_config = (args.use_config).lower()\n",
    "    api_key = args.openai_api_key\n",
    "                    \n",
    "#     if have_api_key == \"yes\" and use_config == \"yes\":\n",
    "#         if config_file is not None:\n",
    "#             config = configparser.ConfigParser()\n",
    "#             config.read(config_file)\n",
    "#             api_key = config.get('section_key', 'OPENAI_API_KEY')\n",
    "    \n",
    "#     if have_api_key == \"no\":\n",
    "#         api_key = None\n",
    "\n",
    "else:\n",
    "    working_dir = \".\"\n",
    "    program_dir = \".\"\n",
    "    data_file = \"HSME_predict.csv\"\n",
    "    #data_file = \"Helping Students Manage Inequity_test.csv\"\n",
    "#     config_file = \"config_file.txt\"\n",
    "#     config = configparser.ConfigParser()\n",
    "#     config.read(config_file)\n",
    "#     api_key = config.get('section_key', 'OPENAI_API_KEY')\n",
    "    api_key = \"some key\"\n",
    "    column_to_score = \"Input\"\n",
    "    #column_to_score = \"Response\"\n",
    "    lesson = \"Helping Students Manage Inequity\"\n",
    "    #lesson = \"Giving Effective Praise\"\n",
    "    predict_explain = \"predict\"\n",
    "    \n",
    "# print(data_file)\n",
    "# print(config_file)\n",
    "# print(column_to_score)\n",
    "# print(lesson)\n",
    "# print(predict_explain)\n",
    "# print(api_key)\n",
    "\n",
    "#get constant from file config.properties\n",
    "config = configparser.ConfigParser()\n",
    "\n",
    "sys_config_file = os.path.join(program_dir, \"program\")\n",
    "sys_config_file = os.path.join(sys_config_file, 'config.properties')\n",
    "config.read(sys_config_file)           \n",
    "            \n",
    "#set parameters\n",
    "MAX_TOKENS = int(config.get('section_engine', 'MAX_TOKENS'))\n",
    "TEMPERATURE = int(config.get('section_engine', 'TEMPERATURE'))\n",
    "RUN_UP_TO = int(config.get('section_engine', 'RUN_UP_TO'))\n",
    "MODEL = config.get('section_engine', 'MODEL')\n",
    "\n",
    "# if api_key is None or api_key == \"\":\n",
    "#     api_key = config.get('section_key', 'OPENAI_API_KEY')\n",
    "\n",
    "lesson_prompt_dic = {\"Helping Students Manage Inequity\" : \"Helping Students Manage Inequity.csv\",\n",
    "                    \"Determining What students Know\" : \"Determining What students Know.csv\",\n",
    "                    \"Giving Effective Praise\" : \"Giving Effective Praise.csv\",\n",
    "                    \"Reacting to Errors\" : \"Reacting to Errors.csv\"}\n",
    "\n",
    "#fresh new log file\n",
    "log_file_name = \"AI_lesson_scoring.wfl\"\n",
    "logFile = open(log_file_name, \"w\")\n",
    "logFile.close();\n",
    "\n",
    "\n",
    "#data file\n",
    "df = pd.read_csv(data_file, encoding=\"ISO-8859-1\")\n",
    "inputs_to_score = df[column_to_score].tolist()\n",
    "\n",
    "#prompt file\n",
    "prompt_file_name = lesson_prompt_dic[lesson]\n",
    "prompt_file = None\n",
    "if prompt_file_name is not None and prompt_file_name != \"\":\n",
    "    prompt_file = os.path.join(program_dir, \"program\")\n",
    "    prompt_file = os.path.join(prompt_file, prompt_file_name)\n",
    "else:\n",
    "    sys.exit(f'Lesson: {lesson} is not supported')\n",
    "df_prompt = None\n",
    "#check if prompt_file exist\n",
    "if os.path.exists(prompt_file):\n",
    "    df_prompt = pd.read_csv(prompt_file, encoding=\"ISO-8859-1\")\n",
    "else:\n",
    "    sys.exit(f'Prompt file not found for lesson: {lesson}')\n",
    "    \n",
    "scoring_prompt_start = df_prompt.loc[df_prompt['type'] == predict_explain, 'scoring_prompt_start'].values[0]\n",
    "#scoring_format_prompt = df_prompt.loc[df_prompt['type'] == predict_explain, 'scoring_format_prompt'].values[0]\n",
    "\n",
    "openai.api_key = api_key\n",
    "\n",
    "#scored_df = score_inputs(inputs_to_score, scoring_prompt_start, scoring_format_prompt)\n",
    "scored_df = score_inputs(inputs_to_score, scoring_prompt_start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#concatenate with original df\n",
    "df_with_score = pd.concat([df, scored_df], axis=1)\n",
    "#reorder column and put new columns next to column_to_score\n",
    "df_with_score_cols = df_with_score.columns.tolist()\n",
    "column_to_score_ind = df_with_score_cols.index(column_to_score)\n",
    "new_cols =  df_with_score_cols[:column_to_score_ind+1] + df_with_score_cols[len(df_with_score_cols)-2:] + df_with_score_cols[column_to_score_ind+1:len(df_with_score_cols)-2]\n",
    "df_with_score = df_with_score[new_cols] \n",
    "#rename\n",
    "df_with_score.rename(columns={'score': 'openAI_score', 'rationale': 'openAI_rationale'}, inplace=True)\n",
    "\n",
    "#new file name\n",
    "new_file_name = os.path.splitext(os.path.basename(data_file))[0] + \"_scored\" + os.path.splitext(os.path.basename(data_file))[1]\n",
    "new_file_name = os.path.join(working_dir, new_file_name)\n",
    "df_with_score.to_csv(new_file_name, index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "36_env",
   "language": "python",
   "name": "36_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
