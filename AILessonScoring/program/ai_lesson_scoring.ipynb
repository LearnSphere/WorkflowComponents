{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afd2a7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from jproperties import Properties\n",
    "import datetime as dt\n",
    "from settings import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ddec62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set constant from settings\n",
    "#set parameters\n",
    "MAX_TOKENS = settings.MAX_TOKENS\n",
    "TEMPERATURE = settings.TEMPERATURE\n",
    "RUN_UP_TO = settings.RUN_UP_TO  \n",
    "MODEL = settings.MODEL\n",
    "\n",
    "lesson_prompt_dic = {\"Helping Students Manage Inequity\" : \"Helping Students Manage Inequity.csv\",\n",
    "                    \"Determining What students Know\" : \"Determining What students Know.csv\",\n",
    "                    \"Giving Effective Praise\" : \"Giving Effective Praise.csv\",\n",
    "                    \"Reacting to Errors\" : \"Reacting to Errors.csv\"}\n",
    "\n",
    "#fresh new log file\n",
    "log_file_name = \"AI_lesson_scoring.wfl\"\n",
    "logFile = open(log_file_name, \"w\")\n",
    "logFile.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "723c80c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_response(response_obj, json=False):\n",
    "    role = response_obj.choices[0].message.role\n",
    "    content = response_obj.choices[0].message.content\n",
    "    if json:\n",
    "        return {\"role\": role, \"content\": content}\n",
    "    else:\n",
    "        return (role, content)\n",
    "    \n",
    "def logProgressToWfl(progressMsg):\n",
    "    logFile = open(log_file_name, \"a\")\n",
    "    now = dt.datetime.now()\n",
    "    progressPrepend = \"%Progress::\"\n",
    "    logFile.write(progressPrepend + \"@\" + str(now) + \"@\" + progressMsg + \"\\n\");\n",
    "    logFile.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f448e90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_inputs(client, inputs, prompt_start, prompt_format):\n",
    "    new_df = pd.DataFrame(columns = ['score','rationale'])\n",
    "    #not over RUN_UP_TO, If an upper bound is set, get response less than this number\n",
    "    if RUN_UP_TO >=  0:  \n",
    "        inputs_upto = inputs[:RUN_UP_TO]\n",
    "    else:\n",
    "        inputs_upto = inputs  # Take the whole set of responses\n",
    "    loop_cnt = 1\n",
    "    for inpt in inputs_upto:\n",
    "        progress = loop_cnt/len(inputs_upto)\n",
    "        progress = f\"{progress * 100:.{0}f}%\"\n",
    "        logProgressToWfl(progress)\n",
    "        new_row = {}\n",
    "        if pd.isna(inpt):\n",
    "            new_row['score'] =  \"---\"\n",
    "            new_row['rationale'] = \"---\" \n",
    "        else:\n",
    "            overall_history = [{\"role\": \"system\", \"content\": prompt_start}, \n",
    "                               {\"role\": \"user\", \"content\": inpt}, \n",
    "                               {\"role\": \"system\", \"content\": prompt_format}]\n",
    "            try:\n",
    "                openai_out = client.chat.completions.create(model=MODEL, messages=overall_history, max_tokens=MAX_TOKENS, temperature = TEMPERATURE)\n",
    "                role, content = extract_response(openai_out)\n",
    "                # We now need to parse the JSON into rational and score\n",
    "                content_json = json.loads(content)  # Run response through JSON\n",
    "                score = str(content_json[\"Score\"])  # Cast to string to avoid type inequality\n",
    "                rationale = str(content_json[\"Rationale\"])  # Fetch the rationale\n",
    "                new_row['score'] = score\n",
    "                new_row['rationale'] = rationale\n",
    "            except Exception as e:\n",
    "                new_row['score'] =  \"---\"\n",
    "                new_row['rationale'] = f\"OpenAI experienced error: {e}\" \n",
    "        new_df = pd.concat([new_df, pd.DataFrame([new_row])], ignore_index=True) # Failsafe\n",
    "        loop_cnt = loop_cnt + 1\n",
    "    \n",
    "    #if data is more than RUN_UP_TO\n",
    "    if len(inputs) > len(new_df):\n",
    "        for i in range(len(inputs)-len(new_df)):\n",
    "            new_row = {'score':\"---\", 'rationale':\"---\"}\n",
    "            new_df = pd.concat([new_df, pd.DataFrame([new_row])], ignore_index=True) \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "84975468",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test command\n",
    "#test: predict, no key\n",
    "#C:\\Users\\hchen\\Anaconda3\\python.exe ai_lesson_scoring.py -programDir . -workingDir . -userId hcheng -have_api_key No -lesson \"Helping Students Manage Inequity\" -predict_explain Predict -scoringCol_nodeIndex 0 -scoringCol_fileIndex 0 -scoringCol Input -use_config Yes -node 0 -fileIndex 0 HSME_predict.csv\n",
    "#test: predict, config file\n",
    "#C:\\Users\\hchen\\Anaconda3\\python.exe ai_lesson_scoring.py -programDir . -workingDir . -userId hcheng -have_api_key Yes -lesson \"Helping Students Manage Inequity\" -predict_explain Explain -scoringCol_nodeIndex 0 -scoringCol_fileIndex 0 -scoringCol Input -use_config Yes -node 0 -fileIndex 0 HSME_predict.csv -node 1 -fileIndex 0 config_file.txt\n",
    "#test: explain with key\n",
    "#C:\\Users\\hchen\\Anaconda3\\python.exe ai_lesson_scoring.py -programDir . -workingDir . -userId hcheng -have_api_key Yes -lesson \"Helping Students Manage Inequity\" -openai_api_key somekey -predict_explain Explain -scoringCol_nodeIndex 0 -scoringCol_fileIndex 0 -scoringCol Input -use_config No -node 0 -fileIndex 0 HSME_predict.csv -node 1 -fileIndex 0 config_file.txt\n",
    "command_line = False\n",
    "if command_line:\n",
    "    parser = argparse.ArgumentParser(description=\"AI Lessons Scoring\")\n",
    "    parser.add_argument('-programDir', type=str, help='the component program directory')\n",
    "    parser.add_argument('-workingDir', type=str, help='the component instance working directory')\n",
    "    parser.add_argument(\"-fileIndex\", nargs=2, action='append')\n",
    "    parser.add_argument(\"-node\", action='append')\n",
    "    parser.add_argument(\"-lesson\", help=\"4 lessons to pick\", type=str, required=True)\n",
    "    parser.add_argument(\"-predict_explain\", help=\"predict or explain\", type=str, required=True, choices=['Predict', 'Explain'])\n",
    "    parser.add_argument(\"-scoringCol\", type=str, help='column to score')\n",
    "    parser.add_argument(\"-have_api_key\", help=\"Boolean to decide which key to use.\", type=str, choices=['Yes', 'No'], default=\"Yes\")\n",
    "    parser.add_argument(\"-use_config\", help=\"Boolean to decide if key is from config file.\", type=str, choices=['Yes', 'No'], default=\"Yes\")\n",
    "    parser.add_argument(\"-openai_api_key\", help=\"API key for the account that we want to use azure\", type=str)\n",
    "    \n",
    "    args, option_file_index_args = parser.parse_known_args()\n",
    "    \n",
    "    working_dir = args.workingDir\n",
    "    program_dir = args.programDir\n",
    "    data_file = None\n",
    "    config_file = None\n",
    "    \n",
    "    for x in range(len(args.node)):\n",
    "        if (args.node[x][0] == \"0\" and args.fileIndex[x][0] == \"0\"):\n",
    "            data_file = args.fileIndex[x][1]\n",
    "        if (args.node[x][0] == \"1\" and args.fileIndex[x][0] == \"0\"):\n",
    "            config_file = args.fileIndex[x][1]\n",
    "            \n",
    "    column_to_score = args.scoringCol\n",
    "    lesson = args.lesson\n",
    "    predict_explain = (args.predict_explain).lower()\n",
    "    \n",
    "    api_key = \"\"\n",
    "    have_api_key = (args.have_api_key).lower()\n",
    "    if args.have_api_key == \"No\":\n",
    "        api_key = settings.OPENAI_API_KEY\n",
    "    else:\n",
    "        if args.use_config == \"Yes\":\n",
    "            if config_file is not None:\n",
    "                configs = Properties()\n",
    "                with open(config_file, 'rb') as cfile:\n",
    "                    configs.load(cfile)\n",
    "                    if configs.get(\"OPENAI_API_KEY\") is not None:\n",
    "                        api_key = configs.get(\"OPENAI_API_KEY\").data\n",
    "        else:\n",
    "            api_key = args.openai_api_key\n",
    "                    \n",
    "else:\n",
    "    working_dir = \".\"\n",
    "    program_dir = \".\"\n",
    "    #data_file = \"HSME_predict.csv\"\n",
    "    data_file = \"Helping Students Manage Inequity_test.csv\"\n",
    "    config_file = \"config_file.txt\"\n",
    "    #column_to_score = \"Input\"\n",
    "    column_to_score = \"Response\"\n",
    "    lesson = \"Helping Students Manage Inequity\"\n",
    "    #lesson = \"Giving Effective Praise\"\n",
    "    predict_explain = \"predict\"\n",
    "    api_key = settings.OPENAI_API_KEY\n",
    "    \n",
    "# print(data_file)\n",
    "# print(config_file)\n",
    "# print(column_to_score)\n",
    "# print(lesson)\n",
    "# print(predict_explain)\n",
    "# print(api_key)\n",
    "\n",
    "\n",
    "#data file\n",
    "df = pd.read_csv(data_file, encoding=\"ISO-8859-1\")\n",
    "inputs_to_score = df[column_to_score].tolist()\n",
    "\n",
    "#prompt file\n",
    "prompt_file_name = lesson_prompt_dic[lesson]\n",
    "prompt_file = None\n",
    "if prompt_file_name is not None and prompt_file_name != \"\":\n",
    "    prompt_file = os.path.join(program_dir, \"program\")\n",
    "    prompt_file = os.path.join(prompt_file, prompt_file_name)\n",
    "else:\n",
    "    sys.exit(f'Lesson: {lesson} is not supported')\n",
    "df_prompt = None\n",
    "#check if prompt_file exist\n",
    "if os.path.exists(prompt_file):\n",
    "    df_prompt = pd.read_csv(prompt_file, encoding=\"ISO-8859-1\")\n",
    "else:\n",
    "    sys.exit(f'Prompt file not found for lesson: {lesson}')\n",
    "    \n",
    "scoring_prompt_start = df_prompt.loc[df_prompt['type'] == predict_explain, 'scoring_prompt_start'].values[0]\n",
    "scoring_format_prompt = df_prompt.loc[df_prompt['type'] == predict_explain, 'scoring_format_prompt'].values[0]\n",
    "\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "scored_df = score_inputs(client, inputs_to_score, scoring_prompt_start, scoring_format_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "52c6341e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate with original df\n",
    "df_with_score = pd.concat([df, scored_df], axis=1)\n",
    "#reorder column and put new columns next to column_to_score\n",
    "df_with_score_cols = df_with_score.columns.tolist()\n",
    "column_to_score_ind = df_with_score_cols.index(column_to_score)\n",
    "new_cols =  df_with_score_cols[:column_to_score_ind+1] + df_with_score_cols[len(df_with_score_cols)-2:] + df_with_score_cols[column_to_score_ind+1:len(df_with_score_cols)-2]\n",
    "df_with_score = df_with_score[new_cols] \n",
    "#rename\n",
    "df_with_score.rename(columns={'score': 'openAI_score', 'rationale': 'openAI_rationale'}, inplace=True)\n",
    "\n",
    "#new file name\n",
    "new_file_name = os.path.splitext(os.path.basename(data_file))[0] + \"_scored\" + os.path.splitext(os.path.basename(data_file))[1]\n",
    "new_file_name = os.path.join(working_dir, new_file_name)\n",
    "df_with_score.to_csv(new_file_name, index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653f1703",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
