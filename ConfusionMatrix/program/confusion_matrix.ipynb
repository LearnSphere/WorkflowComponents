{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b3f15f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f81fa1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_manual(y_true, y_pred, labels=None):\n",
    "    #get all possible labels\n",
    "    if labels is None:\n",
    "        labels = sorted(set(y_true) | set(y_pred))\n",
    "    #paired index with label\n",
    "    label_to_index = {label: i for i, label in enumerate(labels)}\n",
    "    size = len(labels)\n",
    "    #blank matrix\n",
    "    matrix = [[0] * size for _ in range(size)]\n",
    "    for actual, predicted in zip(y_true, y_pred):\n",
    "        i = label_to_index[actual]\n",
    "        j = label_to_index[predicted]\n",
    "        matrix[i][j] += 1\n",
    "\n",
    "    return labels, matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "acf632bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(labels, cm):\n",
    "    total = sum(sum(row) for row in cm)\n",
    "    correct = sum(cm[i][i] for i in range(len(labels)))\n",
    "    accuracy = correct / total\n",
    "\n",
    "    precision = {}\n",
    "    recall = {}\n",
    "    for i, label in enumerate(labels):\n",
    "        tp = cm[i][i]\n",
    "        fp = sum(cm[r][i] for r in range(len(labels))) - tp\n",
    "        fn = sum(cm[i][c] for c in range(len(labels))) - tp\n",
    "        precision[label] = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall[label] = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "\n",
    "    return accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0dd43b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_kappa(labels, cm):\n",
    "    total = sum(sum(row) for row in cm)\n",
    "    observed_agreement = sum(cm[i][i] for i in range(len(labels))) / total\n",
    "\n",
    "    row_sums = [sum(row) for row in cm]\n",
    "    col_sums = [sum(cm[r][c] for r in range(len(labels))) for c in range(len(labels))]\n",
    "\n",
    "    expected_agreement = sum(\n",
    "        (row_sums[i] * col_sums[i]) for i in range(len(labels))\n",
    "    ) / (total * total)\n",
    "\n",
    "    kappa = (observed_agreement - expected_agreement) / (1 - expected_agreement) if expected_agreement != 1 else 0.0\n",
    "    return kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ef77fb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kappa_strength(kappa):\n",
    "    if kappa < 0:\n",
    "        return \"Less than chance\"\n",
    "    elif 0 <= kappa <= 0.2:\n",
    "        return \"Slight\"\n",
    "    elif 0.2 < kappa <= 0.4:\n",
    "        return \"Fair\"\n",
    "    elif 0.4 < kappa <= 0.6:\n",
    "        return \"Moderate\"\n",
    "    elif 0.6 < kappa <= 0.8:\n",
    "        return \"Substantial\"\n",
    "    elif 0.8 < kappa <= 1:\n",
    "        return \"Almost perfect\"\n",
    "    else:\n",
    "        return \"Invalid kappa value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1802fe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_numeric(value):\n",
    "    if isinstance(value, float) and value.is_integer():\n",
    "        return int(value)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "299b54f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "command_line=False\n",
    "if command_line:\n",
    "    parser = argparse.ArgumentParser(description=\"Tutor Evaluation\")\n",
    "    parser.add_argument('-programDir', type=str, help='the component program directory')\n",
    "    parser.add_argument('-workingDir', type=str, help='the component instance working directory')\n",
    "    parser.add_argument(\"-source_a_col\", help=\"\", type=str)\n",
    "    parser.add_argument(\"-source_b_col\", help=\"\", type=str)    \n",
    "    parser.add_argument(\"-fileIndex\", nargs=2, action='append')\n",
    "    parser.add_argument(\"-node\", action='append')\n",
    "    args, option_file_index_args = parser.parse_known_args()\n",
    "    working_dir = args.workingDir\n",
    "    program_dir = args.programDir\n",
    "    if working_dir is None:\n",
    "        working_dir = \".//\"\n",
    "    if program_dir is None:\n",
    "        program_dir = \".//\"\n",
    "    \n",
    "    source_a_col = args.source_a_col\n",
    "    source_b_col = args.source_b_col\n",
    "    \n",
    "    #process files for WF:\n",
    "    if args.node is not None:\n",
    "        for x in range(len(args.node)):\n",
    "            if (args.node[x][0] == \"0\" and args.fileIndex[x][0] == \"0\"):\n",
    "                in_file = args.fileIndex[x][1]\n",
    "                 \n",
    "else: #for test\n",
    "    working_dir = \".//\"\n",
    "    program_dir = \".//\"\n",
    "    #in_file = \"Human and AI output for reacting to student error & dictionary of moves - June 11th 2025 original.csv\" \n",
    "    in_file = \"convertedDelimited.csv\"\n",
    "    source_a_col = \"Human good react to error \"\n",
    "    source_b_col = \"Score\"\n",
    "    \n",
    "df = None\n",
    "#check if csv or txt\n",
    "if in_file.lower().endswith('.csv'):\n",
    "    df = pd.read_csv(in_file, encoding='ISO-8859-1')\n",
    "elif in_file.lower().endswith('.txt'):\n",
    "    df = pd.read_csv(in_file, sep='\\t', encoding='ISO-8859-1')\n",
    "\n",
    "#convert na to \"\"\n",
    "df[source_a_col] = df[source_a_col].fillna('')\n",
    "df[source_b_col] = df[source_b_col].fillna('')\n",
    "#make sure string type\n",
    "df[source_a_col] = df[source_a_col].apply(convert_numeric).astype(str)\n",
    "df[source_b_col] = df[source_b_col].apply(convert_numeric).astype(str)\n",
    "# Extract columns from DataFrame\n",
    "source_a = df[source_a_col].tolist()\n",
    "source_b = df[source_b_col].tolist()\n",
    "\n",
    "# Compute confusion matrix\n",
    "labels, cm = confusion_matrix_manual(source_a, source_b)\n",
    "df_cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "df_cm.index = df_cm.index.map(lambda x: f'{source_a_col}_' + str(x))\n",
    "df_cm.columns = df_cm.columns.map(lambda x: f'{source_b_col}_' + str(x))\n",
    "#print(df_cm)\n",
    "#total for columns\n",
    "df_cm['Total'] = df_cm.sum(axis=1)\n",
    "#add the agreement count column\n",
    "df_cm['Agreement'] = [df_cm.iloc[i, i] for i in range(len(df_cm))]\n",
    "#total for rows\n",
    "df_cm.loc['Total'] = df_cm.sum(axis=0)\n",
    "#percent aggreement\n",
    "df_cm['Percent of agreement'] = round(df_cm['Agreement']/df_cm['Total'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "456275ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "accuracy, precision, recall = calculate_metrics(labels, cm)\n",
    "df_precision = pd.DataFrame.from_dict(precision, orient='index', columns=['Precision'])\n",
    "df_precision['Precision'] = round(df_precision['Precision'], 2)\n",
    "df_precision.index = df_precision.index.map(lambda x: f'{source_a_col}_' + str(x))\n",
    "df_precision.loc['Total'] = np.nan\n",
    "df_recall = pd.DataFrame.from_dict(recall, orient='index', columns=['Recall'])\n",
    "df_recall['Recall'] = round(df_recall['Recall'], 2)\n",
    "df_recall.index = df_recall.index.map(lambda x: f'{source_a_col}_' + str(x))\n",
    "df_recall.loc['Total'] = np.nan\n",
    "df_cm = df_cm.join(df_precision)\n",
    "df_cm = df_cm.join(df_recall)\n",
    "output_file = os.path.join(working_dir,'confusion_matrix.csv')\n",
    "df_cm.to_csv(output_file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a732385e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add accuracy and kappa\n",
    "accuracy = round(accuracy, 2)\n",
    "kappa = round(calculate_kappa(labels, cm), 2)\n",
    "kappa_strength = get_kappa_strength(kappa)\n",
    "with open(output_file, 'a') as f:\n",
    "    f.write('\\n')\n",
    "    f.write(f'Accuracy: {accuracy}\\n')\n",
    "    f.write(f'Kappa: {kappa}; Strength of Agreement: {kappa_strength}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bd1ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
