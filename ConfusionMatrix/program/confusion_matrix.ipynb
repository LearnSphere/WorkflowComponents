{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0b3f15f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f81fa1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix_manual(y_true, y_pred, labels=None):\n",
    "    #get all possible labels\n",
    "    if labels is None:\n",
    "        labels = sorted(set(y_true) | set(y_pred))\n",
    "    #paired index with label\n",
    "    label_to_index = {label: i for i, label in enumerate(labels)}\n",
    "    size = len(labels)\n",
    "    #blank matrix\n",
    "    matrix = [[0] * size for _ in range(size)]\n",
    "    for actual, predicted in zip(y_true, y_pred):\n",
    "        i = label_to_index[actual]\n",
    "        j = label_to_index[predicted]\n",
    "        matrix[i][j] += 1\n",
    "\n",
    "    return labels, matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2b1df79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(labels, cm):\n",
    "    total = sum(sum(row) for row in cm)\n",
    "    correct = sum(cm[i][i] for i in range(len(labels)))\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "\n",
    "    precision = {}\n",
    "    recall = {}\n",
    "    f1 = {}\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        tp = cm[i][i]\n",
    "        fp = sum(cm[r][i] for r in range(len(labels))) - tp\n",
    "        fn = sum(cm[i][c] for c in range(len(labels))) - tp\n",
    "\n",
    "        prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        rec = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1_score = (2 * prec * rec / (prec + rec)) if (prec + rec) > 0 else 0.0\n",
    "\n",
    "        precision[label] = prec\n",
    "        recall[label] = rec\n",
    "        f1[label] = f1_score\n",
    "\n",
    "    # If binary classification, return only the \"positive\" class metrics\n",
    "    if len(labels) == 2:\n",
    "        pos_label = labels[1]  # treat the 2nd label as positive\n",
    "        return {\n",
    "            \"type\": \"binary\",\n",
    "            \"accuracy\": accuracy,\n",
    "            \"positive_label\": pos_label,\n",
    "            \"precision\": precision[pos_label],\n",
    "            \"recall\": recall[pos_label],\n",
    "            \"f1\": f1[pos_label],\n",
    "            \"confusion_matrix\": cm\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"type\": \"multiclass\",\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "            \"confusion_matrix\": cm\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0dd43b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_kappa(labels, cm):\n",
    "    total = sum(sum(row) for row in cm)\n",
    "    observed_agreement = sum(cm[i][i] for i in range(len(labels))) / total\n",
    "\n",
    "    row_sums = [sum(row) for row in cm]\n",
    "    col_sums = [sum(cm[r][c] for r in range(len(labels))) for c in range(len(labels))]\n",
    "\n",
    "    expected_agreement = sum(\n",
    "        (row_sums[i] * col_sums[i]) for i in range(len(labels))\n",
    "    ) / (total * total)\n",
    "\n",
    "    kappa = (observed_agreement - expected_agreement) / (1 - expected_agreement) if expected_agreement != 1 else 0.0\n",
    "    return kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ef77fb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kappa_strength(kappa):\n",
    "    if kappa < 0:\n",
    "        return \"Less than chance\"\n",
    "    elif 0 <= kappa <= 0.2:\n",
    "        return \"Slight\"\n",
    "    elif 0.2 < kappa <= 0.4:\n",
    "        return \"Fair\"\n",
    "    elif 0.4 < kappa <= 0.6:\n",
    "        return \"Moderate\"\n",
    "    elif 0.6 < kappa <= 0.8:\n",
    "        return \"Substantial\"\n",
    "    elif 0.8 < kappa <= 1:\n",
    "        return \"Almost perfect\"\n",
    "    else:\n",
    "        return \"Invalid kappa value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1802fe2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_numeric(value):\n",
    "    if isinstance(value, float) and value.is_integer():\n",
    "        return int(value)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e5c1ac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_style = '''border=\"1\" cellspacing=\"0\" cellpadding=\"5\" \n",
    "       style=\"border-collapse: collapse; text-align: center; font-family: Arial, sans-serif; font-size: 12px; width: auto; min-width: 50%; height: auto; border: 2px solid solid #D0E0F0;\"'''\n",
    "title_row_style = '''style=\\\"text-align:center; font-size: 14px; background-color: #F0F8FF;\\\"'''\n",
    "title_col_style = '''style=\\\"text-align:center; font-size: 14px; background-color: #F0F8FF; writing-mode: vertical-rl; text-orientation: mixed; text-align: center; vertical-align: middle;\\\"'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "02435254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_inner_html(df_cm, row_source, col_source):\n",
    "    nrow = df_cm.shape[0] + 1\n",
    "    ncol = df_cm.shape[1] + 1\n",
    "\n",
    "    # Start HTML table\n",
    "    html = f\"<table {table_style}>\\n\"\n",
    "    #first spanning rows\n",
    "    #html += f\"<tr><td></td><td colspan=\\\"{ncol}\\\" style=\\\"text-align:center; font-weight:bold; background-color: #f2f2f2;\\\">{col_source}</td></tr>\"\n",
    "    html += f\"<tr {title_row_style}><td></td><td colspan=\\\"{ncol}\\\">{col_source}</td></tr>\"\n",
    "    # index row\n",
    "    #html += f\" <tr><td rowspan=\\\"{nrow}\\\" style='writing-mode: vertical-rl; text-orientation: mixed; text-align: center;vertical-align: middle; background-color: #f2f2f2; '>{row_source}</td>\"\n",
    "    html += f\" <tr {title_row_style}><td rowspan=\\\"{nrow}\\\" {title_col_style}>{row_source}</td>\"\n",
    "    html += \"<td></td>\"\n",
    "    for col in df_cm.columns:\n",
    "        html += f\"<td>{col}</td>\"\n",
    "    html += \"</tr>\\n\"\n",
    "    # Data rows\n",
    "    printPct = False\n",
    "    for idx, row in df_cm.iterrows():\n",
    "        html += f\"  <tr><td {title_row_style}>{idx}</td>\"\n",
    "        for col_name, value in row.items():\n",
    "            if \"percent\" in col_name.lower():\n",
    "                html += f\"<td>{row[col_name]:.1%}</td>\"\n",
    "            else:\n",
    "                html += f\"<td>{row[col_name]:.0f}</td>\"\n",
    "        html += \"</tr>\\n\"\n",
    "        \n",
    "    \n",
    "    # End table\n",
    "    html += \"</table>\"\n",
    "    return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "133c2caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_to_html(metrics: dict) -> str:\n",
    "    \"\"\"\n",
    "    Convert binary or multiclass metrics dictionary into an HTML table (manual HTML generation).\n",
    "    Allows styling control over each row/col.\n",
    "    \"\"\"\n",
    "    #html = '<table class=\"metrics-table\" border=\"1\" cellspacing=\"0\" cellpadding=\"5\">\\n'\n",
    "    html = f'<table {table_style}>\\n'\n",
    "    \n",
    "    \n",
    "\n",
    "    if metrics[\"type\"] == \"binary\":\n",
    "        # Single row without label\n",
    "        html += \"  <thead>\\n\"\n",
    "        html += f\"    <tr {title_row_style}><th>Precision</th><th>Recall</th><th>F1</th></tr>\\n\"\n",
    "        html += \"  </thead>\\n\"\n",
    "        html += \"  <tbody>\\n\"\n",
    "        html += f'    <tr>'\n",
    "        html += f'<td>{metrics[\"precision\"]:.3f}</td>'\n",
    "        html += f'<td>{metrics[\"recall\"]:.3f}</td>'\n",
    "        html += f'<td>{metrics[\"f1\"]:.3f}</td>'\n",
    "        html += f'</tr>\\n'\n",
    "\n",
    "    elif metrics[\"type\"] == \"multiclass\":\n",
    "        html += \"  <thead>\\n\"\n",
    "        html += f\"    <tr {title_row_style}><th></th><th>Precision</th><th>Recall</th><th>F1</th></tr>\\n\"\n",
    "        html += \"  </thead>\\n\"\n",
    "        html += \"  <tbody>\\n\"\n",
    "        for label in metrics[\"precision\"].keys():\n",
    "            html += f'    <tr>'\n",
    "            html += f'<td {title_row_style}\">{label}</td>'\n",
    "            html += f'<td>{metrics[\"precision\"][label]:.3f}</td>'\n",
    "            html += f'<td>{metrics[\"recall\"][label]:.3f}</td>'\n",
    "            html += f'<td>{metrics[\"f1\"][label]:.3f}</td>'\n",
    "            html += f'</tr>\\n'\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unknown metrics type\")\n",
    "\n",
    "    html += \"  </tbody>\\n</table>\"\n",
    "\n",
    "    return html\n",
    "\n",
    "# #test\n",
    "# binary_example = {\n",
    "#     'type': 'binary',\n",
    "#     'accuracy': 0.94,\n",
    "#     'positive_label': '1',\n",
    "#     'precision': 0.8636363636363636,\n",
    "#     'recall': 1.0,\n",
    "#     'f1': 0.9268292682926829\n",
    "# }\n",
    "\n",
    "# multiclass_example = {\n",
    "#     'type': 'multiclass',\n",
    "#     'accuracy': 0.76,\n",
    "#     'precision': {'': 1.0, '0': 0.6363636363636364, '1': 0.2727272727272727},\n",
    "#     'recall': {'': 0.9032258064516129, '0': 0.5384615384615384, '1': 0.5},\n",
    "#     'f1': {'': 0.9491525423728813, '0': 0.5833333333333334, '1': 0.3529411764705882}\n",
    "# }\n",
    "\n",
    "# print(metrics_to_html(binary_example))\n",
    "# #print(metrics_to_html(multiclass_example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "299b54f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "command_line=False\n",
    "if command_line:\n",
    "    parser = argparse.ArgumentParser(description=\"Tutor Evaluation\")\n",
    "    parser.add_argument('-programDir', type=str, help='the component program directory')\n",
    "    parser.add_argument('-workingDir', type=str, help='the component instance working directory')\n",
    "    parser.add_argument(\"-source_a_col\", help=\"\", type=str)\n",
    "    parser.add_argument(\"-source_b_col\", help=\"\", type=str)    \n",
    "    parser.add_argument(\"-fileIndex\", nargs=2, action='append')\n",
    "    parser.add_argument(\"-node\", action='append')\n",
    "    args, option_file_index_args = parser.parse_known_args()\n",
    "    working_dir = args.workingDir\n",
    "    program_dir = args.programDir\n",
    "    if working_dir is None:\n",
    "        working_dir = \".//\"\n",
    "    if program_dir is None:\n",
    "        program_dir = \".//\"\n",
    "    \n",
    "    source_a_col = args.source_a_col\n",
    "    source_b_col = args.source_b_col\n",
    "    \n",
    "    #process files for WF:\n",
    "    if args.node is not None:\n",
    "        for x in range(len(args.node)):\n",
    "            if (args.node[x][0] == \"0\" and args.fileIndex[x][0] == \"0\"):\n",
    "                in_file = args.fileIndex[x][1]\n",
    "                 \n",
    "else: #for test\n",
    "    working_dir = \".//\"\n",
    "    program_dir = \".//\"\n",
    "    #in_file = \"Human and AI output for reacting to student error & dictionary of moves - June 11th 2025 original.csv\" \n",
    "    in_file = \"ECTEL2025 Transcript Scores human-paper-gpt4o.csv\"\n",
    "    source_a_col = \"praise_gpt4o-eval\" #praise_gpt4o-present, praise_gpt4o-eval\n",
    "    source_b_col = \"praise_human-eval\" #praise_human-present, praise_human-eval\n",
    "    \n",
    "df = None\n",
    "#check if csv or txt\n",
    "if in_file.lower().endswith('.csv'):\n",
    "    df = pd.read_csv(in_file, encoding='ISO-8859-1')\n",
    "elif in_file.lower().endswith('.txt'):\n",
    "    df = pd.read_csv(in_file, sep='\\t', encoding='ISO-8859-1')\n",
    "\n",
    "#convert na to \"\"\n",
    "df[source_a_col] = df[source_a_col].fillna('')\n",
    "df[source_b_col] = df[source_b_col].fillna('')\n",
    "#make sure string type\n",
    "df[source_a_col] = df[source_a_col].apply(convert_numeric).astype(str)\n",
    "df[source_b_col] = df[source_b_col].apply(convert_numeric).astype(str)\n",
    "# Extract columns from DataFrame\n",
    "source_a = df[source_a_col].tolist()\n",
    "source_b = df[source_b_col].tolist()\n",
    "\n",
    "# Compute confusion matrix\n",
    "labels, cm = confusion_matrix_manual(source_a, source_b)\n",
    "df_cm = pd.DataFrame(cm, index=labels, columns=labels)\n",
    "\n",
    "#total for columns\n",
    "df_cm['Total'] = df_cm.sum(axis=1)\n",
    "#add the agreement count column\n",
    "df_cm['Agreement'] = [df_cm.iloc[i, i] for i in range(len(df_cm))]\n",
    "#total for rows\n",
    "df_cm.loc['Total'] = df_cm.sum(axis=0)\n",
    "#percent aggreement\n",
    "df_cm['Percent of agreement'] = round(df_cm['Agreement']/df_cm['Total'], 2)\n",
    "\n",
    "cm_html = make_inner_html(df_cm, source_a_col, source_b_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b8e6d791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p style=\"font-family: Arial, sans-serif;\"><span style=\"font-weight: bold;\">Agreement:</span> 0.76; \n",
      "                <span style=\"font-weight: bold;\">Kappa:</span> 0.58; <span style=\"font-weight: bold;\">Kappa Strength:</span> Moderate</p>\n"
     ]
    }
   ],
   "source": [
    "metrics = calculate_metrics(labels, cm)\n",
    "accuracy = metrics[\"accuracy\"]\n",
    "accuracy = round(accuracy, 2)\n",
    "kappa = round(calculate_kappa(labels, cm), 2)\n",
    "kappa_strength = get_kappa_strength(kappa)\n",
    "sum_html = f'''<p style=\"font-family: Arial, sans-serif;\"><span style=\"font-weight: bold;\">Agreement:</span> {accuracy}; \n",
    "                <span style=\"font-weight: bold;\">Kappa:</span> {kappa}; <span style=\"font-weight: bold;\">Kappa Strength:</span> {kappa_strength}</p>'''\n",
    "metric_html = metrics_to_html(metrics)\n",
    "all_html = f'''<!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <meta charset=\"utf-8\">\n",
    "        \n",
    "    </head>\n",
    "    <body>\n",
    "    {cm_html} \n",
    "    {sum_html}\n",
    "    {metric_html}\n",
    "    </body>\n",
    "    </html>'''\n",
    "\n",
    "print(sum_html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "724f6efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output files\n",
    "output_file = os.path.join(working_dir,'confusion_matrix.csv')\n",
    "df_cm.to_csv(output_file)\n",
    "with open(\"confusion_matrix.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(all_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f007cfec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
