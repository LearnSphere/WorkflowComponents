{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c989f088",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Deidentification script that gives the user the option between Azure langauge services (paid), Amazon Comprehend(paid), or Presidio (free) for pii\n",
    "detection. Also, when it comes to replacing the pii the user has the option to either encode with hash values or to use HIPS.\n",
    "\"\"\"\n",
    "import argparse\n",
    "import csv\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import warnings\n",
    "from typing import Union, Tuple\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from presidio_analyzer import AnalyzerEngine\n",
    "from presidio_analyzer.nlp_engine import SpacyNlpEngine\n",
    "import spacy\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    import boto3\n",
    "    import botocore.exceptions\n",
    "from jproperties import Properties\n",
    "from email_validator import validate_email, EmailNotValidError\n",
    "\n",
    "#from azureConfig import loginConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2210da55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_azure_client(API_KEY, END_POINT) -> TextAnalyticsClient:\n",
    "    \"\"\"Retrieves the azure client so we can interact with api\"\"\"\n",
    "\n",
    "    client = TextAnalyticsClient(\n",
    "        endpoint=END_POINT,\n",
    "        credential=AzureKeyCredential(API_KEY)\n",
    "    )\n",
    "    return client\n",
    "#print(get_azure_client())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "86c1062e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_presidio_client() -> AnalyzerEngine:\n",
    "    \"\"\"Initializes the presidio engine we will use for pii detection\"\"\"\n",
    "    nlp = spacy.load('en_core_web_lg')\n",
    "    #nlp = spacy.load('en_core_web_sm')\n",
    "    class CustomSpacyNlpEngine(SpacyNlpEngine):\n",
    "        \"\"\"Custom nlp model that extends the SpacyNlpEngine from presidio.\"\"\"\n",
    "        def __init__(self, model):\n",
    "            super().__init__()\n",
    "            self.nlp = {\"en\": model}\n",
    "    return AnalyzerEngine(nlp_engine=CustomSpacyNlpEngine(nlp))\n",
    "#print(get_presidio_client())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "664d6b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comprehend_client(access_key, secret_key) -> boto3.client:\n",
    "    \"\"\"\n",
    "    Retrieves the AWS Comprehend client so we can interact with api.\n",
    "    After configuring your aws account on the cl a .aws/credentials is created in the local directory\n",
    "    for boto3 to reference when it comes to access keys\n",
    "    \"\"\"\n",
    "\n",
    "    client = boto3.client(\n",
    "        service_name='comprehend',\n",
    "        aws_access_key_id=access_key,\n",
    "        aws_secret_access_key=secret_key,\n",
    "        region_name='us-west-2'\n",
    "    )\n",
    "\n",
    "    return client\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d7d04712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_encoding_file(hash_key, value) -> None:\n",
    "    \"\"\"update our encoding file as we go with any new hashes we find\"\"\"\n",
    "    with open(updated_encoding_file, 'a', newline='', encoding='utf-8') as f:\n",
    "        if f.tell() == 0:\n",
    "            f.write('\\n')\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([value, hash_key])\n",
    "\n",
    "def load_name_hash_mapping(csv_file, name_col, hash_col) -> None:\n",
    "    \"\"\"\n",
    "    This function reads in a csv file and fills our name_hash_dict dictionary that represents the key, value pairs \n",
    "    between names and their encoded hashes.\n",
    "    \"\"\"\n",
    "    name_hash_df = pd.read_csv(csv_file, dtype=str)\n",
    "    name_id = name_hash_df.columns.get_loc(name_col)\n",
    "    hash_id = name_hash_df.columns.get_loc(hash_col)\n",
    "    for _, row in name_hash_df.iterrows():\n",
    "        name, hash_value = str(row.iloc[name_id]).strip(), str(row.iloc[hash_id]).strip()\n",
    "        name_hash_dict[name.lower()] = hash_value\n",
    "        \n",
    "def is_email(entity) -> bool:\n",
    "    \"\"\"Checks if a string is a valid email address\"\"\"\n",
    "    try:\n",
    "        validate_email(entity, check_deliverability=False)\n",
    "        return True\n",
    "    except EmailNotValidError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "141c4369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def redact_pii(text, client, method, hips_boolean, scoring_threshold) -> Union[str, Tuple[None, Union[str, Exception]]]:\n",
    "    \"\"\"\n",
    "    Takes in text and using the apropiate client (Azure/Presidio/Comprehend) will either encode/HIPS for all instances of pii in the text \n",
    "    PARAMETERS:\n",
    "    client - \n",
    "        The tool we're using to detect pii -> Azure/Presidio/Comprehend\n",
    "    method - \n",
    "        String representation of the tool that we'll be using so that we can make the apropiate client calls\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or len(text) == 0:\n",
    "        return text\n",
    "    try:\n",
    "        #build aprop client for our method\n",
    "        if method == 'azure':\n",
    "            response = client.recognize_pii_entities([text], language=\"en\")\n",
    "            if response is None:\n",
    "                return None, \"Error: No response received from recognize_pii_entities.\"\n",
    "            response = response[0].entities\n",
    "\n",
    "        elif method == 'presidio':\n",
    "            response = client.analyze(text=text, entities=[\"PERSON\", \"EMAIL_ADDRESS\"], language='en', score_threshold=scoring_threshold)\n",
    "            #Lower Threshold (e.g., 0.3): More detections (may include false positives).\n",
    "            #Higher Threshold (e.g., 0.8): Fewer detections (reduces false positives).\n",
    "            #example response: [type: EMAIL_ADDRESS, start: 42, end: 63, score: 1.0, type: PERSON, start: 7, end: 12, score: 0.85, type: PERSON, start: 14, end: 24, score: 0.85]\n",
    "            if response is None:\n",
    "                return None, \"Error: No response received from analyze of presidio.\"\n",
    "        \n",
    "        elif method == 'comprehend':\n",
    "            response = client.detect_entities(Text=text, LanguageCode='en')\n",
    "            if response is None:\n",
    "                return None, \"Error: No response received from detect_entities\"\n",
    "            response = response['Entities']\n",
    "            \n",
    "        redacted_text = text\n",
    "        # sort in reverse so we dont mess up index order, but not needed\n",
    "        #response.sort(key=lambda x: x.start, reverse=True)\n",
    "        #print(response)\n",
    "        for entity in response: \n",
    "            #filter out all values that dont fall into the aprop person/email categories for our methods\n",
    "            if method == 'azure':\n",
    "                category = entity.category\n",
    "                entity_text = entity.text\n",
    "            elif method == 'presidio':\n",
    "                category = entity.entity_type\n",
    "                entity_text = text[entity.start:entity.end]\n",
    "            elif method == 'comprehend':\n",
    "                category = entity['Type']\n",
    "                entity_text = entity['Text']\n",
    "\n",
    "            if category not in ['Person', 'Email'] and method == 'azure':\n",
    "                continue\n",
    "            if category not in ['PERSON', 'EMAIL_ADDRESS'] and method == 'presidio':\n",
    "                continue\n",
    "            if category not in ['PERSON', 'OTHER', 'ORGANIZATION'] and method == 'comprehend':\n",
    "                continue\n",
    "            #Comprehend doesn't have an email tag, so we have to make sure this OTHER instance is actually an email\n",
    "            if category in ['OTHER'] and not is_email(entity_text):\n",
    "                continue\n",
    "            #switch other -> email for comprehend so we can get prefixes when encoding \n",
    "            if category in ['OTHER'] and method == 'comprehend':\n",
    "                category = 'email'\n",
    "            #switch org -> person for comprehend so we can get prefixes when encoding\n",
    "            if category in ['ORGANIZATION'] and method == 'comprehend':\n",
    "                category = 'person'\n",
    "\n",
    "            #determine if we'll use hips/encoding\n",
    "            if hips_boolean:\n",
    "                redacted_text = hide_pii(entity_text, category, redacted_text)\n",
    "            else:\n",
    "                redacted_text = encode_pii(entity_text, category, redacted_text)\n",
    "        return redacted_text\n",
    "    \n",
    "    #catch if anything goes wrong, return as tuple so we can be flagged at the None then print the error to user\n",
    "    except Exception as e:\n",
    "        return None, e\n",
    "    \n",
    "# #test presidio\n",
    "# #line_text = redact_pii(\"hello, Jaden, John Smith can you hear me? hcheng688@hotmail.com\", get_presidio_client(), \"presidio\", True)\n",
    "# line_text = redact_pii(\"hello, Jaden, John Smith can you hear me? hcheng688@hotmail.com\", get_presidio_client(), \"presidio\", False)\n",
    "# print(line_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "24e11fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#name_hash_dict should have the pairs of name and hash. if not existing in file, we make new hash and add it to updated encoding file\n",
    "def encode_pii(entity_text, category, current_text) -> str:\n",
    "    \"\"\"Takes in the text of our instance and encodes it based off our hash values\"\"\"\n",
    "    prefix = category.lower() if category.lower() in ['person', 'email', 'email_address'] else 'obj'\n",
    "    if entity_text.lower() in name_hash_dict:\n",
    "        current_text = current_text.replace(entity_text, name_hash_dict[entity_text.lower()])\n",
    "    else:\n",
    "        #if not in our dict generate new hash value for entity and update the dict\n",
    "        new_index = len(name_hash_dict) + 1\n",
    "        hash_value = f\"{prefix}_{new_index:07d}\"\n",
    "        name_hash_dict[entity_text.lower()] = hash_value\n",
    "        update_encoding_file(hash_value, entity_text.lower())\n",
    "        current_text = current_text.replace(entity_text, name_hash_dict[entity_text.lower()])\n",
    "    return current_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7fd52c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get Random name and email if not deidentified already. if new, add to updated encoding file\n",
    "def hide_pii(entity_text, category, current_text) -> str:\n",
    "    \"\"\"Takes in the text of our instance and generates a fake value to hip\"\"\"\n",
    "    if category.lower() == \"person\":\n",
    "        if entity_text.lower() in name_hips_dict:\n",
    "            random_value = name_hips_dict[entity_text.lower()]\n",
    "        else:\n",
    "            random_value = RANDOMS.first_name()\n",
    "            name_hips_dict[entity_text.lower()] = random_value\n",
    "            update_encoding_file(random_value, entity_text.lower())\n",
    "\n",
    "    elif category.lower() == \"email\" or category.lower() == 'email_address' or category.lower() == 'other':\n",
    "        if entity_text.lower() in name_hips_dict:\n",
    "            random_value = name_hips_dict[entity_text.lower()]\n",
    "        else:\n",
    "            random_value = RANDOMS.email()\n",
    "            name_hips_dict[entity_text.lower()] = random_value\n",
    "            update_encoding_file(random_value, entity_text.lower())\n",
    "    else:\n",
    "        #keep the same \n",
    "        random_value = entity_text\n",
    "    current_text = current_text.replace(entity_text, random_value)\n",
    "    return current_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "35e43031",
   "metadata": {},
   "outputs": [],
   "source": [
    "#handle csv. call redact_pii which depending on hips_boolean to call HIPS or encoding\n",
    "def handle_csv(output_file, transcript_file, client, method, hips_boolean, ignore_columns, scoring_threshold) -> None:\n",
    "    df = pd.read_csv(transcript_file, quotechar='\"')\n",
    "    print(f\"starting to encode: {transcript_file}\")\n",
    "    for col in df.columns:\n",
    "        if col not in ignore_columns:\n",
    "            df[col] = df[col].apply(lambda x:  redact_pii(x, client, method, hips_boolean, scoring_threshold))\n",
    "    df.to_csv(output_file, index=False)        \n",
    "    print(f\"{len(df)} rows completed\")    \n",
    "    \n",
    "# #test\n",
    "# #handle_csv(\"Zoom_Mathia_Mohawk_10_24_2024_1305_Geramita_cleaned.csv\", \"Zoom_Mathia_Mohawk_10_24_2024_1305_Geramita.csv\", get_presidio_client(), \"presidio\", True, ['Input'])\n",
    "# handle_csv(\"Zoom_Mathia_Mohawk_10_24_2024_1305_Geramita_non_hips_cleaned.csv\", \"Zoom_Mathia_Mohawk_10_24_2024_1305_Geramita_simple.csv\", get_presidio_client(), \"presidio\", False, [])\n",
    "# print(name_hips_dict)\n",
    "# print(name_hash_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6b9a668e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#handle all text file format. call redact_pii which depending on hips_boolean to call HIPS or encoding\n",
    "def handle_other(output_file, transcript_file, client, method, hips_boolean, scoring_threshold) -> None:\n",
    "    \"\"\"\n",
    "    This method is meant to handle text files that can be passed in\n",
    "    PARAMETERS:\n",
    "    output_file - \n",
    "        The file we mean to write all of our output to\n",
    "    transcript_file - \n",
    "        The file with pii that was originall passed into the script on the cl\n",
    "    \"\"\"\n",
    "    with open(transcript_file, 'r', encoding='utf-8') as file:\n",
    "        transcript = file.read()\n",
    "\n",
    "    #azure has a cap on the max amount of lines that can be passed in at once, batch here\n",
    "    chunks = [transcript[i:i+5120] for i in range(0, len(transcript), 5120)]\n",
    "    print(f\"starting to encode: {transcript_file}\")\n",
    "    with open(output_file, 'w', encoding='utf-8', newline='\\n') as file:\n",
    "        index = 1\n",
    "        for chunk in chunks:\n",
    "            modified_chunk = redact_pii(chunk, client, method, hips_boolean, scoring_threshold)\n",
    "            if isinstance(modified_chunk, tuple):\n",
    "                sys.exit(f\"Managed to process {index-1} chunks before failing. We encountered the following error: {modified_chunk[1]}\")\n",
    "            file.write(modified_chunk)\n",
    "            print(f\"finished chunk {index}\")\n",
    "            index += 1\n",
    "\n",
    "#handle_other(\"random_transcript_cleaned.json\", \"random_transcript.json\", get_presidio_client(), \"presidio\", False)\n",
    "#handle_other(\"random_transcript_cleaned.json\", \"random_transcript.json\", get_presidio_client(), \"presidio\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e154b8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use HIPS to deidentify. depending the pii_file extension, will call handle_csv or handle_other\n",
    "def hips_method(pii_file, client, method, ignore_columns, scoring_threshold) -> None:\n",
    "    \"\"\"Declares file name for the returned pii file and preps clients to use hips when replacing pii\"\"\"\n",
    "    filename, file_extension = os.path.splitext(os.path.basename(pii_file))\n",
    "    return_pii_file = f\"{filename}_cleaned{file_extension}\"\n",
    "    #not the best way to write this but updated_encoding_file is defined before this function is called\n",
    "    with open(updated_encoding_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"name\", \"hash\"])\n",
    "\n",
    "    try:\n",
    "        if file_extension == '.csv':\n",
    "            handle_csv(return_pii_file, pii_file, client, method, True, ignore_columns, scoring_threshold)\n",
    "        else:\n",
    "            handle_other(return_pii_file, pii_file, client, method, True, scoring_threshold)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the PII file: {e}\")\n",
    "        sys.exit(1)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "eb537c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use encoding file to deidentify. depending the pii_file extension, will call handle_csv or handle_other\n",
    "def encoding_method(encoding_file, pii_file, name_col, hash_col, client, method, ignore_columns, scoring_threshold) -> None:\n",
    "    \"\"\"Declares file names for the returned pii file and the encodings we have. Preps clients to use encoding when replacing pii\"\"\"\n",
    "    global updated_encoding_file\n",
    "    try:\n",
    "        load_name_hash_mapping(encoding_file, name_col, hash_col)\n",
    "        filename, file_extension = os.path.splitext(os.path.basename(encoding_file))\n",
    "        updated_encoding_file = f\"{filename}_updated{file_extension}\"\n",
    "        #copy encoding_file to updated_encoding_file\n",
    "        shutil.copyfile(encoding_file, updated_encoding_file)\n",
    "    except:\n",
    "        print('No passed in encoding file, will start with empty one')\n",
    "        with open(updated_encoding_file, 'w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"name\", \"hash\"])\n",
    "\n",
    "    filename, file_extension = os.path.splitext(os.path.basename(pii_file))\n",
    "    return_pii_file = f\"{filename}_cleaned{file_extension}\"\n",
    "\n",
    "    try:\n",
    "        if file_extension == '.csv':\n",
    "            handle_csv(return_pii_file, pii_file, client, method, False, ignore_columns, scoring_threshold)\n",
    "        else:\n",
    "            handle_other(return_pii_file, pii_file, client, method, False, scoring_threshold)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the PII file: {e}\")\n",
    "        sys.exit(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "52db73a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to encode: Zoom_Mathia_Mohawk_10_24_2024_1305_Geramita_simple.csv\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "0.3\n",
      "868 rows completed\n",
      "Successfully hid PII in Zoom_Mathia_Mohawk_10_24_2024_1305_Geramita_simple.csv\n"
     ]
    }
   ],
   "source": [
    "# Globals\n",
    "#API_KEY = loginConfig['API_KEY']\n",
    "#END_POINT = loginConfig['END_POINT']\n",
    "name_hash_dict = {}\n",
    "name_hips_dict = {}\n",
    "updated_encoding_file = \"updated_encoding_file.csv\"\n",
    "CSV_ROW_UPDATE = 100\n",
    "RANDOMS = Faker()\n",
    "DATE_TIME = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "#test on command line\n",
    "#C:\\Users\\hchen\\Anaconda3\\envs\\36_env\\python.exe azidioPii_LS_py36.py -programDir . -workingDir . -userId 1 -Hips_boolean No -method Presidio -piiFileType Non-CSV -skipCol No -useEncoding No -node 0 -fileIndex 0 random_transcript.json\n",
    "\n",
    "command_line = False\n",
    "if command_line:\n",
    "    parser = argparse.ArgumentParser(description=\"Deidentification script using Azure, Comprehend, or Presidio\")\n",
    "    parser.add_argument('-programDir', type=str, help='the component program directory')\n",
    "    parser.add_argument('-workingDir', type=str, help='the component instance working directory')\n",
    "    parser.add_argument(\"-fileIndex\", nargs=2, action='append')\n",
    "    parser.add_argument(\"-node\", action='append')\n",
    "    parser.add_argument(\"-method\", help=\"Method to use for deidentification: 'Azure', 'Comprehend' or 'Presidio'\", type=str, required=True, choices=['Azure', 'Presidio', 'Comprehend'])\n",
    "    parser.add_argument(\"-presidioScoreThreshold\", help=\"Scoring threshold for Presidio\", type=float, default=0.6)\n",
    "    parser.add_argument(\"-Hips_boolean\", help=\"Boolean to decide which method to use.\", type=str, choices=['Yes', 'No'], default=\"No\")\n",
    "    parser.add_argument(\"-useEncoding\", help=\"Boolean to decide if using encoding.\", type=str, choices=['Yes', 'No'], default=\"No\")\n",
    "    parser.add_argument(\"-skipCol\", type=str, choices=['Yes', 'No'], default=\"No\")\n",
    "    parser.add_argument(\"-skip_columns\", action='append')\n",
    "    parser.add_argument(\"-piiFileType\", help=\"File type.\", type=str, choices=['CSV', 'Non-CSV'], default=\"Non-CSV\")\n",
    "    parser.add_argument('-Hash_col', type=str, help='column to use for hash in the encoding file')\n",
    "    parser.add_argument('-name_col', type=str, help='column to use for name in the encoding file')\n",
    "    parser.add_argument(\"-aws_access_key\", help=\"Access key for the account that we want to use for the amazon comprehend call\", type=str)\n",
    "    parser.add_argument(\"-aws_secret_key\", help=\"Secret key for the account that we want to use azure\", type=str)\n",
    "    parser.add_argument(\"-api_key\", help=\"API key for the account that we want to use azure\", type=str)\n",
    "    parser.add_argument(\"-end_point\", help=\"END POINT for the account that we want to use azure\", type=str)\n",
    "    #parser.add_argument(\"-use_config\", help=\"Use config file.\", type=str, choices=['Yes', 'No'], default=\"No\")\n",
    "    \n",
    "\n",
    "    #args = parser.parse_args()\n",
    "    args, option_file_index_args = parser.parse_known_args()\n",
    "    working_dir = args.workingDir\n",
    "    program_dir = args.programDir\n",
    "    pii_file = None\n",
    "    encoding_file = None\n",
    "    #config_file = None\n",
    "    \n",
    "    \n",
    "    \n",
    "    for x in range(len(args.node)):\n",
    "        if (args.node[x][0] == \"0\" and args.fileIndex[x][0] == \"0\"):\n",
    "            pii_file = args.fileIndex[x][1]\n",
    "        if (args.node[x][0] == \"1\" and args.fileIndex[x][0] == \"0\"):\n",
    "            encoding_file = args.fileIndex[x][1]\n",
    "#         if (args.node[x][0] == \"2\" and args.fileIndex[x][0] == \"0\"):\n",
    "#             config_file = args.fileIndex[x][1]\n",
    "\n",
    "    method = (args.method).lower()\n",
    "    scoring_threshold = args.presidioScoreThreshold\n",
    "    hips_boolean = False\n",
    "    if (args.Hips_boolean).lower() == \"yes\":\n",
    "        hips_boolean = True\n",
    "    skip_columns = None\n",
    "    if args.skipCol == \"Yes\" and args.skip_columns is not None:\n",
    "        skip_columns = args.skip_columns\n",
    "    else:\n",
    "        skip_columns = []\n",
    "    hash_col = None\n",
    "    name_col = None\n",
    "    if args.Hash_col is not None:\n",
    "        hash_col = args.Hash_col\n",
    "    if args.name_col is not None:\n",
    "        name_col = args.name_col\n",
    "        \n",
    "    aws_access_key = None\n",
    "    aws_secret_key = None\n",
    "    api_key = None\n",
    "    end_point = None\n",
    "    if method in ['azure', 'comprehend']:\n",
    "        #if (args.use_config).lower() == \"no\":\n",
    "        if args.aws_access_key is not None:\n",
    "            aws_access_key = args.aws_access_key\n",
    "        if args.aws_secret_key is not None:\n",
    "            aws_secret_key = args.aws_secret_key\n",
    "        if args.api_key is not None:\n",
    "            api_key = args.api_key\n",
    "        if args.end_point is not None:\n",
    "            end_point = args.end_point\n",
    "\n",
    "                    \n",
    "else:\n",
    "    # use proper client\n",
    "    #method = \"azure\"\n",
    "    method = \"presidio\"\n",
    "\n",
    "    # hips_boolean = True\n",
    "    # encoding_file = None\n",
    "\n",
    "    # hips_boolean = False\n",
    "    # encoding_file = None\n",
    "\n",
    "    hips_boolean = True\n",
    "    encoding_file = \"updated_encoding_file_v1.csv\"\n",
    "    scoring_threshold = 0.3\n",
    "    \n",
    "    #pii_file = \"random_transcript.json\"\n",
    "    #pii_file = \"csv_file_test.csv\"\n",
    "    pii_file = \"Zoom_Mathia_Mohawk_10_24_2024_1305_Geramita_simple.csv\"\n",
    "    skipCol = \"Yes\"\n",
    "    skip_columns = [\"CF (Rule Id)\"]\n",
    "    api_key = \"\"\n",
    "    end_point = \"https://remove-pii.cognitiveservices.azure.com/\"\n",
    "    hash_col = \"name\"\n",
    "    name_col = \"hash\"\n",
    "    \n",
    "    \n",
    "#test\n",
    "# print(pii_file)\n",
    "# print(encoding_file)\n",
    "# print(method)\n",
    "# print(hips_boolean)\n",
    "# print(skip_columns)\n",
    "# print(hash_col)\n",
    "# print(name_col)\n",
    "# print(aws_access_key)\n",
    "# print(aws_secret_key)\n",
    "# print(api_key)\n",
    "# print(end_point)\n",
    "# print(scoring_threshold)\n",
    "    \n",
    "if method == 'azure':\n",
    "    client = get_azure_client(api_key, end_point)\n",
    "elif method == 'presidio':\n",
    "    client = get_presidio_client()\n",
    "else:\n",
    "    if aws_access_key is None or aws_secret_key is None:\n",
    "        print(\"AWS access key or secret key not provided\")\n",
    "        sys.exit(1)\n",
    "    try:\n",
    "        client = get_comprehend_client(aws_access_key, aws_secret_key)\n",
    "    except botocore.exceptions.NoCredentialsError:\n",
    "        print(\"Invalid AWS credentials\")\n",
    "        sys.exit(1)\n",
    "\n",
    "# track if there are columns we want to skip, if not just use empty set\n",
    "try:\n",
    "    skip_columns = set(skip_columns)\n",
    "except:\n",
    "    skip_columns = set()\n",
    "\n",
    "# use proper encoding method\n",
    "if hips_boolean:\n",
    "    hips_method(pii_file, client, method, skip_columns, scoring_threshold)\n",
    "    print(f\"Successfully hid PII in {pii_file}\")\n",
    "else:\n",
    "    encoding_method(encoding_file, pii_file, name_col, hash_col, client, method, skip_columns, scoring_threshold)\n",
    "    print(f\"Successfully encrypted {pii_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23402e26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
