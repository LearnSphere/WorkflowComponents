{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c989f088",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Deidentification script that gives the user the option between Azure langauge services (paid), Amazon Comprehend(paid), or Presidio (free) for pii\n",
    "detection. Also, when it comes to replacing the pii the user has the option to either encode with hash values or to use HIPS.\n",
    "\"\"\"\n",
    "import argparse\n",
    "import csv\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import warnings\n",
    "from typing import Union, Tuple\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from faker import Faker\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "from presidio_analyzer import AnalyzerEngine\n",
    "from presidio_analyzer.nlp_engine import SpacyNlpEngine\n",
    "import spacy\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    import boto3\n",
    "    import botocore.exceptions\n",
    "from jproperties import Properties\n",
    "from email_validator import validate_email, EmailNotValidError\n",
    "\n",
    "#from azureConfig import loginConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2210da55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_azure_client(API_KEY, END_POINT) -> TextAnalyticsClient:\n",
    "    \"\"\"Retrieves the azure client so we can interact with api\"\"\"\n",
    "\n",
    "    client = TextAnalyticsClient(\n",
    "        endpoint=END_POINT,\n",
    "        credential=AzureKeyCredential(API_KEY)\n",
    "    )\n",
    "    return client\n",
    "#print(get_azure_client())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86c1062e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_presidio_client() -> AnalyzerEngine:\n",
    "    \"\"\"Initializes the presidio engine we will use for pii detection\"\"\"\n",
    "    nlp = spacy.load('en_core_web_lg')\n",
    "    #nlp = spacy.load('en_core_web_sm')\n",
    "    class CustomSpacyNlpEngine(SpacyNlpEngine):\n",
    "        \"\"\"Custom nlp model that extends the SpacyNlpEngine from presidio.\"\"\"\n",
    "        def __init__(self, model):\n",
    "            super().__init__()\n",
    "            self.nlp = {\"en\": model}\n",
    "    return AnalyzerEngine(nlp_engine=CustomSpacyNlpEngine(nlp))\n",
    "#print(get_presidio_client())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "664d6b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comprehend_client(access_key, secret_key) -> boto3.client:\n",
    "    \"\"\"\n",
    "    Retrieves the AWS Comprehend client so we can interact with api.\n",
    "    After configuring your aws account on the cl a .aws/credentials is created in the local directory\n",
    "    for boto3 to reference when it comes to access keys\n",
    "    \"\"\"\n",
    "\n",
    "    client = boto3.client(\n",
    "        service_name='comprehend',\n",
    "        aws_access_key_id=access_key,\n",
    "        aws_secret_access_key=secret_key,\n",
    "        region_name='us-west-2'\n",
    "    )\n",
    "\n",
    "    return client\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d7d04712",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_encoding_file(hash_key, value) -> None:\n",
    "    \"\"\"update our encoding file as we go with any new hashes we find\"\"\"\n",
    "    with open(updated_encoding_file, 'a', newline='', encoding='utf-8') as f:\n",
    "        if f.tell() == 0:\n",
    "            f.write('\\n')\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([value, hash_key])\n",
    "\n",
    "def load_name_hash_mapping(csv_file, name_col, hash_col) -> None:\n",
    "    \"\"\"\n",
    "    This function reads in a csv file and fills our name_hash_dict dictionary that represents the key, value pairs \n",
    "    between names and their encoded hashes.\n",
    "    \"\"\"\n",
    "\n",
    "    name_hash_df = pd.read_csv(csv_file)\n",
    "    name_id = name_hash_df.columns.get_loc(name_col)\n",
    "    hash_id = name_hash_df.columns.get_loc(hash_col)\n",
    "    for _, row in name_hash_df.iterrows():\n",
    "        name, hash_value = row.iloc[name_id].strip(), row.iloc[hash_id].strip()\n",
    "        name_hash_dict[name.lower()] = hash_value\n",
    "        \n",
    "def is_email(entity) -> bool:\n",
    "    \"\"\"Checks if a string is a valid email address\"\"\"\n",
    "    try:\n",
    "        validate_email(entity, check_deliverability=False)\n",
    "        return True\n",
    "    except EmailNotValidError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "141c4369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def redact_pii(text, client, method, hips_boolean) -> Union[str, Tuple[None, Union[str, Exception]]]:\n",
    "    \"\"\"\n",
    "    Takes in text and using the apropiate client (Azure/Presidio/Comprehend) will either encode/HIPS for all instances of pii in the text \n",
    "    PARAMETERS:\n",
    "    client - \n",
    "        The tool we're using to detect pii -> Azure/Presidio/Comprehend\n",
    "    method - \n",
    "        String representation of the tool that we'll be using so that we can make the apropiate client calls\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or len(text) == 0:\n",
    "        return text\n",
    "    try:\n",
    "        #build aprop client for our method\n",
    "        if method == 'azure':\n",
    "            response = client.recognize_pii_entities([text], language=\"en\")\n",
    "            if response is None:\n",
    "                return None, \"Error: No response received from recognize_pii_entities.\"\n",
    "            response = response[0].entities\n",
    "\n",
    "        elif method == 'presidio':\n",
    "            response = client.analyze(text=text, entities=[\"PERSON\", \"EMAIL_ADDRESS\"], language='en')\n",
    "            if response is None:\n",
    "                return None, \"Error: No response received from analyze.\"\n",
    "        elif method == 'comprehend':\n",
    "            response = client.detect_entities(Text=text, LanguageCode='en')\n",
    "            if response is None:\n",
    "                return None, \"Error: No response received from detect_entities\"\n",
    "            response = response['Entities']\n",
    "        redacted_text = text\n",
    "        # sort in reverse so we dont mess up index order, but not needed\n",
    "        #response.sort(key=lambda x: x.start, reverse=True)\n",
    "        #print(response)\n",
    "        for entity in response: \n",
    "            #filter out all values that dont fall into the aprop person/email categories for our methods\n",
    "            if method == 'azure':\n",
    "                category = entity.category\n",
    "                entity_text = entity.text\n",
    "            elif method == 'presidio':\n",
    "                category = entity.entity_type\n",
    "                entity_text = text[entity.start:entity.end]\n",
    "            elif method == 'comprehend':\n",
    "                category = entity['Type']\n",
    "                entity_text = entity['Text']\n",
    "\n",
    "            if category not in ['Person', 'Email'] and method == 'azure':\n",
    "                continue\n",
    "            if category not in ['PERSON', 'EMAIL_ADDRESS'] and method == 'presidio':\n",
    "                continue\n",
    "            if category not in ['PERSON', 'OTHER', 'ORGANIZATION'] and method == 'comprehend':\n",
    "                continue\n",
    "            #Comprehend doesn't have an email tag, so we have to make sure this OTHER instance is actually an email\n",
    "            if category in ['OTHER'] and not is_email(entity_text):\n",
    "                continue \n",
    "\n",
    "            #switch other -> email for comprehend so we can get prefixes when encoding \n",
    "            if category in ['OTHER'] and method == 'comprehend':\n",
    "                category = 'email'\n",
    "\n",
    "            #switch org -> person for comprehend so we can get prefixes when encoding\n",
    "            if category in ['ORGANIZATION'] and method == 'comprehend':\n",
    "                category = 'person'\n",
    "\n",
    "            #determine if we'll use hips/encoding\n",
    "            if hips_boolean:\n",
    "                redacted_text = hide_pii(entity_text, category, redacted_text)\n",
    "            else:\n",
    "                redacted_text = encode_pii(entity_text, category, redacted_text)\n",
    "        return redacted_text\n",
    "    \n",
    "    #catch if anything goes wrong, return as tuple so we can be flagged at the None then print the error to user\n",
    "    except Exception as e:\n",
    "        return None, e\n",
    "    \n",
    "#test presidio\n",
    "#line_text = redact_pii(\"hello, Jaden, John Smith can you hear me? hcheng688@hotmail.com\", get_presidio_client(), \"presidio\", True)\n",
    "#line_text = redact_pii(\"hello, Jaden, John Smith can you hear me? hcheng688@hotmail.com\", get_presidio_client(), \"presidio\", False)\n",
    "#print(line_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "24e11fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_pii(entity_text, category, current_text) -> str:\n",
    "    \"\"\"Takes in the text of our instance and encodes it based off our hash values\"\"\"\n",
    "    prefix = category.lower() if category.lower() in ['person', 'email', 'email_address'] else 'obj'\n",
    "    if entity_text.lower() in name_hash_dict:\n",
    "        current_text = current_text.replace(entity_text, name_hash_dict[entity_text.lower()])\n",
    "    else:\n",
    "        #if not in our dict generate new hash value for entity and update the dict\n",
    "        new_index = len(name_hash_dict) + 1\n",
    "        hash_value = f\"{prefix}_{new_index:07d}\"\n",
    "        name_hash_dict[entity_text.lower()] = hash_value\n",
    "        update_encoding_file(hash_value, entity_text.lower())\n",
    "        current_text = current_text.replace(entity_text, name_hash_dict[entity_text.lower()])\n",
    "    return current_text\n",
    "\n",
    "def hide_pii(entity_text, category, current_text) -> str:\n",
    "    \"\"\"Takes in the text of our instance and generates a fake value to hip\"\"\"\n",
    "    if category.lower() == \"person\":\n",
    "        if entity_text.lower() in name_hips_dict:\n",
    "            random_value = name_hips_dict[entity_text.lower()]\n",
    "        else:\n",
    "            random_value = RANDOMS.first_name()\n",
    "            name_hips_dict[entity_text.lower()] = random_value\n",
    "            update_encoding_file(random_value, entity_text.lower())\n",
    "\n",
    "    elif category.lower() == \"email\" or category.lower() == 'email_address' or category.lower() == 'other':\n",
    "        if entity_text.lower() in name_hips_dict:\n",
    "            random_value = name_hips_dict[entity_text.lower()]\n",
    "        else:\n",
    "            random_value = RANDOMS.email()\n",
    "            name_hips_dict[entity_text.lower()] = random_value\n",
    "            update_encoding_file(random_value, entity_text.lower())\n",
    "    else:\n",
    "        #keep the same \n",
    "        random_value = entity_text\n",
    "    current_text = current_text.replace(entity_text, random_value)\n",
    "    return current_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "80f95a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_csv(output_file, transcript_file, client, method, hips_boolean, ignore_columns) -> None:\n",
    "    \"\"\"This file is meant to handle when csvs are passed in\"\"\"\n",
    "\n",
    "    df = pd.read_csv(transcript_file, quotechar='\"')\n",
    "    print(f\"starting to encode: {transcript_file}\")\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8', newline='\\n') as file:\n",
    "        file.write(','.join(df.columns) + '\\n')\n",
    "\n",
    "        columns = df.columns\n",
    "        for index, row in df.iterrows():\n",
    "            cleaned_row = []\n",
    "            column_index = 0\n",
    "            #goes cell by cell -> row by row to detect, update pii\n",
    "            for cell in row:\n",
    "                if pd.isnull(cell):\n",
    "                    cell = ''\n",
    "                #check if we should skip\n",
    "                cell_column = columns[column_index]\n",
    "                if cell_column in ignore_columns:\n",
    "                    column_index += 1\n",
    "                    if isinstance(cell, str) and len(cell) > 0:\n",
    "                        #escape double quote\n",
    "                        cell = cell.replace('\"', r'\\\"')\n",
    "                        #add quote \n",
    "                        cell = f'\"{cell}\"'\n",
    "                    cleaned_row.append(cell)\n",
    "                    continue\n",
    "\n",
    "                redacted_cell = redact_pii(cell, client, method, hips_boolean)\n",
    "                #check if comma is present, add quote\n",
    "#                 if \",\" in str(redacted_cell):\n",
    "#                     redacted_cell = f'\"{redacted_cell}\"'\n",
    "                if isinstance(redacted_cell, str) and len(redacted_cell) > 0:\n",
    "                    #escape double quote\n",
    "                    redacted_cell = redacted_cell.replace('\"', r'\\\"')\n",
    "                    #add quote\n",
    "                    redacted_cell = f'\"{redacted_cell}\"'\n",
    "                if isinstance(redacted_cell, tuple):\n",
    "                    sys.exit(f\"Managed to process {index-1} rows before failing. We encountered the following error: {redacted_cell[1]}\")\n",
    "\n",
    "                cleaned_row.append(f'{redacted_cell}')\n",
    "                column_index += 1\n",
    "            file.write(','.join(map(str, cleaned_row)) + '\\n')\n",
    "            if (index+1) % CSV_ROW_UPDATE == 0:\n",
    "                print(f\"In progress: {index} rows encoded\")\n",
    "        print(f\"{len(df)} rows completed\")\n",
    "\n",
    "def handle_other(output_file, transcript_file, client, method, hips_boolean) -> None:\n",
    "    \"\"\"\n",
    "    This method is meant to handle text files that can be passed in\n",
    "    PARAMETERS:\n",
    "    output_file - \n",
    "        The file we mean to write all of our output to\n",
    "    transcript_file - \n",
    "        The file with pii that was originall passed into the script on the cl\n",
    "    \"\"\"\n",
    "    with open(transcript_file, 'r', encoding='utf-8') as file:\n",
    "        transcript = file.read()\n",
    "\n",
    "    #azure has a cap on the max amount of lines that can be passed in at once, batch here\n",
    "    chunks = [transcript[i:i+5120] for i in range(0, len(transcript), 5120)]\n",
    "    print(f\"starting to encode: {transcript_file}\")\n",
    "    with open(output_file, 'w', encoding='utf-8', newline='\\n') as file:\n",
    "        index = 1\n",
    "        for chunk in chunks:\n",
    "            modified_chunk = redact_pii(chunk, client, method, hips_boolean)\n",
    "            if isinstance(modified_chunk, tuple):\n",
    "                sys.exit(f\"Managed to process {index-1} chunks before failing. We encountered the following error: {modified_chunk[1]}\")\n",
    "            file.write(modified_chunk)\n",
    "            print(f\"finished chunk {index}\")\n",
    "            index += 1\n",
    "\n",
    "#handle_other(\"random_transcript_cleaned.json\", \"random_transcript.json\", get_presidio_client(), \"presidio\", False)\n",
    "#handle_other(\"random_transcript_cleaned.json\", \"random_transcript.json\", get_presidio_client(), \"presidio\", True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e154b8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hips_method(pii_file, client, method, ignore_columns) -> None:\n",
    "    \"\"\"Declares file name for the returned pii file and preps clients to use hips when replacing pii\"\"\"\n",
    "    filename, file_extension = os.path.splitext(os.path.basename(pii_file))\n",
    "    return_pii_file = f\"{filename}_cleaned{file_extension}\"\n",
    "    \n",
    "    with open(updated_encoding_file, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"name\", \"hash\"])\n",
    "\n",
    "    try:\n",
    "        if file_extension == '.csv':\n",
    "            handle_csv(return_pii_file, pii_file, client, method, True, ignore_columns)\n",
    "        else:\n",
    "            handle_other(return_pii_file, pii_file, client, method, True)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the PII file: {e}\")\n",
    "        sys.exit(1)\n",
    "        \n",
    "def encoding_method(encoding_file, pii_file, name_col, hash_col, client, method, ignore_columns) -> None:\n",
    "    \"\"\"Declares file names for the returned pii file and the encodings we have. Preps clients to use encoding when replacing pii\"\"\"\n",
    "    global updated_encoding_file\n",
    "    try:\n",
    "        load_name_hash_mapping(encoding_file, name_col, hash_col)\n",
    "        filename, file_extension = os.path.splitext(os.path.basename(encoding_file))\n",
    "        updated_encoding_file = f\"{filename}_updated{file_extension}\"\n",
    "        shutil.copyfile(encoding_file, updated_encoding_file)\n",
    "    except:\n",
    "        print('No passed in encoding file, will start with empty one')\n",
    "        with open(updated_encoding_file, 'w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"name\", \"hash\"])\n",
    "\n",
    "    filename, file_extension = os.path.splitext(os.path.basename(pii_file))\n",
    "    return_pii_file = f\"{filename}_cleaned{file_extension}\"\n",
    "\n",
    "    try:\n",
    "        if file_extension == '.csv':\n",
    "            handle_csv(return_pii_file, pii_file, client, method, False, ignore_columns)\n",
    "        else:\n",
    "            handle_other(return_pii_file, pii_file, client, method, False)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the PII file: {e}\")\n",
    "        sys.exit(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2e00d3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "#API_KEY = loginConfig['API_KEY']\n",
    "#END_POINT = loginConfig['END_POINT']\n",
    "name_hash_dict = {}\n",
    "name_hips_dict = {}\n",
    "updated_encoding_file = \"updated_encoding_file.csv\"\n",
    "CSV_ROW_UPDATE = 100\n",
    "RANDOMS = Faker()\n",
    "DATE_TIME = datetime.now().strftime(\"%Y%m%d_%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "52db73a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to encode: Zoom_Mathia_Mohawk_10_24_2024_1305_Geramita_simple.csv\n",
      "In progress: 99 rows encoded\n",
      "In progress: 199 rows encoded\n",
      "In progress: 299 rows encoded\n",
      "In progress: 399 rows encoded\n",
      "In progress: 499 rows encoded\n",
      "In progress: 599 rows encoded\n",
      "In progress: 699 rows encoded\n",
      "In progress: 799 rows encoded\n",
      "868 rows completed\n",
      "Successfully hid PII in Zoom_Mathia_Mohawk_10_24_2024_1305_Geramita_simple.csv\n"
     ]
    }
   ],
   "source": [
    "#test on command line\n",
    "#C:\\Users\\hchen\\Anaconda3\\envs\\36_env\\python.exe azidioPii_LS_py36.py -programDir . -workingDir . -userId 1 -Hips_boolean No -method Presidio -piiFileType Non-CSV -skipCol No -useEncoding No -node 0 -fileIndex 0 random_transcript.json\n",
    "#config file, hips no, azure, noncsv, skip no\n",
    "#C:\\Users\\hchen\\Anaconda3\\python.exe azidioPii_LS.py -programDir . -workingDir . -userId hcheng -Hips_boolean No -method Azure -piiFileType Non-CSV -skipCol No -useEncoding No -node 0 -fileIndex 0 random_transcript.json -node 2 -fileIndex 0 config_file.txt\n",
    "#C:\\Users\\hchen\\Anaconda3\\envs\\36_env\\python.exe azidioPii_LS_py36.py -programDir . -workingDir . -userId hcheng -Hips_boolean Yes -method Presidio -piiFileType CSV -skipCol Yes -skip_columns_nodeIndex 0 -skip_columns_fileIndex 0 -skip_columns \"CF (Rule Id)\" -useEncoding No -node 0 -fileIndex 0 Zoom_Mathia_Mohawk_10_24_2024_1305_Geramita_simple.csv\n",
    "#C:\\Users\\hchen\\Anaconda3\\envs\\36_env\\python.exe azidioPii_LS_py36.py -programDir . -workingDir . -userId hcheng -Hips_boolean Yes -method Presidio -piiFileType CSV -skipCol Yes -skip_columns_nodeIndex 0 -skip_columns_fileIndex 0 -skip_columns \"CF (Rule Id)\" -useEncoding No -node 0 -fileIndex 0 Zoom_Mathia_Mohawk_10_24_2024_1305_Geramita_simple_cleaned.csv\n",
    "#command line\n",
    "command_line = False\n",
    "if command_line:\n",
    "    parser = argparse.ArgumentParser(description=\"Deidentification script using Azure, Comprehend, or Presidio\")\n",
    "    parser.add_argument('-programDir', type=str, help='the component program directory')\n",
    "    parser.add_argument('-workingDir', type=str, help='the component instance working directory')\n",
    "    parser.add_argument(\"-fileIndex\", nargs=2, action='append')\n",
    "    parser.add_argument(\"-node\", action='append')\n",
    "    parser.add_argument(\"-method\", help=\"Method to use for deidentification: 'Azure', 'Comprehend' or 'Presidio'\", type=str, required=True, choices=['Azure', 'Presidio', 'Comprehend'])\n",
    "    parser.add_argument(\"-Hips_boolean\", help=\"Boolean to decide which method to use.\", type=str, choices=['Yes', 'No'], default=\"No\")\n",
    "    parser.add_argument(\"-useEncoding\", help=\"Boolean to decide if using encoding.\", type=str, choices=['Yes', 'No'], default=\"No\")\n",
    "    parser.add_argument(\"-skipCol\", type=str, choices=['Yes', 'No'], default=\"No\")\n",
    "    parser.add_argument(\"-skip_columns\", action='append')\n",
    "    parser.add_argument(\"-piiFileType\", help=\"File type.\", type=str, choices=['CSV', 'Non-CSV'], default=\"Non-CSV\")\n",
    "    parser.add_argument('-Hash_col', type=str, help='column to use for hash in the encoding file')\n",
    "    parser.add_argument('-name_col', type=str, help='column to use for name in the encoding file')\n",
    "    parser.add_argument(\"-aws_access_key\", help=\"Access key for the account that we want to use for the amazon comprehend call\", type=str)\n",
    "    parser.add_argument(\"-aws_secret_key\", help=\"Secret key for the account that we want to use azure\", type=str)\n",
    "    parser.add_argument(\"-api_key\", help=\"API key for the account that we want to use azure\", type=str)\n",
    "    parser.add_argument(\"-end_point\", help=\"END POINT for the account that we want to use azure\", type=str)\n",
    "    #parser.add_argument(\"-use_config\", help=\"Use config file.\", type=str, choices=['Yes', 'No'], default=\"No\")\n",
    "    \n",
    "\n",
    "    #args = parser.parse_args()\n",
    "    args, option_file_index_args = parser.parse_known_args()\n",
    "    working_dir = args.workingDir\n",
    "    program_dir = args.programDir\n",
    "    pii_file = None\n",
    "    encoding_file = None\n",
    "    #config_file = None\n",
    "    \n",
    "    \n",
    "    \n",
    "    for x in range(len(args.node)):\n",
    "        if (args.node[x][0] == \"0\" and args.fileIndex[x][0] == \"0\"):\n",
    "            pii_file = args.fileIndex[x][1]\n",
    "        if (args.node[x][0] == \"1\" and args.fileIndex[x][0] == \"0\"):\n",
    "            encoding_file = args.fileIndex[x][1]\n",
    "#         if (args.node[x][0] == \"2\" and args.fileIndex[x][0] == \"0\"):\n",
    "#             config_file = args.fileIndex[x][1]\n",
    "\n",
    "    method = (args.method).lower()\n",
    "    hips_boolean = False\n",
    "    if (args.Hips_boolean).lower() == \"yes\":\n",
    "        hips_boolean = True\n",
    "    skip_columns = None\n",
    "    if args.skipCol == \"Yes\" and args.skip_columns is not None:\n",
    "        skip_columns = args.skip_columns\n",
    "    else:\n",
    "        skip_columns = []\n",
    "    hash_col = None\n",
    "    name_col = None\n",
    "    if args.Hash_col is not None:\n",
    "        hash_col = args.Hash_col\n",
    "    if args.name_col is not None:\n",
    "        name_col = args.name_col\n",
    "        \n",
    "    aws_access_key = None\n",
    "    aws_secret_key = None\n",
    "    api_key = None\n",
    "    end_point = None\n",
    "    if method in ['azure', 'comprehend']:\n",
    "        #if (args.use_config).lower() == \"no\":\n",
    "        if args.aws_access_key is not None:\n",
    "            aws_access_key = args.aws_access_key\n",
    "        if args.aws_secret_key is not None:\n",
    "            aws_secret_key = args.aws_secret_key\n",
    "        if args.api_key is not None:\n",
    "            api_key = args.api_key\n",
    "        if args.end_point is not None:\n",
    "            end_point = args.end_point\n",
    "#         else:\n",
    "#             if config_file is not None:\n",
    "#                 configs = Properties()\n",
    "#                 with open(config_file, 'rb') as cfile:\n",
    "#                     configs.load(cfile)\n",
    "#                     if configs.get(\"AWS_ACCESS_KEY\") is not None:\n",
    "#                         aws_access_key = configs.get(\"AWS_ACCESS_KEY\").data\n",
    "#                     if configs.get(\"AWS_SECRET_KEY\") is not None:\n",
    "#                         aws_secret_key = configs.get(\"AWS_SECRET_KEY\").data\n",
    "#                     if configs.get(\"API_KEY\") is not None:\n",
    "#                         api_key = configs.get(\"API_KEY\").data\n",
    "#                     if configs.get(\"END_POINT\") is not None:\n",
    "#                         end_point = configs.get(\"END_POINT\").data\n",
    "                    \n",
    "else:\n",
    "    # use proper client\n",
    "    #method = \"azure\"\n",
    "    method = \"presidio\"\n",
    "\n",
    "    # hips_boolean = True\n",
    "    # encoding_file = None\n",
    "\n",
    "    # hips_boolean = False\n",
    "    # encoding_file = None\n",
    "\n",
    "    hips_boolean = True\n",
    "    encoding_file = \"updated_encoding_file.csv\"\n",
    "    \n",
    "    #pii_file = \"random_transcript.json\"\n",
    "    #pii_file = \"csv_file_test.csv\"\n",
    "    pii_file = \"Zoom_Mathia_Mohawk_10_24_2024_1305_Geramita_simple.csv\"\n",
    "    skipCol = \"Yes\"\n",
    "    skip_columns = [\"CF (Rule Id)\"]\n",
    "    api_key = \"\"\n",
    "    end_point = \"https://remove-pii.cognitiveservices.azure.com/\"\n",
    "    hash_col = \"hash\"\n",
    "    name_col = \"name\"\n",
    "    \n",
    "    \n",
    "#test\n",
    "# print(pii_file)\n",
    "# print(encoding_file)\n",
    "# print(method)\n",
    "# print(hips_boolean)\n",
    "# print(skip_columns)\n",
    "# print(hash_col)\n",
    "# print(name_col)\n",
    "# print(aws_access_key)\n",
    "# print(aws_secret_key)\n",
    "# print(api_key)\n",
    "# print(end_point)\n",
    "    \n",
    "if method == 'azure':\n",
    "    client = get_azure_client(api_key, end_point)\n",
    "elif method == 'presidio':\n",
    "    client = get_presidio_client()\n",
    "else:\n",
    "    if aws_access_key is None or aws_secret_key is None:\n",
    "        print(\"AWS access key or secret key not provided\")\n",
    "        sys.exit(1)\n",
    "    try:\n",
    "        client = get_comprehend_client(aws_access_key, aws_secret_key)\n",
    "    except botocore.exceptions.NoCredentialsError:\n",
    "        print(\"Invalid AWS credentials\")\n",
    "        sys.exit(1)\n",
    "\n",
    "# track if there are columns we want to skip, if not just use empty set\n",
    "try:\n",
    "    skip_columns = set(skip_columns)\n",
    "except:\n",
    "    skip_columns = set()\n",
    "\n",
    "# use proper encoding method\n",
    "if hips_boolean:\n",
    "    hips_method(pii_file, client, method, skip_columns)\n",
    "    print(f\"Successfully hid PII in {pii_file}\")\n",
    "else:\n",
    "    encoding_method(encoding_file, pii_file, name_col, hash_col, client, method, skip_columns)\n",
    "    print(f\"Successfully encrypted {pii_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23402e26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
