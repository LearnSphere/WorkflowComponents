{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import argparse\n",
    "import os\n",
    "import datetime as dt\n",
    "import warnings\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "from scipy.sparse import load_npz, csr_matrix\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from math import sqrt\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, log_loss, brier_score_loss, mean_squared_error\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "from utils.queue import TimeWindowQueue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data_file, working_dir, min_interactions_per_user, kc_col_name, remove_nan_skills, train_split_type=None, train_split=0.8, cv_student=None, cv_item=None, cv_fold=3):\n",
    "    \"\"\"Preprocess dataset.\n",
    "\n",
    "    Arguments:\n",
    "        data_file (str): student-step rollup data file location\n",
    "        min_interactions_per_user (int): minimum number of interactions per student\n",
    "        kc_col_name (str): Skills id column\n",
    "        remove_nan_skills (bool): if True, remove interactions with no skill tag\n",
    "        train_split_type (str): student or item, when None, don't split\n",
    "        train_split (float): proportion of data to use for training, when None, don't split\n",
    "        cv_fold_type (str): student or item; if None, don't run.\n",
    "        cv_fold (int): cv fold; if None, don't run.\n",
    "\n",
    "    Outputs:\n",
    "        df (pandas DataFrame): preprocessed dataset with user_id, item_id,\n",
    "            timestamp, correct and unique skill features\n",
    "        Q_mat (item-skill relationships sparse array): corresponding q-matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(data_file, delimiter='\\t')\n",
    "    df = df.rename(columns={'Anon Student Id': 'user_id',\n",
    "                            'First Attempt': 'correct'})\n",
    "\n",
    "    # Create item from problem and step\n",
    "    df[\"item_id\"] = df[\"Problem Hierarchy\"] + \";\" + df[\"Problem Name\"] + \";\" + df[\"Step Name\"]\n",
    "\n",
    "    # Add timestamp\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"First Transaction Time\"])\n",
    "    df[\"timestamp\"] = df[\"timestamp\"] - df[\"timestamp\"].min()\n",
    "    df[\"timestamp\"] = df[\"timestamp\"].apply(lambda x: x.total_seconds()).astype(np.int64)\n",
    "    \n",
    "    # change to 1 and 0\n",
    "    df['correct'] = df['correct'].astype(str).str.lower()\n",
    "    df.loc[df['correct'].isin(['correct','true','1']), 'correct'] = 1\n",
    "    df.loc[df['correct'] != 1, 'correct'] = 0\n",
    "    df['correct'] = df['correct'].astype(np.int32)\n",
    "    \n",
    "    # Filter nan skills\n",
    "    if remove_nan_skills:\n",
    "        df = df[~df[kc_col_name].isnull()]\n",
    "    else:\n",
    "        df.ix[df[kc_col_name].isnull(), kc_col_name] = 'NaN'\n",
    "\n",
    "    # Drop duplicates\n",
    "    df.drop_duplicates(subset=[\"user_id\", \"item_id\", \"timestamp\"], inplace=True)\n",
    "\n",
    "    # Filter too short sequences\n",
    "    df = df.groupby(\"user_id\").filter(lambda x: len(x) >= min_interactions_per_user)\n",
    "    df[kc_col_name] = df[kc_col_name].astype('str')\n",
    "    # Extract KCs\n",
    "    kc_list = []\n",
    "    for kc_str in df[kc_col_name].unique():\n",
    "        for kc in kc_str.split('~~'):\n",
    "            kc_list.append(kc)\n",
    "    kc_set = set(kc_list)\n",
    "    #kc2idx is skill name to skill id mapping dict\n",
    "    kc2idx = {kc: i for i, kc in enumerate(kc_set)}\n",
    "    \n",
    "    df[\"user_id_orig\"] = df[\"user_id\"].copy()\n",
    "    df[\"user_id\"] = np.unique(df[\"user_id\"], return_inverse=True)[1]\n",
    "    df[\"item_id_orig\"] = df[\"item_id\"].copy()\n",
    "    df[\"item_id\"] = np.unique(df[\"item_id\"], return_inverse=True)[1]\n",
    "    \n",
    "    user2idx = (df[[\"user_id_orig\", \"user_id\"]]).drop_duplicates()\n",
    "    user2idx = user2idx.set_index('user_id_orig').to_dict()['user_id']\n",
    "    \n",
    "    item2idx = (df[[\"item_id_orig\", \"item_id\"]]).drop_duplicates()\n",
    "    item2idx = item2idx.set_index('item_id_orig').to_dict()['item_id']\n",
    "    \n",
    "#     print(kc2idx)\n",
    "#     print(user2idx)\n",
    "#     print(item2idx)\n",
    "    \n",
    "    # Build Q-matrix\n",
    "    #item is the rows and skill is the column\n",
    "    Q_mat = np.zeros((len(df[\"item_id\"].unique()), len(kc_set)))\n",
    "    for item_id, kc_str in df[[\"item_id\", kc_col_name]].values:\n",
    "        for kc in kc_str.split('~~'):\n",
    "            Q_mat[item_id, kc2idx[kc]] = 1 \n",
    "    \n",
    "    # Get unique skill id from combination of all skill ids\n",
    "    unique_skill_ids = np.unique(Q_mat, axis=0, return_inverse=True)[1]\n",
    "    #print(np.unique(Q_mat, axis=0, return_inverse=True))\n",
    "    \n",
    "    df[\"skill_id\"] = unique_skill_ids[df[\"item_id\"]]\n",
    "    \n",
    "    # Sort data temporally\n",
    "    df.sort_values(by=\"timestamp\", inplace=True)\n",
    "\n",
    "    # Sort data by users, preserving temporal order for each user\n",
    "    df = pd.concat([u_df for _, u_df in df.groupby(\"user_id\")])\n",
    "\n",
    "    df = df[[\"user_id\", \"item_id\", \"timestamp\", \"correct\", \"skill_id\"]]\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    # Text files for BKT implementation (https://github.com/robert-lindsey/WCRP/)\n",
    "    bkt_dataset = df[[\"user_id\", \"item_id\", \"correct\"]]\n",
    "    bkt_skills = unique_skill_ids\n",
    "    #reshape (1,-1): one row and python figure out how many columns\n",
    "    #split students into 5 groups\n",
    "    bkt_split = np.random.randint(low=0, high=5, size=df[\"user_id\"].nunique()).reshape(1, -1)\n",
    "    \n",
    "    # Save data\n",
    "    sparse.save_npz(os.path.join(working_dir, \"q_mat.npz\"), sparse.csr_matrix(Q_mat))\n",
    "    df.to_csv(os.path.join(working_dir, \"preprocessed_data.txt\"), sep=\"\\t\", index=False)\n",
    "    np.savetxt(os.path.join(working_dir, \"bkt_dataset.txt\"), bkt_dataset, fmt='%i')\n",
    "    np.savetxt(os.path.join(working_dir, \"bkt_expert_labels.txt\"), bkt_skills, fmt='%i')\n",
    "    np.savetxt(os.path.join(working_dir, \"bkt_splits.txt\"), bkt_split, fmt='%i')\n",
    "    \n",
    "    if train_split_type is not None:\n",
    "        # Train-test split\n",
    "        #this is split by item\n",
    "        if train_split_type == 'item':\n",
    "            #item_id in order: 0,1,.... \n",
    "            items = df[\"item_id\"].unique()\n",
    "            #item_id in order randomized: 137,11,96....\n",
    "            np.random.shuffle(items)\n",
    "            #default is 80% in training, 20% in testing\n",
    "            split = int(train_split * len(items))\n",
    "            train_df = df[df[\"item_id\"].isin(items[:split])]\n",
    "            test_df = df[df[\"item_id\"].isin(items[split:])]\n",
    "        #all other split is defaulted to user\n",
    "        else:   \n",
    "            #user_id in order: 0,1,.... \n",
    "            users = df[\"user_id\"].unique()\n",
    "            #user_id in order randomized: 137,11,96....\n",
    "            np.random.shuffle(users)\n",
    "            #default is 80% in training, 20% in testing\n",
    "            split = int(train_split * len(users))\n",
    "            train_df = df[df[\"user_id\"].isin(users[:split])]\n",
    "            test_df = df[df[\"user_id\"].isin(users[split:])]\n",
    "        train_df.to_csv(os.path.join(working_dir, \"preprocessed_data_train.txt\"), sep=\"\\t\", index=False)\n",
    "        test_df.to_csv(os.path.join(working_dir, \"preprocessed_data_test.txt\"), sep=\"\\t\", index=False)\n",
    "        \n",
    "    if cv_item is not None or cv_student is not None:\n",
    "        # cv split\n",
    "        test_split = 1/cv_fold\n",
    "        #this is split by item\n",
    "        if cv_item:\n",
    "            #item_id in order: 0,1,.... \n",
    "            items = df[\"item_id\"].unique()\n",
    "            #item_id in order randomized: 137,11,96....\n",
    "            np.random.shuffle(items)\n",
    "            split_unit = int(test_split * len(items))\n",
    "            for x in range(cv_fold):\n",
    "                if x < cv_fold-1:\n",
    "                    item_ids_in_fold = items[x*split_unit:(x+1)*split_unit]\n",
    "                else:\n",
    "                    item_ids_in_fold = items[x*split_unit:]\n",
    "                test_df = df.loc[df['item_id'].isin(item_ids_in_fold)]\n",
    "                train_df = df.loc[~df['item_id'].isin(item_ids_in_fold)]\n",
    "                train_df.to_csv(os.path.join(working_dir, f\"preprocessed_data_cv_item_train_fold_{(x+1)}.txt\"), sep=\"\\t\", index=False)\n",
    "                test_df.to_csv(os.path.join(working_dir, f\"preprocessed_data_cv_item_test_fold_{(x+1)}.txt\"), sep=\"\\t\", index=False)\n",
    "        if cv_student:    \n",
    "            #user_id in order: 0,1,.... \n",
    "            users = df[\"user_id\"].unique()\n",
    "            #user_id in order randomized: 137,11,96....\n",
    "            np.random.shuffle(users)\n",
    "            split_unit = int(test_split * len(users))\n",
    "            for x in range(cv_fold):\n",
    "                if x < cv_fold-1:\n",
    "                    user_ids_in_fold = users[x*split_unit:(x+1)*split_unit]\n",
    "                else:\n",
    "                    user_ids_in_fold = users[x*split_unit:]\n",
    "                test_df = df.loc[df['user_id'].isin(user_ids_in_fold)]\n",
    "                train_df = df.loc[~df['user_id'].isin(user_ids_in_fold)]\n",
    "                train_df.to_csv(os.path.join(working_dir, f\"preprocessed_data_cv_student_train_fold_{(x+1)}.txt\"), sep=\"\\t\", index=False)\n",
    "                test_df.to_csv(os.path.join(working_dir, f\"preprocessed_data_cv_student_test_fold_{(x+1)}.txt\"), sep=\"\\t\", index=False)\n",
    "    return df, Q_mat, kc2idx, user2idx, item2idx\n",
    "#test\n",
    "# print(\"before time for preparing data: \", datetime.now().strftime(\"%H:%M:%S\"))    \n",
    "# prepare_data(data_file=\"ds76_student_step_All_Data_74_2020_0926_034727.txt\", working_dir=\".\",\n",
    "#             min_interactions_per_user=1,\n",
    "#             kc_col_name=\"KC (Circle-Collapse)\",\n",
    "#             remove_nan_skills=True, \n",
    "#             train_split_type='user',\n",
    "#              cv_student=True, cv_item=True, cv_fold=3\n",
    "#             )\n",
    "# print(\"after time for preparing data: \", datetime.now().strftime(\"%H:%M:%S\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi(x):\n",
    "    return np.log(1 + x)\n",
    "\n",
    "###### opportunity always starts at 1 and logged!!! This is done in phi()\n",
    "#seconds in month, week, day, hour\n",
    "WINDOW_LENGTHS = [3600 * 24 * 30, 3600 * 24 * 7, 3600 * 24, 3600]\n",
    "NUM_WINDOWS = len(WINDOW_LENGTHS) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_sparse(df, Q_mat, active_features):\n",
    "    \"\"\"Build sparse dataset from dense dataset and q-matrix.\n",
    "\n",
    "    Arguments:\n",
    "        df (pandas DataFrame): output by prepare_data\n",
    "        Q_mat (sparse array): q-matrix, output by prepare_data\n",
    "        active_features (list of str): features\n",
    "\n",
    "    Output:\n",
    "        sparse_df (sparse array): sparse dataset where first 5 columns are the same as in df\n",
    "    \"\"\"\n",
    "    num_items, num_skills = Q_mat.shape\n",
    "    features = {}\n",
    "\n",
    "    # Counters for continuous time windows\n",
    "    counters = defaultdict(lambda: TimeWindowQueue(WINDOW_LENGTHS))\n",
    "    \n",
    "    #when a key is not found in counters, a new entry will be entered to counter as key:(a new class of TimeWindowQueue)\n",
    "    #TimeWindowQueue is defined in util/queue.py\n",
    "#     test_counters = defaultdict(lambda: TimeWindowQueue(WINDOW_LENGTHS))\n",
    "#     print(test_counters[\"mine\"].window_lengths)\n",
    "#     print(test_counters[\"mine\"].cursors)\n",
    "\n",
    "    # Transform q-matrix into dictionary for fast lookup\n",
    "    # Q_mat_dict has item_id as key, set of skill id as value, e.g., 0: {103, 79}, 1: {103, 79},\n",
    "    Q_mat_dict = {i: set() for i in range(num_items)}\n",
    "    for i, j in np.argwhere(Q_mat == 1):\n",
    "        Q_mat_dict[i].add(j)\n",
    "    #print(Q_mat_dict)\n",
    "    \n",
    "\n",
    "    # Keep track of original dataset\n",
    "    features['df'] = np.empty((0, len(df.keys())))\n",
    "    #features's df element is an empty array with o rows and 5 columns\n",
    "    \n",
    "    # Skill features\n",
    "    if 's' in active_features:\n",
    "        features[\"s\"] = sparse.csr_matrix(np.empty((0, num_skills)))\n",
    "    #features's s is an empty sparse matrix with o rows and num_skills columns\n",
    "    \n",
    "    # Past attempts and wins features\n",
    "    #-w: historical counts include wins\n",
    "    #-a: historical counts include attempts\n",
    "    #-tw: historical counts are encoded as time windows\n",
    "    for key in ['a', 'w']:\n",
    "        if key in active_features:\n",
    "            if 'tw' in active_features: #with tw and a or/and w\n",
    "                features[key] = sparse.csr_matrix(np.empty((0, (num_skills + 2) * NUM_WINDOWS)))\n",
    "            else: #without tw and a or/and w\n",
    "                features[key] = sparse.csr_matrix(np.empty((0, num_skills + 2)))\n",
    "    #features's a or w element is an empty array with o rows and num_skills + 2 columns if without tw\n",
    "    #features'sa or w element is an empty array with o rows and (num_skills+2)*5 columns if with tw (5 is NUM_WINDOWS)\n",
    "    \n",
    "    # Build feature rows by iterating over users\n",
    "    #count = 1\n",
    "    for user_id in df[\"user_id\"].unique():\n",
    "#         if count >2:\n",
    "#             break\n",
    "#         count = count +1\n",
    "        df_user = df[df[\"user_id\"] == user_id][[\"user_id\", \"item_id\", \"timestamp\", \"correct\", \"skill_id\"]].copy()\n",
    "        #turn dataframe to array of list, as rows are array, values of each column is elements of list\n",
    "        df_user = df_user.values\n",
    "        num_items_user = df_user.shape[0]\n",
    "        #get skills from q_mat based on item_id: skills's format: [[0. 0. 0. ... 0. 0. 0.][0. 0. 0. ... 0. 0. 0.].....]\n",
    "        skills = Q_mat[df_user[:, 1].astype(int)].copy()\n",
    "        #vstack: stack arrays in sequence vertically, features['df'] keeps entire df\n",
    "        features['df'] = np.vstack((features['df'], df_user))\n",
    "        #put item_ids in matrix row x 1\n",
    "        item_ids = df_user[:, 1].reshape(-1, 1)\n",
    "        #put success in matrix row x 1\n",
    "        labels = df_user[:, 3].reshape(-1, 1)\n",
    "        # Current skills one hot encoding\n",
    "        if 's' in active_features:\n",
    "            #turn skills into \n",
    "            #(0,20) 1 \n",
    "            #(1,20) 1  \n",
    "            #(2, 49) 1.0\n",
    "            #(2, 53) 1.0 formated matrix, i.e. third row has multi skills: 49 and 53\n",
    "            #save to features['s']\n",
    "            features['s'] = sparse.vstack([features[\"s\"], sparse.csr_matrix(skills)])\n",
    "            \n",
    "        \n",
    "        # Attempts\n",
    "        if 'a' in active_features:\n",
    "            # Time windows\n",
    "            if 'tw' in active_features:\n",
    "                #make a matrix of row_count x (each skill + 2)*5 (5 is seconds in month, week, day, hour, <hour)\n",
    "                attempts = np.zeros((num_items_user, (num_skills + 2) * NUM_WINDOWS))\n",
    "                #df_user[:,1:3]: item_id and timestamp\n",
    "                for i, (item_id, ts) in enumerate(df_user[:, 1:3]):\n",
    "                    # Past attempts for relevant skills: skills historical counts\n",
    "                    if 'sc' in active_features:\n",
    "                        #Q_mat_dict[item_id]: gives skill_id(s) for item like: {12,41,109}\n",
    "                        for skill_id in Q_mat_dict[item_id]:\n",
    "                            #for this feature, counters dict user key=(user_id, skill_id, \"skill\"); value=TimeWindowQueue\n",
    "                            #TimeWindowQueue is defined in queue.py file \n",
    "                            counts = phi(np.array(counters[user_id, skill_id, \"skill\"].get_counters(ts)))\n",
    "                            #in attempts, for each row, there are 5 (i.e. NUM_WINDOWS) columns for each skill that the item has\n",
    "                            attempts[i, skill_id * NUM_WINDOWS:(skill_id + 1) * NUM_WINDOWS] = counts\n",
    "                            #reset the counters for (user_id, skill_id, \"skill\")\n",
    "                            counters[user_id, skill_id, \"skill\"].push(ts)\n",
    "\n",
    "                    # Past attempts for item: items historical counts\n",
    "                    if 'ic' in active_features:\n",
    "                        #for this feature, counters dict uses key=(user_id, skill_id, \"item\"); value=TimeWindowQueue\n",
    "                        #TimeWindowQueue is defined in queue.py file\n",
    "                        counts = phi(np.array(counters[user_id, item_id, \"item\"].get_counters(ts)))\n",
    "                        #add to the second last set of attempts' columns (remember: attempts is row X (num_skills+2))\n",
    "                        attempts[i, -2 * NUM_WINDOWS:-1 * NUM_WINDOWS] = counts\n",
    "                        #reset the counters for (user_id, skill_id, \"item\")\n",
    "                        counters[user_id, item_id, \"item\"].push(ts)\n",
    "\n",
    "                    # Past attempts for all items\n",
    "                    if 'tc' in active_features:\n",
    "                        #for this feature, counters dict uses key=user_id; value=TimeWindowQueue\n",
    "                        #TimeWindowQueue is defined in queue.py file\n",
    "                        counts = phi(np.array(counters[user_id].get_counters(ts)))\n",
    "                        #add to the last set of attempts' columns (remember: attempts is row X (num_skills+2))\n",
    "                        attempts[i, -1 * NUM_WINDOWS:] = counts\n",
    "                        counters[user_id].push(ts)\n",
    "\n",
    "            # Counts\n",
    "            else:\n",
    "                #attempts is all 0 array of row x (num_skills + 2)\n",
    "                attempts = np.zeros((num_items_user, num_skills + 2))\n",
    "\n",
    "                # Past attempts for relevant skills\n",
    "                if 'sc' in active_features:\n",
    "                    #add an all 0 rows to skills (rowsxnum_skills); and delete last row\n",
    "                    tmp = np.vstack((np.zeros(num_skills), skills))[:-1]\n",
    "                    #np.cumsum(tmp, 0):\n",
    "                    #add all previous rows together to get opportunity count for each skill: e.g. 6 skills\n",
    "                    #90 80 0 52 18 0 second last row \n",
    "                    #90 81 1 52 18 0 meaning last row sees two skills: skill#2 and skill#3\n",
    "                    attempts[:, :num_skills] = phi(np.cumsum(tmp, 0) * skills)\n",
    "                    #attempts is row x (num_skills + 2)\n",
    "                    #each row represent skill log(opportunity+1)\n",
    "                    #e.g. 5 skills,  second row  log2= 0.6931\n",
    "                    #0 0 0 0 0 0 0 (row 1 see skill#2)\n",
    "                    #0 0.69314718 0 0 0 0 0 (row 2 skill#2 again)\n",
    "                    #0 0 0 0 0 0 0 (row 3 skill#1)\n",
    "                    #0.69314718 1.0986 0 0 0 0 0 (row 3 skill#1 again and skill#2 third time)\n",
    "                    \n",
    "\n",
    "                # Past attempts for item\n",
    "                if 'ic' in active_features:\n",
    "                    #OneHotEncoder is from sklearn. \n",
    "                    #The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. \n",
    "                    #The features are encoded using a one-hot (aka ‘one-of-K’ or ‘dummy’) encoding scheme. \n",
    "                    #This creates a binary column for each category and returns a sparse matrix or dense array (depending on the sparse parameter)\n",
    "                    #onehot is to get the code for each item_id. if there are 3 items, there are 3 columns, 100 is 1; 010 is 2, 001 is 3 \n",
    "                    onehot = OneHotEncoder(n_values=df_user[:, 1].max() + 1)\n",
    "                    #item_ids is a matrix of rowX1 to store item_id for each row\n",
    "                    #item_ids_onehot is a matirx row X item_id.max+1; each row represent which item_id it is. e.g. if total 6 items\n",
    "                    #0, 0, 0, 1, 0, 0: this row has item#4\n",
    "                    #1, 0, 0, 0, 0, 0: this row has item#1\n",
    "                    item_ids_onehot = onehot.fit_transform(item_ids).toarray()\n",
    "                    #np.cumsum(item_ids_onehot, 0): add all previous rows count for each column to get the item opportunity \n",
    "                    #tmp is add one row of all 0 array (size of item_id.max+1) to  and delete the last row, matrix of row X item_id.max+1\n",
    "                    tmp = np.vstack((np.zeros(item_ids_onehot.shape[1]), np.cumsum(item_ids_onehot, 0)))[:-1]\n",
    "                    #add to the second last column of attempts with opportunity count for the item of that row, e.g.\n",
    "                    #0 row 1 for item#1\n",
    "                    #0.0 row 2 for item#2\n",
    "                    #1.0 row 3 for item#1\n",
    "                    #2.0 row 3 for item#1\n",
    "                    attempts[:, -2] = phi(tmp[np.arange(num_items_user), df_user[:, 1]])\n",
    "                    \n",
    "                    \n",
    "            \n",
    "                # Past attempts for all items\n",
    "                if 'tc' in active_features:\n",
    "                    #the last column is the log of row No. for the student starting from log(1) to log(total row) for the student \n",
    "                    attempts[:, -1] = phi(np.arange(num_items_user))\n",
    "                \n",
    "\n",
    "            features['a'] = sparse.vstack([features['a'], sparse.csr_matrix(attempts)])\n",
    "\n",
    "        # Wins\n",
    "        if \"w\" in active_features:\n",
    "            # Time windows\n",
    "            if 'tw' in active_features:\n",
    "                wins = np.zeros((num_items_user, (num_skills + 2) * NUM_WINDOWS))\n",
    "\n",
    "                for i, (item_id, ts, correct) in enumerate(df_user[:, 1:4]):\n",
    "                    # Past wins for relevant skills\n",
    "                    if 'sc' in active_features:\n",
    "                        for skill_id in Q_mat_dict[item_id]:\n",
    "                            counts = phi(np.array(counters[user_id, skill_id, \"skill\", \"correct\"].get_counters(ts)))\n",
    "                            wins[i, skill_id * NUM_WINDOWS:(skill_id + 1) * NUM_WINDOWS] = counts\n",
    "                            if correct:\n",
    "                                counters[user_id, skill_id, \"skill\", \"correct\"].push(ts)\n",
    "\n",
    "                    # Past wins for item\n",
    "                    if 'ic' in active_features:\n",
    "                        counts = phi(np.array(counters[user_id, item_id, \"item\", \"correct\"].get_counters(ts)))\n",
    "                        wins[i, -2 * NUM_WINDOWS:-1 * NUM_WINDOWS] = counts\n",
    "                        if correct:\n",
    "                            counters[user_id, item_id, \"item\", \"correct\"].push(ts)\n",
    "\n",
    "                    # Past wins for all items\n",
    "                    if 'tc' in active_features:\n",
    "                        counts = phi(np.array(counters[user_id, \"correct\"].get_counters(ts)))\n",
    "                        wins[i, -1 * NUM_WINDOWS:] = counts\n",
    "                        if correct:\n",
    "                            counters[user_id, \"correct\"].push(ts)\n",
    "\n",
    "            # Counts\n",
    "            else:\n",
    "                wins = np.zeros((num_items_user, num_skills + 2))\n",
    "\n",
    "                # Past wins for relevant skills\n",
    "                if 'sc' in active_features:\n",
    "                    tmp = np.vstack((np.zeros(num_skills), skills))[:-1]\n",
    "                    corrects = np.hstack((np.array(0), df_user[:, 3])).reshape(-1, 1)[:-1]\n",
    "                    wins[:, :num_skills] = phi(np.cumsum(tmp * corrects, 0) * skills)\n",
    "                    \n",
    "\n",
    "                # Past wins for item\n",
    "                if 'ic' in active_features:\n",
    "                    onehot = OneHotEncoder(n_values=df_user[:, 1].max() + 1)\n",
    "                    item_ids_onehot = onehot.fit_transform(item_ids).toarray()\n",
    "                    tmp = np.vstack((np.zeros(item_ids_onehot.shape[1]), np.cumsum(item_ids_onehot * labels, 0)))[:-1]\n",
    "                    wins[:, -2] = phi(tmp[np.arange(num_items_user), df_user[:, 1]])\n",
    "\n",
    "                # Past wins for all items\n",
    "                if 'tc' in active_features:\n",
    "                    wins[:, -1] = phi(np.concatenate((np.zeros(1), np.cumsum(df_user[:, 3])[:-1])))\n",
    "\n",
    "            features['w'] = sparse.vstack([features['w'], sparse.csr_matrix(wins)])\n",
    "\n",
    "    # User and item one hot encodings\n",
    "    onehot = OneHotEncoder()\n",
    "    if 'u' in active_features:\n",
    "        features['u'] = onehot.fit_transform(features[\"df\"][:, 0].reshape(-1, 1))\n",
    "    if 'i' in active_features:\n",
    "        features['i'] = onehot.fit_transform(features[\"df\"][:, 1].reshape(-1, 1))\n",
    "        \n",
    "    '''\n",
    "    for features: s, a, sc\n",
    "    features['df']\n",
    "    0\t159661\t616236\t1\t190 \n",
    "    0\t159662\t616298\t1\t190 \n",
    "    0\t159665\t616339\t1\t114\n",
    "    0\t159666\t616393\t1\t42\n",
    "    0\t159668\t616498\t1\t42\n",
    "    0\t159667\t616533\t1\t108\n",
    "    0\t159669\t616574\t1\t108\n",
    "    0\t159670\t616585\t1\t42\n",
    "\n",
    "    skill combination vs skill_id\n",
    "    190: 20\n",
    "    114: 49, 53\n",
    "    42: 76, 91, 100\n",
    "    108: 49\n",
    "\n",
    "    featrues['a']\n",
    "    (1, 20)\t0.6931471805599453\n",
    "    (4, 76)\t0.6931471805599453\n",
    "    (4, 91)\t0.6931471805599453\n",
    "    (4, 100)\t0.6931471805599453\n",
    "    (5, 49)\t0.6931471805599453\n",
    "    (6, 49)\t1.0986122886681098\n",
    "    (7, 76)\t1.0986122886681098\n",
    "    (7, 91)\t1.0986122886681098\n",
    "    (7, 100)\t1.0986122886681098\n",
    "\n",
    "    features['s']\n",
    "    (0, 20)\t1.0\n",
    "    (1, 20)\t1.0\n",
    "    (2, 49)\t1.0\n",
    "    (2, 53)\t1.0\n",
    "    (3, 76)\t1.0\n",
    "    (3, 91)\t1.0\n",
    "    (3, 100)\t1.0\n",
    "    (4, 76)\t1.0\n",
    "    (4, 91)\t1.0\n",
    "    (4, 100)\t1.0\n",
    "    (5, 49)\t1.0\n",
    "    (6, 49)\t1.0\n",
    "    (7, 76)\t1.0\n",
    "    (7, 91)\t1.0\n",
    "    (7, 100)\t1.0\n",
    "    \n",
    "    when 2 students and each have 1131 4630 rows\n",
    "    for feature s,a,sc:\n",
    "    df: 5761x5\n",
    "    a: 5761x114\n",
    "    s: 5761x112\n",
    "    for feature s,a,sc,u,i,ic,tc,w:\n",
    "    df: 5761x5\n",
    "    a: 5761x114\n",
    "    s: 5761x112\n",
    "    u: 5761x2\n",
    "    i: 5761x5764\n",
    "    w: 5761x114\n",
    "    '''\n",
    "\n",
    "    X = sparse.hstack([sparse.csr_matrix(features['df']),\n",
    "                       sparse.hstack([features[x] for x in features.keys() if x != 'df'])]).tocsr()\n",
    "    return X\n",
    "\n",
    "#test\n",
    "# print(\"before time for encoding data: \", datetime.now().strftime(\"%H:%M:%S\"))    \n",
    "# df = pd.read_csv('preprocessed_data.txt', sep=\"\\t\")\n",
    "# df = df[[\"user_id\", \"item_id\", \"timestamp\", \"correct\", \"skill_id\"]]\n",
    "# Q_mat = sparse.load_npz('q_mat.npz').toarray()\n",
    "# active_features = ['s','a', 'sc']\n",
    "# features_suffix = ''.join(active_features)\n",
    "# X = df_to_sparse(df, Q_mat, active_features)\n",
    "# sparse.save_npz(f\"X-{features_suffix}\", X)\n",
    "# print(\"after time for encoding data: \", datetime.now().strftime(\"%H:%M:%S\"))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_sparse_afm(df, Q_mat):\n",
    "    \"\"\"Build sparse dataset from dense dataset and q-matrix for AFM\n",
    "\n",
    "    Arguments:\n",
    "        df (pandas DataFrame): output by prepare_data\n",
    "        Q_mat (sparse array): q-matrix, output by prepare_data\n",
    "\n",
    "    Output:\n",
    "        sparse_df (sparse array): sparse dataset where first 5 columns are the same as in df, next set of columns are for each students;\n",
    "        followed by skillX2 columns that a pair of columns for each skill (skill existence + opportunity)\n",
    "    \"\"\"\n",
    "    num_items, num_skills = Q_mat.shape\n",
    "    features = {}\n",
    "    \n",
    "    num_users = len(df[\"user_id\"].unique())\n",
    "\n",
    "    # Transform q-matrix into dictionary for fast lookup\n",
    "    # Q_mat_dict has item_id as key, set of skill id as value, e.g., 0: {103, 79}, 1: {103, 79},\n",
    "    Q_mat_dict = {i: set() for i in range(num_items)}\n",
    "    for i, j in np.argwhere(Q_mat == 1):\n",
    "        Q_mat_dict[i].add(j)\n",
    "\n",
    "    # Keep track of original dataset\n",
    "    features['df'] = np.empty((0, len(df.keys())))\n",
    "    #features's df element is an empty array with o rows and 5 columns\n",
    "    \n",
    "    #features's u is an empty sparse matrix with o rows and num_users columns\n",
    "    features[\"u\"] = sparse.csr_matrix(np.empty((0, num_users)))\n",
    "    #features's s is an empty sparse matrix with o rows and num_skills columns\n",
    "    features[\"s\"] = sparse.csr_matrix(np.empty((0, num_skills)))\n",
    "    #features's a is an empty sparse matrix with o rows and num_skills columns\n",
    "    features['a'] = sparse.csr_matrix(np.empty((0, num_skills)))\n",
    "    \n",
    "    # Build feature rows by iterating over users\n",
    "    #count = 1\n",
    "    for user_id in df[\"user_id\"].unique():\n",
    "#         if count >2:\n",
    "#             break\n",
    "#         count = count +1\n",
    "        df_user = df[df[\"user_id\"] == user_id][[\"user_id\", \"item_id\", \"timestamp\", \"correct\", \"skill_id\"]].copy()\n",
    "        #turn dataframe to array of list, as rows are array, values of each column is elements of list\n",
    "        df_user = df_user.values\n",
    "        num_items_user = df_user.shape[0]\n",
    "        #get skills from q_mat based on item_id: skills's format: [[0. 0. 0. ... 0. 0. 0.][0. 0. 0. ... 0. 0. 0.].....]\n",
    "        skills = Q_mat[df_user[:, 1].astype(int)].copy()\n",
    "        #vstack: stack arrays in sequence vertically, features['df'] keeps entire df\n",
    "        features['df'] = np.vstack((features['df'], df_user))\n",
    "        features['s'] = sparse.vstack([features[\"s\"], sparse.csr_matrix(skills)])\n",
    "        #attempts is all 0 array of row x (num_skills)\n",
    "        attempts = np.zeros((num_items_user, num_skills))\n",
    "        #add an all 0 rows to skills (rowsxnum_skills); and delete last row\n",
    "        tmp = np.vstack((np.zeros(num_skills), skills))[:-1]\n",
    "        #np.cumsum(tmp, 0):\n",
    "        #add all previous rows together to get opportunity count for each skill: e.g. 6 skills\n",
    "        #90 80 0 52 18 0 second last row \n",
    "        #90 81 1 52 18 0 meaning last row sees two skills: skill#2 and skill#3\n",
    "        #attempts is row x (num_skills)\n",
    "        #each row represent skill opportunity starting with 0\n",
    "        #e.g. 5 skills,  second row  log2= 0.6931\n",
    "        #0 0 0 0 0 (row 1 see skill#2)\n",
    "        #0 1 0 0 0 (row 2 skill#2 again)\n",
    "        #0 0 0 0 0 (row 3 skill#1)\n",
    "        #1 2 0 0 0 (row 3 skill#1 again and skill#2 third time)\n",
    "        attempts[:, :num_skills] = np.cumsum(tmp, 0) * skills\n",
    "        features['a'] = sparse.vstack([features['a'], sparse.csr_matrix(attempts)])\n",
    "    \n",
    "    #add users\n",
    "    onehot = OneHotEncoder()\n",
    "    features['u'] = onehot.fit_transform(features[\"df\"][:, 0].reshape(-1, 1))\n",
    "    \n",
    "    X = sparse.hstack([sparse.csr_matrix(features['df']), sparse.csr_matrix(features['u']), sparse.csr_matrix(features['s']),\n",
    "                       sparse.csr_matrix(features['a'])]).tocsr()\n",
    "    \n",
    "    ''' X looks like this:\n",
    "    features['df']\n",
    "    0\t159661\t616236\t1\t190 \n",
    "    0\t159662\t616298\t1\t190 \n",
    "    0\t159665\t616339\t1\t114\n",
    "    0\t159666\t616393\t1\t42\n",
    "    0\t159668\t616498\t1\t42\n",
    "    0\t159667\t616533\t1\t108\n",
    "    0\t159669\t616574\t1\t108\n",
    "    0\t159670\t616585\t1\t42\n",
    "\n",
    "    skill combination vs skill_id\n",
    "    190: 20\n",
    "    114: 49, 53\n",
    "    42: 76, 91, 100\n",
    "    108: 49\n",
    "\n",
    "    featrues['a']\n",
    "    (1, 20)\t0.6931471805599453\n",
    "    (4, 76)\t0.6931471805599453\n",
    "    (4, 91)\t0.6931471805599453\n",
    "    (4, 100)\t0.6931471805599453\n",
    "    (5, 49)\t0.6931471805599453\n",
    "    (6, 49)\t1.0986122886681098\n",
    "    (7, 76)\t1.0986122886681098\n",
    "    (7, 91)\t1.0986122886681098\n",
    "    (7, 100)\t1.0986122886681098\n",
    "\n",
    "    features['s']\n",
    "    (0, 20)\t1.0\n",
    "    (1, 20)\t1.0\n",
    "    (2, 49)\t1.0\n",
    "    (2, 53)\t1.0\n",
    "    (3, 76)\t1.0\n",
    "    (3, 91)\t1.0\n",
    "    (3, 100)\t1.0\n",
    "    (4, 76)\t1.0\n",
    "    (4, 91)\t1.0\n",
    "    (4, 100)\t1.0\n",
    "    (5, 49)\t1.0\n",
    "    (6, 49)\t1.0\n",
    "    (7, 76)\t1.0\n",
    "    (7, 91)\t1.0\n",
    "    (7, 100)\t1.0\n",
    "    \n",
    "    when 2 students and each have 1131 4630 rows\n",
    "    for feature s,a,sc:\n",
    "    df: 5761x5\n",
    "    a: 5761x114\n",
    "    s: 5761x112\n",
    "    for feature s,a,sc,u,i,ic,tc,w:\n",
    "    df: 5761x5\n",
    "    a: 5761x114\n",
    "    s: 5761x112\n",
    "    u: 5761x2\n",
    "    i: 5761x5764\n",
    "    w: 5761x114\n",
    "    '''\n",
    "    \n",
    "    return X\n",
    "\n",
    "#test\n",
    "#first get data ready\n",
    "# print(\"before time for preparing data: \", datetime.now().strftime(\"%H:%M:%S\"))    \n",
    "# df, Q_mat, kc2idx, user2idx, item2idx = prepare_data(data_file=\"ds76_student_step_All_Data_74_2020_0926_034727.txt\", working_dir=\".\",\n",
    "#             min_interactions_per_user=1,\n",
    "#             kc_col_name=\"KC (Circle-Collapse)\",\n",
    "#             remove_nan_skills=True\n",
    "#             )\n",
    "# print(\"after time for preparing data: \", datetime.now().strftime(\"%H:%M:%S\"))                 \n",
    "\n",
    "# print(kc2idx)\n",
    "# print(user2idx)\n",
    "\n",
    "# print(\"before time for AFM encoding data: \", datetime.now().strftime(\"%H:%M:%S\"))    \n",
    "# df = pd.read_csv('preprocessed_data.txt', sep=\"\\t\")\n",
    "# df = df[[\"user_id\", \"item_id\", \"timestamp\", \"correct\", \"skill_id\"]]\n",
    "# X = df_to_sparse_afm(df, Q_mat)\n",
    "# print(X)\n",
    "# sparse.save_npz(f\"X-afm\", X)\n",
    "# print(\"after time for AFM encoding data: \", datetime.now().strftime(\"%H:%M:%S\"))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y, y_pred):\n",
    "    acc = accuracy_score(y, np.round(y_pred))\n",
    "    auc = roc_auc_score(y, y_pred)\n",
    "    nll = log_loss(y, y_pred)\n",
    "    mse = brier_score_loss(y, y_pred)\n",
    "    return acc, auc, nll, mse\n",
    "\n",
    "def compute_rmse(y_actual, y_pred):\n",
    "    return sqrt(mean_squared_error(y_actual, y_pred))\n",
    "\n",
    "# calculate ll for regression\n",
    "def calculate_ll(y_actual, y_pred):\n",
    "    ll = -log_loss(y_actual, y_pred)*len(y_actual)\n",
    "    return ll\n",
    "\n",
    "# calculate aic for regression by ll\n",
    "def calculate_aic_by_ll(ll, num_params):\n",
    "    aic = -2*ll + 2*num_params\n",
    "    return aic\n",
    "    \n",
    "# calculate bic for regression by ll\n",
    "def calculate_bic_by_ll(ll, n, num_params):\n",
    "    bic = -2*ll + num_params*np.log(n)\n",
    "    return bic\n",
    "\n",
    "# calculate aic for regression by mse\n",
    "#AIC = n*log(residual sum of squares/n) + 2K\n",
    "def calculate_aic_by_mse(n, mse, num_params):\n",
    "    aic = n * np.log(mse) + 2 * num_params\n",
    "    return aic\n",
    "\n",
    "# calculate bic for regression by mse\n",
    "def calculate_bic_by_mse(n, mse, num_params):\n",
    "    bic = n * np.log(mse) + num_params * np.log(n)\n",
    "    return bic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logToWfl(msg):\n",
    "    logFile = open(\"learner_performnce_prediction.wfl\", \"a\")\n",
    "    now = dt.datetime.now()\n",
    "    logFile.write(str(now) + \": \" + msg + \"\\n\");\n",
    "    logFile.close();\n",
    "    \n",
    "def logProgressToWfl(progressMsg):\n",
    "    logFile = open(\"learner_performnce_prediction.wfl\", \"a\")\n",
    "    now = dt.datetime.now()\n",
    "    progressPrepend = \"%Progress::\"\n",
    "    logFile.write(progressPrepend + \"@\" + str(now) + \"@\" + progressMsg + \"\\n\");\n",
    "    logFile.close();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test command from WF component:\n",
    "#C:/ProgramData/Anaconda3/Python learner_performance_prediction.py -programDir . -workingDir . -userId 1 -includeCv Yes -kcModel_nodeIndex 0 -kcModel_fileIndex 0 -kcModel \"KC (Circle-Collapse)\" -numFold 3 -tpCV \"item blocked\" -node 0 -fileIndex 0 \"C:\\WPIDevelopment\\dev06_dev\\WorkflowComponents\\AnalysisPythonLogisticRegression\\test\\test_data\\ds76_student_step_export.txt\" -f \"ds76_student_step_export.txt\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#fresh new log file\n",
    "logFile = open(\"learner_performnce_prediction.wfl\", \"w\")\n",
    "logFile.close();\n",
    "\n",
    "#command line\n",
    "command_line = False\n",
    "if command_line:\n",
    "    parser = argparse.ArgumentParser(description='Python program to compute logistic regression.')\n",
    "    parser.add_argument('-programDir', type=str, help='the component program directory')\n",
    "    parser.add_argument('-workingDir', type=str, help='the component instance working directory')\n",
    "    parser.add_argument(\"-node\", nargs=1, action='append')\n",
    "    parser.add_argument(\"-fileIndex\", nargs=2, action='append')\n",
    "    parser.add_argument(\"-includeCv\", type=str, choices=[\"Yes\", \"No\"], default=\"No\")\n",
    "    parser.add_argument(\"-kcModel\", type=str)\n",
    "    parser.add_argument(\"-numFold\", type=int, default=3)\n",
    "    parser.add_argument(\"-tpCV\", type=str, choices=[\"student blocked\", \"item blocked\", \"both student and item blocked\" ], default='item blocked')\n",
    "    parser.add_argument('-userId', type=str,  help='placeholder for WF', default='')\n",
    "    args, option_file_index_args = parser.parse_known_args()\n",
    "    #no train_split_type\n",
    "    working_dir = args.workingDir\n",
    "    train_split_type = None\n",
    "    train_split = None\n",
    "    file_name = args.fileIndex[0][1]\n",
    "    kc_col_name = args.kcModel\n",
    "    cv_student = False\n",
    "    cv_item = False\n",
    "    cv_fold = 3\n",
    "    if (args.includeCv).lower() == \"yes\":\n",
    "        if (args.tpCV).lower() == \"item blocked\" or (args.tpCV).lower() == \"both student and item blocked\":\n",
    "            cv_item = True\n",
    "        if (args.tpCV).lower() == \"student blocked\" or (args.tpCV).lower() == \"both student and item blocked\":\n",
    "            cv_student = True\n",
    "        cv_fold = args.numFold\n",
    "else:\n",
    "    #student_step file\n",
    "    #file_name = \"ds1943_student_step_All_Data_3691_2017_0522_203358.txt\"\n",
    "    file_name = \"ds76_student_step_All_Data_74_2020_0926_034727.txt\"\n",
    "    #file_name = \"./data/algebra05/data_DS_format.txt\"\n",
    "    #kc mode\n",
    "    #kc_col_name=\"KC (Default)\"\n",
    "    kc_col_name=\"KC (Circle-Collapse)\"\n",
    "    #kc_col_name=\"KC (Item Model)\"\n",
    "    #kc_col_name=\"KC(Default)\"\n",
    "    #working_dir\n",
    "    working_dir = \".\"\n",
    "    #working_dir = \"./data/algebra05\"\n",
    "    \n",
    "    #for spliting data\n",
    "    train_split_type='item'\n",
    "    train_split=0.8\n",
    "\n",
    "    #for cv\n",
    "    cv_student = True\n",
    "    cv_item = True\n",
    "    cv_fold = 3\n",
    "    \n",
    "#min_interactions_per_user\n",
    "min_interactions_per_user = 1\n",
    "#min_interactions_per_user = 10\n",
    "#remove_nan_skills\n",
    "remove_nan_skills = True\n",
    "\n",
    "#get kc_name\n",
    "kc_name = re.search(r'\\((.*?)\\)',kc_col_name).group(1)\n",
    "\n",
    "logToWfl(\"Calling prepare_data.\")  \n",
    "logProgressToWfl(\"0%\")\n",
    "df, Q_mat, kc2idx, user2idx, item2idx = prepare_data(data_file=file_name, \n",
    "                                                     working_dir=working_dir,\n",
    "                                                     min_interactions_per_user=min_interactions_per_user,\n",
    "                                                     kc_col_name=kc_col_name,\n",
    "                                                     remove_nan_skills=remove_nan_skills,\n",
    "                                                     train_split_type=train_split_type, \n",
    "                                                     train_split=train_split,\n",
    "                                                     cv_student=cv_student, \n",
    "                                                     cv_item=cv_item,\n",
    "                                                     cv_fold=cv_fold)\n",
    "skill_df = pd.DataFrame(list(kc2idx.items()), columns =['skill_name', 'skill_id'])\n",
    "student_df = pd.DataFrame(list(user2idx.items()), columns =['student_name', 'student_id'])\n",
    "logToWfl(\"Finished prepare_data.\")  \n",
    "#approximate % of time\n",
    "logProgressToWfl(\"45%\")\n",
    "\n",
    "logToWfl(\"Calling df_to_sparse_afm to encode data.\")  \n",
    "#df = pd.read_csv('preprocessed_data.txt', sep=\"\\t\")\n",
    "df = df[[\"user_id\", \"item_id\", \"timestamp\", \"correct\", \"skill_id\"]]\n",
    "X = df_to_sparse_afm(df, Q_mat)\n",
    "sparse.save_npz(f\"X-afm\", X)\n",
    "logToWfl(\"Finished df_to_sparse_afm to encode data.\")\n",
    "#approximate % of time\n",
    "logProgressToWfl(\"50%\")\n",
    "\n",
    "logToWfl(\"Starting training data.\")  \n",
    "iter = 1000\n",
    "#do model on all data and get AIC, BIC\n",
    "X_all, y_all = X[:, 5:], X[:, 3].toarray().flatten()\n",
    "# Train\n",
    "#solver{‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}, default=’lbfgs’\n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "model = LogisticRegression(solver=\"lbfgs\", max_iter=iter)\n",
    "model.fit(X_all, y_all)\n",
    "if cv_student or cv_item: \n",
    "    #approximate % of time\n",
    "    logProgressToWfl(\"75%\")\n",
    "\n",
    "\n",
    "#get params\n",
    "params = model.coef_[0]\n",
    "num_params = len(params)\n",
    "#attach to student_name and skill_name\n",
    "student_param = params[:student_df.shape[0]]\n",
    "student_df[\"intercept\"] = student_param\n",
    "student_df = student_df[['student_name', 'intercept']]\n",
    "#print(student_df)\n",
    "end_ind_student = student_df.shape[0]\n",
    "end_ind_skill_intercept = end_ind_student+skill_df.shape[0]\n",
    "skill_intercept_param = params[end_ind_student:end_ind_skill_intercept]\n",
    "skill_slope_param = params[end_ind_skill_intercept:]\n",
    "skill_df[\"intercept\"] = skill_intercept_param\n",
    "skill_df[\"slope\"] = skill_slope_param\n",
    "skill_df = skill_df[['skill_id', 'skill_name', 'intercept', 'slope']]\n",
    "skill_df = skill_df.sort_values(by=['skill_name'])\n",
    "pd.set_option(\"display.max_rows\", None, \"display.max_columns\", None)\n",
    "#print(skill_df)\n",
    "skill_df['intercept_exp'] = np.exp(skill_df['intercept'])\n",
    "skill_df['intercept_probability'] = skill_df['intercept_exp']/(1+skill_df['intercept_exp'])\n",
    "skill_df = skill_df.drop(['skill_id', 'intercept_exp'], axis=1)\n",
    "skill_df = skill_df[['skill_name', 'intercept', 'intercept_probability', 'slope']]\n",
    "\n",
    "#get prediction\n",
    "y_all_pred = pd.DataFrame(model.predict_proba(X_all))\n",
    "#use only the prediction to success\n",
    "y_all_pred = y_all_pred[y_all_pred.columns[1]]\n",
    "df[\"prediction\"] = y_all_pred\n",
    "#get prediction and attached to original file\n",
    "df_original = pd.read_csv(file_name, delimiter='\\t')\n",
    "if remove_nan_skills:\n",
    "    df_original = df_original[~df_original[kc_col_name].isnull()]\n",
    "#error rate prediction!\n",
    "df_original[f\"Predicted Error Rate ({kc_name})\"] = 1-y_all_pred\n",
    "df_original.to_csv(os.path.join(working_dir, \"student_step_with_prediction.txt\"), sep=\"\\t\", index=False)\n",
    "\n",
    "#get ll, aic, bic\n",
    "ll = calculate_ll(y_all, y_all_pred)\n",
    "aic = calculate_aic_by_ll(ll, num_params)\n",
    "bic = calculate_bic_by_ll(ll, len(y_all), num_params)\n",
    "# print('LL: %.3f' % ll)\n",
    "# print('AIC: %.3f' % aic)\n",
    "# print('BIC: %.3f' % bic)\n",
    "#get other measures\n",
    "# acc, auc, nll, mse = compute_metrics(y_all, y_all_pred)\n",
    "# print('ACC: %.3f' % acc)\n",
    "# print('AUC: %.3f' % auc)\n",
    "# print('MSE: %.3f' % mse)\n",
    "logToWfl(\"After training data.\")\n",
    "\n",
    "#handle train split\n",
    "if train_split_type is not None:\n",
    "    train_df = pd.read_csv(os.path.join(working_dir, \"preprocessed_data_train.txt\"), sep=\"\\t\")\n",
    "    test_df = pd.read_csv(os.path.join(working_dir, \"preprocessed_data_test.txt\"), sep=\"\\t\")\n",
    "    if train_split_type == 'item':\n",
    "        item_ids = X[:, 1].toarray().flatten()\n",
    "        items_train = train_df[\"item_id\"].unique()\n",
    "        items_test = test_df[\"item_id\"].unique()\n",
    "        #np.isin(item_ids, items_train) gives True and False for all rows\n",
    "        #np.where(np.isin(item_ids, items_train)) gives row id for all True\n",
    "        #train is all X in train\n",
    "        train = X[np.where(np.isin(item_ids, items_train))]\n",
    "        test = X[np.where(np.isin(item_ids, items_test))]\n",
    "    else:\n",
    "        # Student-wise train-test split\n",
    "        user_ids = X[:, 0].toarray().flatten()\n",
    "        users_train = train_df[\"user_id\"].unique()\n",
    "        users_test = test_df[\"user_id\"].unique()\n",
    "        #np.isin(user_ids, users_train) gives True and False for all rows\n",
    "        #np.where(np.isin(user_ids, users_train)) gives row id for all True\n",
    "        #train is all X in train\n",
    "        train = X[np.where(np.isin(user_ids, users_train))]\n",
    "        test = X[np.where(np.isin(user_ids, users_test))]\n",
    "    X_train, y_train = train[:, 5:], train[:, 3].toarray().flatten()\n",
    "    X_test, y_test = test[:, 5:], test[:, 3].toarray().flatten()\n",
    "    model = LogisticRegression(solver=\"lbfgs\", max_iter=iter)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_train = model.predict_proba(X_train)[:, 1]\n",
    "    y_pred_test = model.predict_proba(X_test)[:, 1]\n",
    "    acc_train, auc_train, nll_train, mse_train = compute_metrics(y_train, y_pred_train)\n",
    "    acc_test, auc_test, nll_test, mse_test = compute_metrics(y_test, y_pred_test)\n",
    "#     print('ACC for training set: %.3f' % acc_train)\n",
    "#     print('AUC for training set: %.3f' % auc_train)\n",
    "#     print('MSE for training set: %.3f' % mse_train)\n",
    "#     print('ACC for testing set: %.3f' % acc_test)\n",
    "#     print('AUC for testing set: %.3f' % auc_test)\n",
    "#     print('MSE for testing set: %.3f' % mse_test)\n",
    "\n",
    "#handle student CV\n",
    "if cv_student:\n",
    "    logToWfl(\"Start student blocked cross validation.\")\n",
    "    y_all = None\n",
    "    y_pred_all = None\n",
    "    for i in range(1, cv_fold+1):\n",
    "        train_file_name = f\"preprocessed_data_cv_student_train_fold_{i}.txt\"\n",
    "        test_file_name = f\"preprocessed_data_cv_student_test_fold_{i}.txt\"\n",
    "        train_df = pd.read_csv(os.path.join(working_dir, train_file_name), sep=\"\\t\")\n",
    "        test_df = pd.read_csv(os.path.join(working_dir, test_file_name), sep=\"\\t\")\n",
    "        user_ids = X[:, 0].toarray().flatten()\n",
    "        users_train = train_df[\"user_id\"].unique()\n",
    "        users_test = test_df[\"user_id\"].unique()\n",
    "        #np.isin(user_ids, users_train) gives True and False for all rows\n",
    "        #np.where(np.isin(user_ids, users_train)) gives row id for all True\n",
    "        #train is all X in train\n",
    "        train = X[np.where(np.isin(user_ids, users_train))]\n",
    "        test = X[np.where(np.isin(user_ids, users_test))]\n",
    "        X_train, y_train = train[:, 5:], train[:, 3].toarray().flatten()\n",
    "        X_test, y_test = test[:, 5:], test[:, 3].toarray().flatten()\n",
    "        model = LogisticRegression(solver=\"lbfgs\", max_iter=iter)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_test = model.predict_proba(X_test)[:, 1]\n",
    "        if i == 1:\n",
    "            y_all = y_test\n",
    "            y_pred_all = y_pred_test\n",
    "        else:\n",
    "            y_all = np.concatenate((y_all,y_test), axis=0)\n",
    "            y_pred_all = np.concatenate((y_pred_all,y_pred_test), axis=0)\n",
    "    student_cv_rmse = compute_rmse(y_all, y_pred_all)\n",
    "    logToWfl(\"Finished student blocked cross validation.\")\n",
    "    \n",
    "    \n",
    "if cv_item:\n",
    "    logToWfl(\"Start item blocked cross validation.\")\n",
    "    y_all = None\n",
    "    y_pred_all = None\n",
    "    for i in range(1, cv_fold+1):\n",
    "        train_file_name = f\"preprocessed_data_cv_item_train_fold_{i}.txt\"\n",
    "        test_file_name = f\"preprocessed_data_cv_item_test_fold_{i}.txt\"\n",
    "        train_df = pd.read_csv(os.path.join(working_dir, train_file_name), sep=\"\\t\")\n",
    "        test_df = pd.read_csv(os.path.join(working_dir, test_file_name), sep=\"\\t\")\n",
    "        item_ids = X[:, 1].toarray().flatten()\n",
    "        items_train = train_df[\"item_id\"].unique()\n",
    "        items_test = test_df[\"item_id\"].unique()\n",
    "        #np.isin(item_ids, items_train) gives True and False for all rows\n",
    "        #np.where(np.isin(item_ids, items_train)) gives row id for all True\n",
    "        #train is all X in train\n",
    "        train = X[np.where(np.isin(item_ids, items_train))]\n",
    "        test = X[np.where(np.isin(item_ids, items_test))]\n",
    "        X_train, y_train = train[:, 5:], train[:, 3].toarray().flatten()\n",
    "        X_test, y_test = test[:, 5:], test[:, 3].toarray().flatten()\n",
    "        model = LogisticRegression(solver=\"lbfgs\", max_iter=iter)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred_test = model.predict_proba(X_test)[:, 1]\n",
    "        if i == 1:\n",
    "            y_all = y_test\n",
    "            y_pred_all = y_pred_test\n",
    "        else:\n",
    "            y_all = np.concatenate((y_all,y_test), axis=0)\n",
    "            y_pred_all = np.concatenate((y_pred_all,y_pred_test), axis=0)\n",
    "    item_cv_rmse = compute_rmse(y_all, y_pred_all)\n",
    "    logToWfl(\"Finished item blocked cross validation.\")\n",
    "\n",
    "#write values to analysis-summary\n",
    "analysis_summary_file_name = os.path.join(working_dir, \"analysis_summary.txt\")\n",
    "analysis_summary_content = \"\"\n",
    "#write values to model-values.xml\n",
    "model_values_file_name = os.path.join(working_dir, \"model_values.xml\")\n",
    "model_values_content = \"\"\n",
    "#write values to parameters.xml\n",
    "parameters_file_name = os.path.join(working_dir, \"parameters.xml\")\n",
    "parameters_content = \"\"\n",
    "\n",
    "analysis_summary_content = analysis_summary_content + \"KC Model Values for {} model\\n\".format(kc_name)\n",
    "analysis_summary_content = analysis_summary_content + \"AIC\\tBIC\\tLog Likelihood\\tNumber of Parameters\\tNumber of Observations\\n\"\n",
    "analysis_summary_content = analysis_summary_content + \"{:.8f}\\t{:.8f}\\t{:.8f}\\t{}\\t{}\\n\\n\".format(aic, bic, ll, num_params, len(y_all))\n",
    "\n",
    "model_values_content = model_values_content + \"<model_values>\\n<model>\\n\"\n",
    "model_values_content = model_values_content + \"<name>{}</name>\\n\".format(kc_col_name)\n",
    "model_values_content = model_values_content + \"<AIC>{:.8f}</AIC>\\n\".format(aic)\n",
    "model_values_content = model_values_content + \"<BIC>{:.8f}</BIC>\\n\".format(bic)\n",
    "model_values_content = model_values_content + \"<log_likelihood>{:.8f}</log_likelihood>\\n\".format(ll)\n",
    "\n",
    "\n",
    "#write CV to analysis-summary\n",
    "if cv_item or cv_student:\n",
    "    analysis_summary_content = analysis_summary_content + \"Cross Validation Values (Blocked)\\n\"\n",
    "    if cv_item and cv_student:\n",
    "        analysis_summary_content = analysis_summary_content + \"Cross Validation RMSE (student blocked)\\tCross Validation RMSE (item blocked)\\n\"\n",
    "        analysis_summary_content = analysis_summary_content + \"{:.8f}\\t{:.8f}\\n\\n\".format(student_cv_rmse, item_cv_rmse)\n",
    "        model_values_content = model_values_content + \"<student_blocked_cv>{:.8f}</student_blocked_cv>\\n\".format(student_cv_rmse)\n",
    "        model_values_content = model_values_content + \"<item_blocked_cv>{:.8f}</item_blocked_cv>\\n\".format(item_cv_rmse)\n",
    "    elif cv_item:\n",
    "        analysis_summary_content = analysis_summary_content + \"Cross Validation RMSE (item blocked)\\n\"\n",
    "        analysis_summary_content = analysis_summary_content + \"{:.8}\\n\\n\".format(item_cv_rmse)\n",
    "        model_values_content = model_values_content + \"<item_blocked_cv>{:.8f}</item_blocked_cv>\\n\".format(item_cv_rmse)\n",
    "    elif cv_student:\n",
    "        analysis_summary_content = analysis_summary_content + \"Cross Validation RMSE (student blocked)\\n\"\n",
    "        analysis_summary_content = analysis_summary_content + \"{:.8f}\\n\\n\".format(student_cv_rmse)\n",
    "        model_values_content = model_values_content + \"<student_blocked_cv>{:.8f}</student_blocked_cv>\\n\".format(student_cv_rmse)\n",
    "\n",
    "model_values_content = model_values_content + \"</model>\\n</model_values>\\n\"\n",
    "model_values = open(model_values_file_name, \"w\")\n",
    "model_values.write(model_values_content)        \n",
    "model_values.close();\n",
    "\n",
    "#write kc values to analysis-summary\n",
    "analysis_summary_content = analysis_summary_content + \"KC Values for {} model\\n\".format(kc_name)\n",
    "analysis_summary_content = analysis_summary_content + \"KC Name\\tIntercept (logit)\\tIntercept (probability)\\tSlope\\n\"\n",
    "\n",
    "analysis_summary = open(analysis_summary_file_name, \"w\")\n",
    "analysis_summary.write(analysis_summary_content)\n",
    "analysis_summary.close()\n",
    "skill_df.to_csv(analysis_summary_file_name, mode='a', sep = \"\\t\", header=False, index = False)\n",
    "#write student values to analysis-suumary\n",
    "analysis_summary = open(analysis_summary_file_name, \"a\")\n",
    "analysis_summary.write(\"\\nStudent Values for {} model\\n\".format(kc_name))\n",
    "analysis_summary.write('A student intercept value of \"N/A\" means the student did not perform any steps associated with any of the KCs in the selected KC model.\\n')\n",
    "analysis_summary.write('Anon Student Id\\tIntercept\\n')\n",
    "analysis_summary.close()\n",
    "student_df.to_csv(analysis_summary_file_name, mode='a', sep = \"\\t\", header=False, index = False)\n",
    "\n",
    "\n",
    "#write parameters.xml\n",
    "parameters_content = parameters_content + \"<parameters>\\n\"\n",
    "#loop through skills\n",
    "for index, row in skill_df.iterrows():\n",
    "    parameters_content = parameters_content + \"<parameter>\\n<type>Skill</type>\\n\"\n",
    "    parameters_content = parameters_content + \"<name>{}</name>\\n\".format(row['skill_name'])\n",
    "    parameters_content = parameters_content + \"<intercept>{:.8f}</intercept>\\n\".format(row['intercept'])\n",
    "    parameters_content = parameters_content + \"<slope>{:.8f}</slope>\\n\".format(row['slope'])\n",
    "    parameters_content = parameters_content + \"</parameter>\\n\"\n",
    "for index, row in student_df.iterrows():\n",
    "    parameters_content = parameters_content + \"<parameter>\\n<type>Student</type>\\n\"\n",
    "    parameters_content = parameters_content + \"<name>{}</name>\\n\".format(row['student_name'])\n",
    "    parameters_content = parameters_content + \"<intercept>{:.8f}</intercept>\\n\".format(row['intercept'])\n",
    "    parameters_content = parameters_content + \"<slope></slope>\\n\"\n",
    "    parameters_content = parameters_content + \"</parameter>\\n\"\n",
    "parameters_content = parameters_content + \"</parameters>\\n\"\n",
    "parameters = open(parameters_file_name, \"w\")\n",
    "parameters.write(parameters_content) \n",
    "parameters.close();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
