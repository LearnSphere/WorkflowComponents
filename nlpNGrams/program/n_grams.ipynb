{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(style='seaborn')\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "import seaborn as sns\n",
    "import urllib\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "import plotly.express as px\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import circlify\n",
    "from tika import parser # pip install tika\n",
    "import requests\n",
    "from fpdf import FPDF\n",
    "from PIL import Image\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining the function to remove punctuation and extra space\n",
    "def remove_punctuation(text):\n",
    "    if(type(text)==float):\n",
    "        return text\n",
    "    #remove punctuation\n",
    "    ans_no_punct = \"\"\n",
    "    for i in text:\n",
    "        if i not in string.punctuation:\n",
    "            ans_no_punct += i\n",
    "    #remove the extra space\n",
    "    ans_no_extra_space = \"\"\n",
    "    prev_char = \"\"\n",
    "    for i in ans_no_punct:\n",
    "        if i.strip() != \"\" or ( i.strip() == \"\" and prev_char.strip() != \"\"):\n",
    "            ans_no_extra_space += i\n",
    "        prev_char = i  \n",
    "    return ans_no_extra_space.strip()\n",
    "#remove_punctuation(\"Goodwill and other intangible assets account for some 2.0 mln euro ( $ 2.6 mln ) of the purchase price , 20 pct of which payable in Aspo shares .\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to generate n-grams:\n",
    "#params:\n",
    "#text-the text for which we have to generate n-grams\n",
    "#ngram-number of grams to be generated from the text(1,2,3,4 etc., default value=1)\n",
    "def generate_N_grams(text, ngram=1, exclude_stopwords=True):\n",
    "    words = []\n",
    "    if exclude_stopwords:\n",
    "        words=[word for word in text.split(\" \") if word not in STOPWORDS] \n",
    "    else:\n",
    "        words=[word for word in text.split(\" \") if word not in []] \n",
    "    temp=zip(*[words[i:] for i in range(0,ngram)])\n",
    "    all_combinations = [' '.join(ngram) for ngram in temp]\n",
    "    all_combination_count = defaultdict(int)\n",
    "    for combination in all_combinations:\n",
    "        all_combination_count[combination] += 1\n",
    "    return all_combination_count\n",
    "#generate_N_grams(remove_punctuation(\"Goodwill and other intangible Goodwill and other intangible assets account for some 2.0 mln euro ( $ 2.6 mln ) of the purchase price , 20 pct of which payable in Aspo shares .\"), 2, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create function to get a color dictionary\n",
    "def get_colordict(palette,number,start):\n",
    "    pal = list(sns.color_palette(palette=palette, n_colors=number).as_hex())\n",
    "    color_d = dict(enumerate(pal, start=start))\n",
    "    return color_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_count_dict(final_dict, to_merge_dict):\n",
    "    final_dict_keys = final_dict.keys()\n",
    "    for to_merge_key in to_merge_dict.keys():\n",
    "        if to_merge_key in final_dict_keys:\n",
    "            final_dict[to_merge_key] = final_dict[to_merge_key] + to_merge_dict[to_merge_key]\n",
    "        else:\n",
    "            final_dict[to_merge_key] = to_merge_dict[to_merge_key]\n",
    "    return final_dict\n",
    "#to_merge_dict = {'key1': 1, 'key2': 2, 'key3': 3}\n",
    "#final_dict = {}\n",
    "#print(merge_count_dict(final_dict, to_merge_dict))\n",
    "#print(merge_count_dict(final_dict, to_merge_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test on command line\n",
    "#\"C:/ProgramData/Anaconda3/Python\" n_grams.py -programDir . -workingDir . -userId 1 -exclude_stopwords Yes -n_grams 1 -plot_top_term 20 -plot_type “Donut chart” -text_column_nodeIndex 0 -text_column_fileIndex 0 -text_column user_id -text_corpus File -node 0 -fileIndex 0 \"ISLS_2023_Codesign_Short_Paper.pdf\"\n",
    "#\"C:/ProgramData/Anaconda3/Python\" n_grams.py -programDir . -workingDir . -userId 1 -exclude_stopwords Yes -n_grams 1 -plot_top_term 20 -plot_type “Bar chart” -text_column_nodeIndex 0 -text_column_fileIndex 0 -text_column answer -text_corpus \"Column in a file\" -node 0 -fileIndex 0 \"user_comment.txt\"\n",
    "\n",
    "#all variables\n",
    "command_line = False\n",
    "working_dir = \"\"\n",
    "program_dir = \"\"\n",
    "file_name = \"\"\n",
    "n_grams = 1\n",
    "plot_top_term = 10\n",
    "exclude_stopwords = \"Yes\"\n",
    "plot_type = \"\"\n",
    "text_corpus = \"\"\n",
    "text_column = \"\"\n",
    "url = \"\"\n",
    "   \n",
    "#command line     \n",
    "if command_line:\n",
    "    arg_parser = argparse.ArgumentParser(description='Python program for NLP NGrams.')\n",
    "    arg_parser.add_argument('-programDir', type=str, help='the component program directory')\n",
    "    arg_parser.add_argument('-workingDir', type=str, help='the component instance working directory')\n",
    "    arg_parser.add_argument(\"-node\", nargs=1, action='append')\n",
    "    arg_parser.add_argument(\"-fileIndex\", nargs=2, action='append')\n",
    "    arg_parser.add_argument(\"-n_grams\", type=int, choices=[1,2,3,4], default=1)\n",
    "    arg_parser.add_argument(\"-plot_top_term\", type=int)\n",
    "    arg_parser.add_argument(\"-exclude_stopwords\", type=str, choices=[\"Yes\", \"No\"], default=\"Yes\")\n",
    "    arg_parser.add_argument(\"-plot_type\", type=str, choices=[\"Bar chart\", \"Donut chart\", \"Treemap\", \"Circle packing\"], default=\"Bar chart\")\n",
    "    arg_parser.add_argument(\"-text_column\", type=str)\n",
    "    arg_parser.add_argument(\"-url\", type=str)\n",
    "    arg_parser.add_argument(\"-text_corpus\", type=str, choices=[\"Column in a file\", \"File\", \"URL\"], default=\"File\")\n",
    "    args, option_file_index_args = arg_parser.parse_known_args()\n",
    "    #var for both text corpus\n",
    "    working_dir = args.workingDir\n",
    "    program_dir = args.programDir\n",
    "    file_name = args.fileIndex[0][1]\n",
    "    n_grams = args.n_grams\n",
    "    plot_top_term = args.plot_top_term\n",
    "    exclude_stopwords = args.exclude_stopwords\n",
    "    if exclude_stopwords == \"No\":\n",
    "        exclude_stopwords = False\n",
    "    else:\n",
    "        exclude_stopwords = True\n",
    "    plot_type = args.plot_type\n",
    "    text_corpus = args.text_corpus\n",
    "    #for Column in a file\n",
    "    if text_corpus == \"Column in a file\":\n",
    "        text_column = args.text_column\n",
    "    #for url\n",
    "    if text_corpus == \"URL\":\n",
    "        url = args.url\n",
    "else: #for testing\n",
    "    working_dir = \".\"\n",
    "    program_dir = \".\"\n",
    "    #file_name = \"ISLS_2023_Codesign_Short_Paper.pdf\"\n",
    "    file_name = \"user_comment.txt\"\n",
    "    n_grams = 2\n",
    "    plot_top_term = 500\n",
    "    exclude_stopwords = False\n",
    "    plot_type = \"Donut chart\"\n",
    "    #Bar chart, Donut chart, Treemap, Circle packing\n",
    "    #text_corpus = \"File\"\n",
    "    text_corpus = \"Column in a file\"\n",
    "    #text_corpus = \"URL\"\n",
    "    text_column = \"answer\"\n",
    "    url = \"https://pslcdatashop.web.cmu.edu\"\n",
    "\n",
    "df_words = None\n",
    "if text_corpus == \"File\" or text_corpus == \"URL\":\n",
    "    text = \"\"\n",
    "    if text_corpus == \"File\":\n",
    "        file_extension = pathlib.Path(file_name).suffix\n",
    "        if file_extension == \".pdf\":\n",
    "            raw = parser.from_file(file_name)\n",
    "            text = raw['content']\n",
    "        elif file_extension == \".doc\" or file_extension == \".docx\":\n",
    "            raw = parser.from_file(file_name)\n",
    "            text = raw['content']\n",
    "        else: #any other file\n",
    "            file = open(file_name, \"r\", encoding=\"ISO-8859-1\") \n",
    "            text = file.read()\n",
    "    elif text_corpus == \"URL\":\n",
    "        url_response = requests.get(url)\n",
    "        text = parser.from_buffer(url_response.content)['content']\n",
    "    # Clean text\n",
    "    text_c = remove_punctuation(text)\n",
    "    if type(text_c) == str:\n",
    "        # clean more\n",
    "        text_c = re.sub('[^A-Za-z0-9°]+', ' ', text_c)\n",
    "        text_c = text_c.replace('\\n', '').lower()\n",
    "    else:\n",
    "        text_c = str(text_c)\n",
    "    all_combination_count = generate_N_grams(text_c, n_grams, exclude_stopwords)\n",
    "    #create DataFrame\n",
    "    df_words = pd.DataFrame(list(all_combination_count.items()), columns = ['term', 'count'])\n",
    "    df_words.sort_values('count', ascending=False, inplace=True)\n",
    "    df_words.reset_index(drop=True, inplace=True)\n",
    "elif text_corpus == \"Column in a file\":\n",
    "    text_corpus_df = pd.read_csv(file_name,sep=\"\\t\",encoding='ISO-8859-1', quotechar='\"',skipinitialspace=True, error_bad_lines=False)\n",
    "    final_combination_count = defaultdict(int)\n",
    "    for index, row in text_corpus_df.iterrows():\n",
    "        # Clean text\n",
    "        text_c = remove_punctuation(row[text_column])\n",
    "        if type(text_c) == str:\n",
    "            # clean more\n",
    "            text_c = re.sub('[^A-Za-z0-9°]+', ' ', text_c)\n",
    "            text_c = text_c.replace('\\n', '').lower()\n",
    "        else:\n",
    "            text_c = str(text_c)   \n",
    "        all_combination_count = generate_N_grams(text_c, n_grams, exclude_stopwords)\n",
    "        #combine with current result\n",
    "        final_combination_count = merge_count_dict(final_combination_count, all_combination_count)\n",
    "        #create DataFrame\n",
    "        df_words = pd.DataFrame(list(final_combination_count.items()), columns = ['term', 'count'])\n",
    "        df_words.sort_values('count', ascending=False, inplace=True)\n",
    "        df_words.reset_index(drop=True, inplace=True)\n",
    "#output result to word_frequency.txt\n",
    "df_words.to_csv('word_frequency.txt', sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_term_plot = df_words.shape[0]\n",
    "if num_term_plot > plot_top_term:\n",
    "    num_term_plot = plot_top_term\n",
    "#draw and output plot\n",
    "if plot_type == \"Bar chart\":\n",
    "    num_col_bar_chart = 5\n",
    "    if plot_top_term < num_col_bar_chart:\n",
    "        num_col_bar_chart = 1\n",
    "    index_list = [[i[0],i[-1]+1] for i in np.array_split(range(num_term_plot), num_col_bar_chart)]\n",
    "    n = df_words['count'].max()\n",
    "    color_dict = get_colordict('viridis', n, 1)\n",
    "    fig, axs = plt.subplots(1, num_col_bar_chart, figsize=(16,8), facecolor='white', squeeze=False)\n",
    "    for col, idx in zip(range(0,num_col_bar_chart), index_list):\n",
    "        df = df_words[idx[0]:idx[-1]]\n",
    "        label = [w + ': ' + str(n) for w,n in zip(df['term'],df['count'])]\n",
    "        color_l = [color_dict.get(i) for i in df['count']]\n",
    "        x = list(df['count'])\n",
    "        y = list(range(0, len(x)))\n",
    "        sns.barplot(x = x, y = y, data=df, alpha=0.9, orient = 'h',\n",
    "                    ax = axs[0][col], palette = color_l)\n",
    "        axs[0][col].set_xlim(0,n+1)                     #set X axis range max\n",
    "        axs[0][col].set_yticklabels(label, fontsize=12)\n",
    "        axs[0][col].spines['bottom'].set_color('white')\n",
    "        axs[0][col].spines['right'].set_color('white')\n",
    "        axs[0][col].spines['top'].set_color('white')\n",
    "        axs[0][col].spines['left'].set_color('white')\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        plt.tight_layout()\n",
    "    plt.savefig('word_frequency_plot.png')  \n",
    "elif plot_type == \"Donut chart\":\n",
    "    max_donut_chart = 25\n",
    "    if num_term_plot > max_donut_chart:\n",
    "        num_term_plot = max_donut_chart\n",
    "    pal = list(sns.color_palette(palette='Reds_r', n_colors=num_term_plot).as_hex())\n",
    "    fig = px.pie(df_words[0:num_term_plot], values='count', names='term',\n",
    "             color_discrete_sequence=pal)\n",
    "    fig.update_traces(textposition='outside', textinfo='percent+label', \n",
    "                  hole=.6, hoverinfo=\"label+percent+name\")\n",
    "    fig.update_layout(width = 800, height = 600,\n",
    "                  margin = dict(t=0, l=0, r=0, b=0))\n",
    "    fig.update_layout(title_text='', title_x=0.5,\n",
    "                 title={\"yref\": \"paper\",\"y\" : 1,\"yanchor\" : \"bottom\"})\n",
    "    fig.write_image(\"word_frequency_plot.png\")\n",
    "elif plot_type == \"Treemap\":\n",
    "    fig = px.treemap(df_words[0:num_term_plot], path=[px.Constant(\"Treemap\"), 'term'],\n",
    "                 values='count',\n",
    "                 color='count',\n",
    "                 color_continuous_scale='viridis',\n",
    "                 color_continuous_midpoint=np.average(df_words['count'])\n",
    "                )\n",
    "    fig.update_layout(margin = dict(t=50, l=25, r=25, b=25))\n",
    "    fig.write_image(\"word_frequency_plot.png\") \n",
    "elif plot_type == \"Circle packing\":\n",
    "    circles = circlify.circlify(df_words['count'][0:num_term_plot].tolist(), \n",
    "                            show_enclosure=False, \n",
    "                            target_enclosure=circlify.Circle(x=0, y=0)\n",
    "                           )\n",
    "    n = df_words['count'][0:num_term_plot].max()\n",
    "    color_dict = get_colordict('RdYlBu_r',n ,1)\n",
    "\n",
    "    #Circle packing showing the top 30 words\n",
    "    fig, ax = plt.subplots(figsize=(9,9), facecolor='white')\n",
    "    ax.axis('off')\n",
    "    lim = max(max(abs(circle.x)+circle.r, abs(circle.y)+circle.r,) for circle in circles)\n",
    "    plt.xlim(-lim, lim)\n",
    "    plt.ylim(-lim, lim)\n",
    "\n",
    "    # list of labels\n",
    "    labels = list(df_words['term'][0:num_term_plot])\n",
    "    counts = list(df_words['count'][0:num_term_plot])\n",
    "    labels.reverse()\n",
    "    counts.reverse()\n",
    "\n",
    "    # print circles\n",
    "    for circle, label, count in zip(circles, labels, counts):\n",
    "        x, y, r = circle\n",
    "        ax.add_patch(plt.Circle((x, y), r, alpha=0.9, color = color_dict.get(count)))\n",
    "        plt.annotate(label +'\\n'+ str(count), (x,y), size=8, va='center', ha='center')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.savefig('word_frequency_plot.png')\n",
    "\n",
    "#save png to a pdf for learnsphere displays pdf better\n",
    "pdf = FPDF('P','mm','A4') # create an A4-size pdf document\n",
    "img = Image.open('word_frequency_plot.png') \n",
    "# get width and height\n",
    "ration = img.width/img.height\n",
    "x,y,w,h = 10,10,180,180/ration\n",
    "pdf.add_page()\n",
    "pdf.image('word_frequency_plot.png', x,y,w,h)\n",
    "pdf.output(\"word_frequency_plot.pdf\",\"F\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
